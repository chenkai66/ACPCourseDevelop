{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "39f10553-d93f-496b-a5c1-7e224fbb7f0f",
      "metadata": {},
      "source": [
        "## 2.4 Automated Evaluation of the Q&A Robot's Performance\n",
        "\n",
        "### üöÑ Preface\n",
        "\n",
        "The new Q&A robot may encounter some issues during actual use, especially when users ask specific questions that require detailed knowledge from internal documents. For example, when a newcomer asks, \"How to request leave,\" the robot might provide a generic response instead of consulting the company‚Äôs policy documents for precise guidance.\n",
        "\n",
        "Just as conventional software development requires testing and validation, it is equally important to establish an **evaluation system** for your Q&A robot project. This ensures that similar issues can be quickly diagnosed and addressed. Moreover, after implementing any optimization or improvement, you should run a batch of test questions to confirm that the changes have a positive impact on the overall performance of the Q&A robot.\n",
        "\n",
        "In this chapter, you will learn how to **automate evaluation processes** using LLMs and specialized frameworks like **Ragas**, enabling you to measure both the quality of answers and the effectiveness of retrieval.\n",
        "\n",
        "## üçÅ Course Objectives\n",
        "After completing this chapter, you will be able to:\n",
        "\n",
        "- Understand how to automate evaluations for large language model (LLM) applications.\n",
        "- Evaluate RAG chatbots using automated tools such as Ragas.\n",
        "- Identify and solve problems in your Q&A bot through analysis of evaluation scores.\n",
        "\n",
        "<!-- ## üìñ Course Outline\n",
        "In this chapter, we will first understand some current issues with RAG chatbot through a specific problem. Then, we will attempt to discover the issue by implementing a simple automated test ourselves. Finally, we will learn how to use the more mature RAG application testing framework, Ragas, to assess the performance of RAG chatbot.\n",
        "\n",
        "- 1.&nbsp;Evaluating RAG Application Performance\n",
        "    - 1.1 Issues with the Q&A robot\n",
        "    - 1.2 Checking RAG retrieval results to troubleshoot issues\n",
        "    - 1.3 Attempting to establish an automated testing mechanism\n",
        "\n",
        "- 2.&nbsp;Using Ragas to Evaluate Application Performance\n",
        "     - 2.1 Evaluating the quality of responses from the RAG chatbot\n",
        "       - 2.1.1 Quick start\n",
        "       - 2.1.2 Understanding the calculation process of answer correctness\n",
        "     - 2.2 Evaluating the recall effectiveness of retrieval\n",
        "       - 2.2.1 Quick start\n",
        "       - 2.2.2 Understanding the calculation process of context recall and context precision -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c917f6bc",
      "metadata": {},
      "source": [
        "## 1. Evaluating RAG Application Performance\n",
        "\n",
        "### 1.1 Issues with the Q&A Bot\n",
        "\n",
        "In the previous chapter, you completed the development of a Q&A bot and began exploring how to evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "33d09a89",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your configured API Key is: sk-4b*****\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from config.load_key import load_key\n",
        "load_key()\n",
        "print(f\"Your configured API Key is: {os.environ[\"DASHSCOPE_API_KEY\"][:5]+\"*\"*5}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "947e89be",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Which department is Michael Johnson in?\n",
            "Answer: Michael Johnson is in the IT Infrastructure department. He holds the position of System Administrator and works under the supervision of Michael Chen at the 456 Tech Hub."
          ]
        }
      ],
      "source": [
        "from chatbot import rag\n",
        "rag.indexing()\n",
        "query_engine = rag.create_query_engine(rag.load_index())\n",
        "print('Question: Which department is Michael Johnson in?')\n",
        "response = query_engine.query('Which department is Michael Johnson in?')\n",
        "print('Answer: ', end='')\n",
        "response.print_response_stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fef052a",
      "metadata": {},
      "source": [
        "As part of this exploration, you asked the question:\n",
        "\n",
        "> **\"Which department is Michael Johnson in?\"**\n",
        "\n",
        "And received the following answer from the bot:\n",
        "\n",
        "> **\"Michael Johnson is in the IT Infrastructure department Department. He serves as a System Administrator, working under Michael Chen at the 456 Tech Hub.\"**\n",
        "\n",
        "The original document contain multiple individuals named Michael Johnson, none of whom are associated with the **Course Development Department**. The LLM, however, generated a confident and specific response that combined elements from different contexts ‚Äî creating the illusion of accuracy without being grounded in factual data.\n",
        "\n",
        "<a href=\"https://img.alicdn.com/imgextra/i1/O1CN01C6ZkQG1uGdQbJVw19_!!6000000006010-2-tps-1478-732.png\" target=\"_blank\">\n",
        "<img src=\"https://img.alicdn.com/imgextra/i1/O1CN01C6ZkQG1uGdQbJVw19_!!6000000006010-2-tps-1478-732.png\" width=\"800\">\n",
        "</a>\n",
        "\n",
        "This highlights a critical issue: the answer was not based on a correct or unambiguous context, but rather on an aggregation or assumption made by the model ‚Äî potentially leading to misleading conclusions.\n",
        "\n",
        "Therefore, the next step is to examine the retrieval results used by the RAG system before generating the final answer, to determine whether the context provided was accurate and relevant to the question."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "793fa40e",
      "metadata": {},
      "source": [
        "### 1.2. Check RAG Retrieval Results for Problem Diagnosis\n",
        "\n",
        "To validate the reasoning behind the answer, we inspect the **context chunks** retrieved by the RAG system before generating the response.\n",
        "\n",
        "Here is the retrieved context:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "79691f6b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Employee Key Contact Information.md 2025-07-10\\n3 / 6Employee\\nIDName SupervisorOffice\\nLocationPosition\\nTitlePhone\\nNumberEmail AddressKey\\nResponsibilities\\nEID-210Jennifer\\nLeeMichael\\nChen456\\nTech\\nHub\\n#210Data Analyst(555)\\n234-\\n5687jennifer.l@educompany.comPerformance\\nmetrics tracking\\nEID-211Christopher\\nLeeMichael\\nChen456\\nTech\\nHub\\n#211Security\\nAnalyst(555)\\n234-\\n5688christopher.l@educompany.comSystem\\nvulnerability\\nassessments\\nEID-212Olivia\\nTaylorMichael\\nChen456\\nTech\\nHub\\n#212Project\\nCoordinator(555)\\n234-\\n5689olivia.t@educompany.comTimeline\\nmanagement\\nEID-213Daniel\\nSmithMichael\\nChen456\\nTech\\nHub\\n#213Frontend\\nDeveloper(555)\\n234-\\n5690daniel.s@educompany.comUI\\nimplementation\\nEID-214Rachel KimMichael\\nChen456\\nTech\\nHub\\n#214Mobile App\\nDeveloper(555)\\n234-\\n5691rachel.k@educompany.comCross-platform\\ndevelopment\\nEID-215Thomas\\nNguyenMichael\\nChen456\\nTech\\nHub\\n#215Cloud\\nEngineer(555)\\n234-\\n5692thomas.nguyen@educompany.comInfrastructure\\nmanagement\\nEID-216Sophia LiMichael\\nChen456\\nTech\\nHub\\n#216DevOps\\nSpecialist(555)\\n234-\\n5693sophia.l@educompany.comContinuous\\nintegration\\npipelines\\nEID-217William\\nBrownMichael\\nChen456\\nTech\\nHub\\n#217Database\\nAdministrator(555)\\n234-\\n5694william.b@educompany.comData\\narchitecture\\ndesign\\nEID-218Emma\\nWilsonMichael\\nChen456\\nTech\\nHub\\n#218Technical\\nSupport(555)\\n234-\\n5695emma.w@educompany.com User assistance\\nEID-219John SmithMichael\\nChen456\\nTech\\nHub\\n#219Junior QA\\nTester(555)\\n234-\\n5696john.smith2@educompany.comBasic testing\\nprocedures\\nEID-220Michael\\nJohnsonMichael\\nChen456\\nTech\\nHub\\n#220System\\nAdministrator(555)\\n234-\\n5697michael.j2@educompany.comServer\\nmaintenance\\nTextbook Production Department\\nEmployee\\nIDNameSupervisorOffice\\nLocationPosition\\nTitlePhone\\nNumberEmail Address Key Responsibilities',\n",
              " 'Employee Key Contact Information.md 2025-07-10\\n1 / 6Company Organizational Structure & Key Contact Information\\nCorporate Mission Statement\\nOur organization is dedicated to pioneering digital transformation in education through innovative learning solutions. As an edtech leader, we\\nspecialize in developing interactive online platforms that integrate cutting-edge educational research with emerging technologies. Our\\nstrategic vision focuses on three core pillars:\\n\\x00. Personalized Learning: Creating adaptive platforms that cater to diverse learning styles and paces\\n\\x00. Educational Equity: Ensuring accessible, high-quality resources for all demographics\\n\\x00. Lifelong Learning: Developing continuous education pathways that support career advancement\\nWe maintain rigorous quality standards through our comprehensive evaluation framework, which includes:\\nLearner engagement metrics\\nPedagogical effectiveness assessments\\nTechnological innovation benchmarks\\nDepartmental Structure Overview\\nDepartment Name Core ResponsibilitiesInterdepartmental\\nPartnerships\\nInstructional DesignCurriculum development, pedagogical research, learning outcomes\\nanalysisCourse Development, Evaluation\\nCourse DevelopmentTechnical implementation of educational content, platform integration IT, Marketing\\nTextbook Production Content curation, exercise design, multimedia resource development Instructional Design\\nQuality Assurance Content validation, compliance monitoring, learner feedback analysis All departments\\nMarketing & Outreach Brand positioning, market research, partnership development Instructional Design\\nHuman ResourcesTalent acquisition, professional development, employee engagement All departments\\nIT Infrastructure System maintenance, cybersecurity protocols, technical support All departments\\nPerformance\\nManagementKPI tracking, 360-degree evaluations, compensation alignment HR, Finance\\nExecutive Leadership Team\\nName Position Office Location Contact Info\\nSarah JohnsonChief Learning Officer (CLO)123 Innovation Dr #101(555) 123-4567\\nMichael ChenCTO & Head of Technology456 Tech Hub #202(555) 234-5678\\nEmily Davis Director of Marketing789 Market Ave #303(555) 345-6789\\nDavid WilsonVP of Human Resources321 People Place #404(555) 456-7890\\nRobert TaylorDirector of IT Infrastructure654 Tech Tower #505(555) 567-8901\\nDepartmental Staff Directory\\nInstructional Design Department\\nEmployee\\nIDName SupervisorOffice\\nLocationPosition TitlePhone\\nNumberEmail AddressKey\\nResponsibilities\\nEID-101John\\nSmithSarah\\nJohnson123\\nInnovation\\nDr #101Lead\\nInstructional\\nDesigner(555)\\n123-\\n4567john.s@educompany.comCurriculum\\narchitecture,\\npedagogical\\nstrategy, learning\\nanalytics']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contexts = [node.get_content() for node in response.source_nodes]\n",
        "contexts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48bbcc22",
      "metadata": {},
      "source": [
        "From this context, we can see that Michael Johnson is indeed listed as a System Administrator in the Course Development Department, which directly supports the generated answer.\n",
        "\n",
        "‚úÖ Conclusion: The retrieval performed well ‚Äî relevant and sufficient information was retrieved to support the correct answer.\n",
        "\n",
        "While this example shows good retrieval performance, not all queries may yield such clear results. Therefore, we must build an automated evaluation framework to consistently measure retrieval quality and answer accuracy across multiple test cases.\n",
        "\n",
        "### 1.3 Attempt to Establish an Automated Testing Mechanism\n",
        "\n",
        "Although manual inspection helps understand individual cases, it becomes impractical when dealing with hundreds or thousands of questions. Hence, we aim to build an automated testing mechanism.\n",
        "\n",
        "#### 1.3.1 Validating Answer Quality Using LLMs\n",
        "\n",
        "Large language models can be leveraged not only to generate answers but also to evaluate them. By providing both the question and the generated answer, we can prompt the LLM to judge whether the answer is valid or invalid based on reference material.\n",
        "\n",
        "Here is a function that checks if the answer effectively addresses the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b9a20f5b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Valid response'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from chatbot import llm\n",
        "\n",
        "def test_answer(question, answer):\n",
        "    prompt = (\"You are a tester.\\n\"\n",
        "        \"You need to check whether the following answer effectively responds to the user's question.\\n\"\n",
        "        \"The reply can only be: Valid response or Invalid response. Do not provide any other information.\\n\"\n",
        "        \"------\"\n",
        "        f\"The answer is {answer}\"\n",
        "        \"------\"\n",
        "        f\"The question is: {question}\"\n",
        "    )\n",
        "    return llm.invoke(prompt,model_name=\"qwen-max\")\n",
        "\n",
        "\n",
        "test_answer(\"Which department is Michael Johnson in?\", \"Michael Johnson is in the IT Infrastructure department Department. He holds the position of System Administrator and works under the supervision of Michael Chen at the 456 Tech Hub.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7c22a73",
      "metadata": {},
      "source": [
        "The LLM confirms that the answer is valid and correctly addresses the question.\n",
        "\n",
        "\n",
        "\n",
        "#### 1.3.2 Validating Context Relevance\n",
        "\n",
        "Equally important is verifying that the retrieved context is useful for answering the question. We define another function to evaluate this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4e20f11a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The reference information is useful.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def test_contexts(question, answer, contexts):\n",
        "    prompt = (\n",
        "        \"You are a tester. Your task is to determine whether the provided reference materials directly support the given answer to the question.\\n\"\n",
        "        \"If the answer can be clearly found or derived from the reference materials, respond with: The reference information is useful.\\n\"\n",
        "        \"Otherwise, respond with: The reference information is not useful.\\n\"\n",
        "        \"Do not provide any other explanation or information.\\n\"\n",
        "        \"------\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "        f\"Answer: {answer}\\n\"\n",
        "        f\"Reference materials: {' '.join(contexts)}\\n\"\n",
        "        \"------\"\n",
        "    )\n",
        "    return llm.invoke(prompt, model_name=\"qwen-max\")\n",
        "test_contexts(\n",
        "    \"Which department is Michael Johnson in?\", \n",
        "    \"Michael Johnson is in the IT Infrastructure department Department. He holds the position of System Administrator and works under the supervision of Michael Chen at the 456 Tech Hub.\", \n",
        "    contexts[0]+contexts[1]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e550e2",
      "metadata": {},
      "source": [
        "#### 1.3.3 Summary of Evaluation Logic\n",
        "\n",
        "| Component         | Method                           | Result                |\n",
        "|------------------|----------------------------------|------------------------|\n",
        "| Question          | \"Which department is Michael Johnson in?\" |                        |\n",
        "| Generated Answer  | Evaluated using `test_answer`     | Valid response         |\n",
        "| Retrieved Context | Evaluated using `test_contexts`   | Reference info is useful |\n",
        "\n",
        "With the two methods above, you have already preliminarily set up a prototype of a LLMs testing project. However, the current implementation is still incomplete. For instance:\n",
        "\n",
        "- Because LLMs sometimes hallucination, their answers may appear convincingly real. In such cases, the `test_answer` method cannot effectively detect this issue.\n",
        "- The higher the proportion of relevant information in the retrieved references, the better (signal-to-noise ratio). However, our current testing method is relatively simple and does not take these factors into account.\n",
        "\n",
        "You might consider using some mature testing frameworks to further improve your testing project. For example, [Ragas](https://docs.ragas.io/en/stable), which is a testing framework specifically designed to evaluate the performance of RAG chatbot."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a0e7e9",
      "metadata": {},
      "source": [
        "## 2. Using Ragas to Evaluate Application Performance\n",
        "\n",
        "Ragas provides multiple metrics that can be used to evaluate the quality of question-answering across the entire chain of an application. For example:\n",
        "\n",
        "- Evaluation of overall response quality:\n",
        "  - Answer Correctness, used to assess the accuracy of answers generated by the RAG application.\n",
        "- Evaluation of the generation phase:\n",
        "  - Answer Relevancy, used to assess whether the answers generated by the RAG application are relevant to the question.\n",
        "  - Faithfulness, used to evaluate the factual consistency between the answers generated by the RAG application and the retrieved reference materials.\n",
        "- Evaluation of the recall phase:\n",
        "  - Context Precision, used to evaluate whether entries related to the correct answer in contexts are ranked high and have a high proportion (signal-to-noise ratio).\n",
        "  - Context Recall, used to evaluate how many relevant reference materials are retrieved; a higher score means fewer relevant references are missed.\n",
        "\n",
        "<a href=\"https://img.alicdn.com/imgextra/i4/O1CN01b2lVQp21JZCJy6Nfe_!!6000000006964-0-tps-739-420.jpg\" target=\"_blank\">\n",
        "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01b2lVQp21JZCJy6Nfe_!!6000000006964-0-tps-739-420.jpg\" width=\"500\">\n",
        "</a>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d819011",
      "metadata": {},
      "source": [
        "### 2.1 Evaluating the Response Quality of RAG Applications\n",
        "\n",
        "#### 2.1.1 Quick Start  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86554952",
      "metadata": {},
      "source": [
        "### 2.1 Evaluating the Response Quality of RAG Applications\n",
        "\n",
        "#### 2.1.1 Quick Start  \n",
        "\n",
        "\n",
        "When evaluating the overall response quality of a RAG chatbot, using Ragas' Answer Correctness is an excellent metric. To calculate this metric, you need to prepare the following two types of data to evaluate the quality of the answer generated by the RAG chatbot:\n",
        "\n",
        "1. question (The question input to the RAG chatbot)\n",
        "2. ground_truth (The correct answer you already know)\n",
        "\n",
        "To illustrate the differences in evaluation metrics for different responses, we have prepared three sets of RAG chatbot responses to the question:\n",
        "\n",
        "**Question:**  \n",
        "\"Which department does Michael Johnson belong to?\"\n",
        "\n",
        "We will compare each model-generated **answer** against the known **ground truth**.\n",
        "\n",
        "Three sample answers are provided below, each representing a different level of correctness:\n",
        "\n",
        "- **Answer 1:** Based on the provided information, there is no mention of the department Michael Johnson belongs to. If you can provide more information about Michael Johnson, I may be able to help you find the answer.  \n",
        "  ‚û§ This is considered an **invalid answer**, as it fails to provide the correct response even when context may have been available.\n",
        "\n",
        "- **Answer 2:** Michael Johnson belongs to the Human Resources Department.  \n",
        "  ‚û§ This is a **hallucinated answer**, as it provides a confident but incorrect response.\n",
        "\n",
        "- **Answer 3:** Michael Johnson belongs to the Course Development Department.  \n",
        "  ‚û§ This is the **correct answer**, matching the ground truth exactly.                                         |\n",
        "\n",
        "We can then run the following code to calculate the score for response accuracy (i.e., Answer Correctness)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "39940304",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8dda0f4d5c91474bbc009fc6d36da617",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Which department is Michael Johnson in?</td>\n",
              "      <td>According to the provided information, there i...</td>\n",
              "      <td>Michael Johnson is a member of the Course Deve...</td>\n",
              "      <td>0.168191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Which department is Michael Johnson in?</td>\n",
              "      <td>Michael Johnson is in the HR department</td>\n",
              "      <td>Michael Johnson is a member of the Course Deve...</td>\n",
              "      <td>0.496046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Which department is Michael Johnson in?</td>\n",
              "      <td>Michael Johnson is in the Course Development D...</td>\n",
              "      <td>Michael Johnson is a member of the Course Deve...</td>\n",
              "      <td>0.998264</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  question  \\\n",
              "0  Which department is Michael Johnson in?   \n",
              "1  Which department is Michael Johnson in?   \n",
              "2  Which department is Michael Johnson in?   \n",
              "\n",
              "                                              answer  \\\n",
              "0  According to the provided information, there i...   \n",
              "1            Michael Johnson is in the HR department   \n",
              "2  Michael Johnson is in the Course Development D...   \n",
              "\n",
              "                                        ground_truth  answer_correctness  \n",
              "0  Michael Johnson is a member of the Course Deve...            0.168191  \n",
              "1  Michael Johnson is a member of the Course Deve...            0.496046  \n",
              "2  Michael Johnson is a member of the Course Deve...            0.998264  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.llms.tongyi import Tongyi\n",
        "from langchain_community.embeddings import DashScopeEmbeddings\n",
        "from datasets import Dataset\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import answer_correctness\n",
        "\n",
        "data_samples = {\n",
        "    'question': [\n",
        "        'Which department is Michael Johnson in?',\n",
        "        'Which department is Michael Johnson in?',\n",
        "        'Which department is Michael Johnson in?'\n",
        "    ],\n",
        "    'answer': [\n",
        "        'According to the provided information, there is no mention of the department where Michael Johnson works. If you can provide more information about Michael Johnson, I may be able to help you find the answer.',\n",
        "        'Michael Johnson is in the HR department',\n",
        "        'Michael Johnson is in the Course Development Department'\n",
        "    ],\n",
        "    'ground_truth':[\n",
        "        'Michael Johnson is a member of the Course Development Department',\n",
        "        'Michael Johnson is a member of the Course Development Department',\n",
        "        'Michael Johnson is a member of the Course Development Department'\n",
        "    ]\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(data_samples)\n",
        "score = evaluate(\n",
        "    dataset = dataset,\n",
        "    metrics=[answer_correctness],\n",
        "    llm=Tongyi(model_name=\"qwen-plus-0919\"),\n",
        "    embeddings=DashScopeEmbeddings(model=\"text-embedding-v3\")\n",
        ")\n",
        "score.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7aa2370",
      "metadata": {},
      "source": [
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>question</th>\n",
        "      <th>answer</th>\n",
        "      <th>ground_truth</th>\n",
        "      <th>answer_correctness</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>Which department does Michael Johnson belong to?</td>\n",
        "      <td>According to the provided information, there is no mention of the department where Michael Johnson works. If you can provide more information about Michael Johnson, I may be able to help you find the answer.</td>\n",
        "      <td>MMichael Johnson is a member of the Course Development Department</td>\n",
        "      <td>0.168191</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>Which department does Michael Johnson belong to?</td>\n",
        "      <td>Michael Johnson belongs to the Human Resources Department.</td>\n",
        "      <td>Michael Johnson is a member of the Course Development Department</td>\n",
        "      <td>0.496046</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>Which department does Michael Johnson belong to?</td>\n",
        "      <td>Michael Johnson is in the Course Development Department</td>\n",
        "      <td>Michael Johnson is a member of the Course Development Department</td>\n",
        "      <td>0.998264\n",
        "</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55141147",
      "metadata": {},
      "source": [
        "As you can see, Ragas's Answer Correctness metric accurately reflects the performance of the three responses, with the more factually accurate answers receiving higher scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd01530",
      "metadata": {},
      "source": [
        "#### 2.1.2 Understanding the Calculation Process of Answer Correctness\n",
        "\n",
        "Intuitively, the scoring of Answer Correctness aligns with your expectations. The scoring process utilizes a large language models (in the code `llm=Tongyi(model_name=\"qwen-plus\")`) and an embedding model (in the code `embeddings=DashScopeEmbeddings(model=\"text-embedding-v3\")`), calculating the result based on the **semantic similarity** and **factual accuracy** between the answer and ground_truth.\n",
        "\n",
        "##### Semantic Similarity\n",
        "Semantic similarity is obtained by generating text vectors for the answer and ground_truth using the embedding model, then computing the similarity between the two text vectors. There are various methods to calculate vector similarity, such as cosine similarity, Euclidean distance, and Manhattan distance. Ragas uses the most common method, cosine similarity.\n",
        "\n",
        "##### Factual Accuracy\n",
        "\n",
        "Factual accuracy measures the differences in factual descriptions between the answer and ground_truth. For example, consider the following two descriptions:\n",
        "\n",
        "- answer: Michael Johnson is a colleague in the Course Development Department responsible for big data direction.\n",
        "- ground_truth: Michael Johnson is a colleague in the Course Development Department responsible for technical writer tasks.\n",
        "\n",
        "There are factual differences between the answer and ground_truth (work direction), but there are also consistent aspects (work department). Such differences are difficult to quantify through simple calls to a LLMs or embedding model. Ragas generates respective lists of assertions for the answer and ground_truth using a LLMs and compares and calculates the elements within these assertion lists.\n",
        "\n",
        "The following diagram can help you understand how Ragas measures factual accuracy:\n",
        "\n",
        "<a href=\"https://img.alicdn.com/imgextra/i1/O1CN01OKrBYc21eB610hjF3_!!6000000007009-2-tps-1967-347.png\" target=\"_blank\">\n",
        "<img src=\"https://img.alicdn.com/imgextra/i1/O1CN01OKrBYc21eB610hjF3_!!6000000007009-2-tps-1967-347.png\" width=\"1000\">\n",
        "</a>\n",
        "\n",
        "1. Generate respective lists of assertions for the answer and ground_truth using a LLMs. For example:\n",
        "    - **Generate the assertion list for the answer**: Michael Johnson is a colleague in the Course Development Department responsible for big data direction. ---> [\"*Michael Johnson is in the Course Development Department*\", \"*Michael Johnson is responsible for big data direction*\"]\n",
        "    - **Generate the assertion list for ground_truth**: Michael Johnson is a colleague in the Course Development Department responsible for technical writer tasks. ---> [\"*Michael Johnson is in the Course Development Department*\", \"*Michael Johnson is responsible for technical writer tasks*\"]\n",
        "\n",
        "2. Traverse the assertion lists for the answer and ground_truth, initializing three lists: TP, FP, and FN.\n",
        "    - For the assertions generated from the **answer**:\n",
        "      - If the assertion matches one from the ground_truth, add it to the TP list. For example: \"*Michael Johnson is in the Course Development Department*\".\n",
        "      - If the assertion cannot be found in the ground_truth list, add it to the FP list. For example: \"*Michael Johnson is responsible for big data direction*\".\n",
        "    - For the assertions generated from the **ground_truth**:\n",
        "      - If the assertion cannot be found in the answer list, add it to the FN list. For example: \"*Michael Johnson is responsible for technical writer tasks*\".\n",
        "      > The judgment process in this step is entirely provided by the LLMs.\n",
        "\n",
        "3. Count the number of elements in the TP, FP, and FN lists, and calculate the F1 score as follows:\n",
        "\n",
        "\n",
        "\n",
        "```shell\n",
        "f1 score = tp / (tp + 0.5 * (fp + fn)) if tp > 0 else 0\n",
        "```\n",
        "\n",
        "Taking the above text as an example: f1 score = 1/(1+0.5*(1+1)) = 0.5\n",
        "\n",
        "##### Score Summary\n",
        "After obtaining the scores for semantic similarity and factual accuracy, a weighted sum of the two can be calculated to obtain the final Answer Correctness score.  \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Answer Correctness score = 0.25 * Semantic Similarity score + 0.75 * Factual Accuracy score\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe5fa7e6",
      "metadata": {},
      "source": [
        "### 2.2 Evaluating the Effectiveness of Retrieval Recall\n",
        "\n",
        "#### 2.2.1 Quick Start\n",
        "\n",
        "The context precision and context recall metrics in Ragas can be used to evaluate the effectiveness of retrieval recall in RAG applications.\n",
        "\n",
        "- Context precision evaluates whether the entries in the retrieved reference information (contexts) that are relevant to the correct answers are ranked higher and have a high proportion (signal-to-noise ratio), **focusing on relevance**.\n",
        "- Context recall evaluates the factual consistency between contexts and ground_truth, **focusing on factual accuracy**.\n",
        "\n",
        "In practical applications, these two metrics can be used together.\n",
        "\n",
        "To calculate these metrics, your dataset should include the following information:\n",
        "\n",
        "- **question**, the question input to the RAG application.\n",
        "- **contexts**, the retrieved reference information.\n",
        "- **ground_truth**, the correct answer you already know.\n",
        "\n",
        "You can continue using the question \"*Which department is Michael Johnson from?*\" and prepare three sets of data. Run the code below to simultaneously calculate the scores for context precision and context recall.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8fb227cb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e614729e0204e50a5505ab44422db75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>contexts</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Which department is Michael Johnson in?</td>\n",
              "      <td>Based on the provided information, there is no...</td>\n",
              "      <td>Michael Johnson is a member of the Course Deve...</td>\n",
              "      <td>[Provides administrative management and coordi...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Which department is Michael Johnson in?</td>\n",
              "      <td>Michael Johnson is in the HR department</td>\n",
              "      <td>Michael Johnson is a member of the Course Deve...</td>\n",
              "      <td>[Michael Chen, Director of the Course Developm...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Which department is Michael Johnson in?</td>\n",
              "      <td>Michael Johnson is in the Course Development D...</td>\n",
              "      <td>Michael Johnson is a member of the Course Deve...</td>\n",
              "      <td>[Newton discovered the law of universal gravit...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  question  \\\n",
              "0  Which department is Michael Johnson in?   \n",
              "1  Which department is Michael Johnson in?   \n",
              "2  Which department is Michael Johnson in?   \n",
              "\n",
              "                                              answer  \\\n",
              "0  Based on the provided information, there is no...   \n",
              "1            Michael Johnson is in the HR department   \n",
              "2  Michael Johnson is in the Course Development D...   \n",
              "\n",
              "                                        ground_truth  \\\n",
              "0  Michael Johnson is a member of the Course Deve...   \n",
              "1  Michael Johnson is a member of the Course Deve...   \n",
              "2  Michael Johnson is a member of the Course Deve...   \n",
              "\n",
              "                                            contexts  context_recall  \\\n",
              "0  [Provides administrative management and coordi...             1.0   \n",
              "1  [Michael Chen, Director of the Course Developm...             1.0   \n",
              "2  [Newton discovered the law of universal gravit...             1.0   \n",
              "\n",
              "   context_precision  \n",
              "0                0.0  \n",
              "1                0.0  \n",
              "2                0.5  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.llms.tongyi import Tongyi\n",
        "from datasets import Dataset\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import context_recall, context_precision\n",
        "\n",
        "data_samples = {\n",
        "    'question': [\n",
        "        'Which department is Michael Johnson in?',\n",
        "        'Which department is Michael Johnson in?',\n",
        "        'Which department is Michael Johnson in?'\n",
        "    ],\n",
        "    'answer': [\n",
        "        'Based on the provided information, there is no mention of the department where Michael Johnson works. If you can provide more information about Michael Johnson, I may be able to help you find the answer.',\n",
        "        'Michael Johnson is in the HR department',\n",
        "        'Michael Johnson is in the Course Development Department'\n",
        "    ],\n",
        "    'ground_truth': [\n",
        "        'Michael Johnson is a member of the Course Development Department',\n",
        "        'Michael Johnson is a member of the Course Development Department',\n",
        "        'Michael Johnson is a member of the Course Development Department'\n",
        "    ],\n",
        "    'contexts': [\n",
        "        ['Provides administrative management and coordination support, optimizing administrative workflows.', 'Performance Management Department Robert Carter EID-701 Course Development Department'],\n",
        "        ['Michael Chen, Director of the Course Development Department', 'Newton discovered the law of universal gravitation'],\n",
        "        ['Newton discovered the law of universal gravitation', 'Michael Johnson, engineer in the Course Development Department, has recently been responsible for technical writer tasks.'],\n",
        "    ],\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(data_samples)\n",
        "score = evaluate(\n",
        "    dataset=dataset,\n",
        "    metrics=[context_recall, context_precision],\n",
        "    llm=Tongyi(model_name=\"qwen-plus-0919\"))\n",
        "score.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b678b82",
      "metadata": {},
      "source": [
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>question</th>\n",
        "      <th>answer</th>\n",
        "      <th>ground_truth</th>\n",
        "      <th>contexts</th>\n",
        "      <th>context_recall</th>\n",
        "      <th>context_precision</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>Which department is Michael Johnson in?</td>\n",
        "      <td>Based on the provided information, there is no mention of the department where Michael Johnson works. If you can provide more information about Michael Johnson, I may be able to help you find the answer.</td>\n",
        "      <td>Michael Johnson is a member of the Course Development Department.</td>\n",
        "      <td>[Provides administrative management and coordination support, optimizing administrative workflows., Performance Management Department Robert Carter EID-701 Course Development Department]</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>Which department is Michael Johnson in?</td>\n",
        "      <td>Michael Johnson is in the HR department</td>\n",
        "      <td>Michael Johnson is a member of the Course Development Department.</td>\n",
        "      <td>[Michael Chen, Director of the Course Development Department, Newton discovered the law of universal gravitation]</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>Which department is Michael Johnson in?</td>\n",
        "      <td>Michael Johnson is in the Course Development Department</td>\n",
        "      <td>Michael Johnson is a member of the Course Development Department.</td>\n",
        "      <td>[Newton discovered the law of universal gravitation, Michael Johnson, engineer in the Course Development Department, has recently been responsible for curriculum development]</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.5</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab1e7a36",
      "metadata": {},
      "source": [
        "From the data above, we can see that:\n",
        "- The answer in the last row of data is accurate.\n",
        "- The reference materials (contexts) retrieved during the process also contain the correct viewpoint, i.e., \"Michael Johnson belongs to the Course Development Department.\" This situation is reflected in the context recall score being 1.\n",
        "- However, not every piece of information in the contexts is relevant to the question and answer. For example, \"Newton discovered gravity.\" This situation is reflected in the context precision score being 0.5."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1938bbd",
      "metadata": {},
      "source": [
        "#### 2.2.2 Understanding the calculation process of context recall and context precision\n",
        "\n",
        "##### Context Recall\n",
        "\n",
        "You have already learned from the previous text that context recall is a metric used to measure whether contexts are consistent with ground_truth.\n",
        "\n",
        "In Ragas, context recall is used to describe what proportion of viewpoints in ground_truth can be supported by contexts. The calculation process is as follows:\n",
        "\n",
        "1. A large language model (LLM) breaks down the ground_truth into n statements.\n",
        "\n",
        "   For example, from the ground_truth \"Michael Johnson is a member of the Course Development Department,\" a list of statements such as [\"Michael Johnson belongs to the Course Development Department\"] can be generated.\n",
        "2. The LLM determines whether each statement can find supporting evidence in the retrieved reference materials (contexts), or whether the context can support the viewpoint of the ground_truth.\n",
        "\n",
        "   For instance, this statement can find supporting evidence in the third row of data's contexts: \"*Michael Johnson, engineer in the Course Development Department, has recently been responsible for curriculum development.*\"\n",
        "3. Then, the proportion of statements in the ground_truth list that can find supporting evidence in the contexts is calculated as the context_recall score.\n",
        "\n",
        "   Here, the score is 1 = 1/1.\n",
        "\n",
        "##### Context Precision\n",
        "\n",
        "In Ragas, context precision not only measures what proportion of contexts are related to the ground_truth but also evaluates the ranking of contexts. The calculation process is more complex:\n",
        "\n",
        "1. Read through the contexts sequentially, and based on the question and ground_truth, determine whether context<sub>i</sub> is relevant. If relevant, it scores 1; otherwise, it scores 0.\n",
        "\n",
        "   For example, in the third row of data, context<sub>1</sub> (\"Newton discovered gravity\") is irrelevant, while context<sub>2</sub> is relevant.\n",
        "2. For each context, calculate the precision score by dividing the cumulative sum of scores of the current context and all preceding contexts (numerator) by the position of the context in the sequence (denominator).\n",
        "\n",
        "   For the third row of data, the precision score for context<sub>1</sub> is 0/1 = 0, and for context<sub>2</sub>, it is 1/2 = 0.5.\n",
        "3. Sum up the precision scores of all contexts and divide by the number of relevant contexts to obtain the context_precision.\n",
        "\n",
        "   For the third row of data, context_precision = (0 + 0.5) / 1 = 0.5.\n",
        "\n",
        "> If you cannot fully understand the calculation process above, it doesn't matter. You only need to know that this metric evaluates the ranking of contexts. If you're interested, we encourage you to read [Ragas's source code](https://github.com/explodinggradients/ragas/blob/cc31f65d4b7c7cd6bbf686b9073a0dfaacfbcbc5/src/ragas/metrics/_context_precision.py#L250)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d7cc24c",
      "metadata": {},
      "source": [
        "### 2.3 Other Recommended Metrics to Explore\n",
        "\n",
        "Ragas also provides many other metrics, which will not be introduced one by one here. You can visit the Ragas documentation to learn more about the applicable scenarios and working principles of these metrics.\n",
        "\n",
        "The metrics supported by Ragas can be accessed at: https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44edd4a3",
      "metadata": {},
      "source": [
        "## 3. How to Optimize Based on Ragas Metrics\n",
        "The ultimate goal of evaluation is not to obtain scores, but to determine the direction of optimization based on these scores. You have already learned the concepts and calculation methods of three metrics: answer correctness, context recall, and context precision. When you observe that the scores of certain metrics are low, corresponding optimization measures should be formulated.\n",
        "\n",
        "### 3.1 Context Recall\n",
        "The context recall metric evaluates the performance of a RAG application during the **retrieval** phase. If this metric has a low score, you can try optimizing from the following aspects:\n",
        "\n",
        "- **Check the Knowledge Base**\n",
        "\n",
        "    <img src=\"https://wanx.alicdn.com/wanx/1937257750879544/text_to_image_lite_v2/6e4ca1055a0c467992b6b719b443a33e_0.png?x-oss-process=image/watermark,image_aW1nL3dhdGVybWFyazIwMjQxMTEyLnBuZz94LW9zcy1wcm9jZXNzPWltYWdlL3Jlc2l6ZSxtX2ZpeGVkLHdfMzAzLGhfNTI=,t_80,g_se,x_10,y_10/format,webp\" width=\"300\">\n",
        "\n",
        "    The knowledge base is the source of a RAG application. If the content of the knowledge base is incomplete, it will lead to insufficient reference information being recalled, thereby affecting context recall. You can compare the content of the knowledge base with test samples and observe whether the content of the knowledge base can support each test sample (this process can also be assisted by LLMs. If you find that some test samples lack relevant knowledge, you need to supplement the knowledge base.\n",
        "- **Replace the Embedding Model**\n",
        "\n",
        "    <img src=\"https://img.alicdn.com/imgextra/i4/O1CN01MMsV3b1U2GzviZv6y_!!6000000002459-2-tps-991-320.png\" width=\"750\">\n",
        "\n",
        "    If your knowledge base content is already very complete, consider replacing the embedding model. A good embedding model can understand the deep semantic meaning of text. If two sentences are deeply related, even if they don't appear to be related, they can still receive a high similarity score. For example, if the question is \"Who is responsible for curriculum development?\" and the corresponding text segment in the knowledge base is \"Michael Johnson is a member of the Course Development Department,\" despite fewer overlapping words, an excellent embedding model can still assign a high similarity score to these two sentences, thereby recalling the text segment \"Michael Johnson is a member of the Course Development Department.\"\n",
        "- **query rewriting**\n",
        "\n",
        "    <img src=\"https://img.alicdn.com/imgextra/i1/O1CN01RpktVQ1FEtg8r4QCX_!!6000000000456-2-tps-1704-1322.png\" width=\"800\">\n",
        "\n",
        "    As a developer, it is unrealistic to impose too many requirements on how users ask questions. Therefore, you might receive vague questions like: \"Course Development Department,\" \"Leave Request,\" \"Project Management.\" If such questions are directly input into a RAG application, they are unlikely to recall effective text segments. You can design a prompt template by organizing common employee questions and use LLMs to rewrite queries, improving the accuracy of recall.\n",
        "\n",
        "\n",
        "### 3.2 Context Precision\n",
        "Similar to context recall, the context precision metric also evaluates the performance of a RAG application during the **retrieval** phase, but it focuses more on whether related text segments have higher rankings. If this metric has a low score, you can try the optimization measures mentioned under context recall, and you can also attempt to add **reranking** during the retrieval phase to improve the ranking of related text segments.\n",
        "\n",
        "\n",
        "### 3.3 Answer Correctness\n",
        "The answer correctness metric evaluates the overall comprehensive performance of a RAG system. If this metric has a low score while the previous two metrics have high scores, it indicates that the RAG system performs well in the **retrieval** phase but encounters issues in the **generation** phase. You can try the methods learned in previous tutorials, such as optimizing prompts, adjusting hyperparameters (such as temperature) of LLMs generation, or replacing with a more powerful LLMs, and even fine-tuning the LLMs (which will be introduced in later tutorials) to enhance the accuracy of generated answers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "448a70bb-47c0-4933-9ed5-acd3072d18cc",
      "metadata": {},
      "source": [
        "## ‚úÖ Summary of this section\n",
        "\n",
        "Through the study of this section, you have learned how to establish automated testing for RAG chatbots.\n",
        "\n",
        "Automated testing is an important means of engineering optimization. With quantified automated testing, it can help you shift from **feeling** better to metrics **quantization** that the application performs better when improving your RAG chatbot. This not only helps you evaluate the question-answering quality of the RAG chatbot faster and find optimization directions, but also quantifies the optimization results you have achieved.\n",
        "\n",
        "Of course, having automated testing does not mean that you no longer need human evaluation at all. It is recommended that in practical applications, you invite domain experts corresponding to the RAG chatbot to jointly build a test set that reflects the distribution of real-world scenario problems, and continuously update the test set.\n",
        "\n",
        "At the same time, since LLMs cannot always achieve 100% accuracy, it is also recommended that you regularly sample and evaluate the accuracy of automated testing results during actual use, and try not to frequently change the LLMs and embedding models. For Ragas, you can improve its performance by adjusting the prompts in the default evaluation method (for example, supplementing reference examples related to your business domain) (for more details, please refer to the extended reading).  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7745583",
      "metadata": {},
      "source": [
        "## Further Reading\n",
        "\n",
        "### Changing the Prompt Template in Ragas\n",
        "Many of Ragas's evaluation metrics rely on large language models for implementation. Like LlamaIndex, Ragas provides default prompt templates but also supports custom prompts that you can modify to suit your needs.\n",
        "\n",
        "We have included example prompt templates in the `ragas_prompt` folder to help you customize the prompts used by Ragas for different evaluation metrics. You can refer to the following code to integrate these updated prompts into your workflow.\n",
        "\n",
        "> Note: Ragas includes example cases in its prompts to guide the model on how to make judgments or generate lists of statements. You can replace or modify these examples to better align with your specific use case or domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b8d949c1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ea98c9c84964eb78a38d4c7056d2a7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>contexts</th>\n",
              "      <th>answer_correctness</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Which department is Michael Johnson in?</td>\n",
              "      <td>Based on the provided information, there is no...</td>\n",
              "      <td>Michael Johnson is a member of the Course Deve...</td>\n",
              "      <td>[Provides administrative management and coordi...</td>\n",
              "      <td>0.166901</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Which department is Michael Johnson in?</td>\n",
              "      <td>Michael Johnson is in the HR department</td>\n",
              "      <td>Michael Johnson is a member of the Course Deve...</td>\n",
              "      <td>[Li Kai, Director of the Course Development De...</td>\n",
              "      <td>0.196046</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Which department is Michael Johnson in?</td>\n",
              "      <td>Michael Johnson is in the Course Development D...</td>\n",
              "      <td>Michael Johnson is a member of the Course Deve...</td>\n",
              "      <td>[Newton discovered the law of universal gravit...</td>\n",
              "      <td>0.998264</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  question  \\\n",
              "0  Which department is Michael Johnson in?   \n",
              "1  Which department is Michael Johnson in?   \n",
              "2  Which department is Michael Johnson in?   \n",
              "\n",
              "                                              answer  \\\n",
              "0  Based on the provided information, there is no...   \n",
              "1            Michael Johnson is in the HR department   \n",
              "2  Michael Johnson is in the Course Development D...   \n",
              "\n",
              "                                        ground_truth  \\\n",
              "0  Michael Johnson is a member of the Course Deve...   \n",
              "1  Michael Johnson is a member of the Course Deve...   \n",
              "2  Michael Johnson is a member of the Course Deve...   \n",
              "\n",
              "                                            contexts  answer_correctness  \\\n",
              "0  [Provides administrative management and coordi...            0.166901   \n",
              "1  [Li Kai, Director of the Course Development De...            0.196046   \n",
              "2  [Newton discovered the law of universal gravit...            0.998264   \n",
              "\n",
              "   context_recall  context_precision  \n",
              "0             0.0                0.0  \n",
              "1             1.0                0.0  \n",
              "2             1.0                0.5  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import prompt templates\n",
        "from ragas_prompt.ragas_test_prompt import ContextRecall, ContextPrecision, AnswerCorrectness\n",
        "\n",
        "# Customize prompt settings for each metric\n",
        "context_recall.context_recall_prompt.instruction = ContextRecall.context_recall_prompt[\"instruction\"]\n",
        "context_recall.context_recall_prompt.output_format_instruction = ContextRecall.context_recall_prompt[\"output_format_instruction\"]\n",
        "context_recall.context_recall_prompt.examples = ContextRecall.context_recall_prompt[\"examples\"]\n",
        "\n",
        "context_precision.context_precision_prompt.instruction = ContextPrecision.context_precision_prompt[\"instruction\"]\n",
        "context_precision.context_precision_prompt.output_format_instruction = ContextPrecision.context_precision_prompt[\"output_format_instruction\"]\n",
        "context_precision.context_precision_prompt.examples = ContextPrecision.context_precision_prompt[\"examples\"]\n",
        "\n",
        "answer_correctness.correctness_prompt.instruction = AnswerCorrectness.correctness_prompt[\"instruction\"]\n",
        "answer_correctness.correctness_prompt.output_format_instruction = AnswerCorrectness.correctness_prompt[\"output_format_instruction\"]\n",
        "answer_correctness.correctness_prompt.examples = AnswerCorrectness.correctness_prompt[\"examples\"]\n",
        "\n",
        "data_samples = {\n",
        "    'question': [\n",
        "        'Which department is Michael Johnson in?',\n",
        "        'Which department is Michael Johnson in?',\n",
        "        'Which department is Michael Johnson in?'\n",
        "    ],\n",
        "    'answer': [\n",
        "        'Based on the provided information, there is no mention of the department where Michael Johnson works. If you can provide more information about Michael Johnson, I may be able to help you find the answer.',\n",
        "        'Michael Johnson is in the HR department',\n",
        "        'Michael Johnson is in the Course Development Department'\n",
        "    ],\n",
        "    'ground_truth': [\n",
        "        'Michael Johnson is a member of the Course Development Department',\n",
        "        'Michael Johnson is a member of the Course Development Department',\n",
        "        'Michael Johnson is a member of the Course Development Department'\n",
        "    ],\n",
        "    'contexts': [\n",
        "        ['Provides administrative management and coordination support, optimizing administrative workflows.', 'Performance Management Department Han Shan Li Fei I902 041 Human Resources'],\n",
        "        ['Li Kai, Director of the Course Development Department', 'Newton discovered the law of universal gravitation'],\n",
        "        ['Newton discovered the law of universal gravitation', 'Michael Johnson, engineer in the Course Development Department, has recently been responsible for curriculum development.'],\n",
        "    ],\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(data_samples)\n",
        "\n",
        "score = evaluate(\n",
        "    dataset=dataset,\n",
        "    metrics=[answer_correctness, context_recall, context_precision],\n",
        "    llm=Tongyi(model_name=\"qwen-plus-0919\"),\n",
        "    embeddings=DashScopeEmbeddings(model=\"text-embedding-v3\"))\n",
        "\n",
        "score.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c1dc0f4",
      "metadata": {},
      "source": [
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>question</th>\n",
        "      <th>answer</th>\n",
        "      <th>ground_truth</th>\n",
        "      <th>contexts</th>\n",
        "      <th>answer_correctness</th>\n",
        "      <th>context_recall</th>\n",
        "      <th>context_precision</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>Which department is Michael Johnson in?</td>\n",
        "      <td>Based on the provided information, there is no mention of the department where Michael Johnson works. If you can provide more information about Michael Johnson, I may be able to help you find the answer.</td>\n",
        "      <td>Michael Johnson is a member of the Course Development Department.</td>\n",
        "      <td>[Providing administrative management and coordination support, optimizing administrative workflows. , Performance Management Department Han Shan Li Fei I902 041 ...</td>\n",
        "      <td>0.166901</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>Which department is Michael Johnson in?</td>\n",
        "      <td>Michael Johnson is in the Human Resources Department.</td>\n",
        "      <td>Michael Johnson is a member of the Course Development Department.</td>\n",
        "      <td>[Li Kai, Director of the Course Development Department , Newton discovered the law of universal gravitation]</td>\n",
        "      <td>0.196046</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>Which department is Michael Johnson in?</td>\n",
        "      <td>Michael Johnson is in the Course Development Department.</td>\n",
        "      <td>Michael Johnson is a member of the Course Development Department.</td>\n",
        "      <td>[Newton discovered the law of universal gravitation, Michael Johnson, an engineer in the Course Development Department, has recently been responsible for curriculum development]</td>\n",
        "      <td>0.998264</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.5</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b35c832",
      "metadata": {},
      "source": [
        "### More Evaluation Metrics\n",
        "In addition to RAG, there are many applications or tasks of LLMs or natural language processing (NLP), such as Agents, natural language to SQL, machine translation, summarization, etc. Ragas provides many metrics that can be used to evaluate these tasks.\n",
        "\n",
        "\n",
        "| Evaluation Metric            | Use Case | Metric Meaning                                                                 |\n",
        "|---------------------|----------|--------------------------------------------------------------------------|\n",
        "| [ToolCallAccuracy](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/agents/#example)    | Agent    | Evaluates the LLM's performance in identifying and invoking tools required to complete specific tasks. This metric is obtained by comparing reference tool calls with tool calls made by the LLM, with a value range of 0-1. |\n",
        "| [DataCompyScore](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/sql/)      | natural language to SQL   | Evaluates the difference between the results obtained from database queries using SQL statements generated by the LLM and the correct results. The value ranges from 0 to 1.                     |\n",
        "| [LLMSQLEquivalence](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/sql/#non-execution-based-metrics)   | natural language to SQL   | Unlike the previous metric, this does not require actual database retrieval; it only evaluates the differences between SQL statements generated by the LLM and the correct SQL statements. The value ranges from 0 to 1.   |\n",
        "| [BleuScore](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/traditional/#bleu-score)           | General     | Evaluates the similarity between responses and correct answers based on n-grams. Initially designed for evaluating machine translation systems, this metric does not require the use of an LLM during evaluation, and its value ranges from 0 to 1. In the [2.7 tutorial](2_7_Improve_Model_Accuracy_and_Efficiency_via_Fine_Tuning.ipynb), you will learn how to fine-tune LLMs, and BleuScore can be used to measure the benefits brought by fine-tuning.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0ed74ea",
      "metadata": {},
      "source": [
        "## üî• Post-class Quiz\n",
        "\n",
        "### üîç Single-choice Question\n",
        "<details>\n",
        "<summary style=\"cursor: pointer; padding: 12px;  border: 1px solid #dee2e6; border-radius: 6px;\">\n",
        "<b>What does the Context Precision metric measure? ‚ùì</b>\n",
        "\n",
        "- A. Evaluation of overall response quality\n",
        "- B. Evaluation of whether retrieved text segments relevant to the question are ranked higher\n",
        "- C. Whether the generated answer is related to the retrieved text segments\n",
        "- D. Whether the generated answer is relevant to the question\n",
        "\n",
        "**[Click to view the answer]**\n",
        "</summary>\n",
        "\n",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
        "\n",
        "‚úÖ **Reference Answer: B**  \n",
        "üìù **Explanation**:  \n",
        "- Context Precision directly evaluates the ranking quality of retrieval results, not the relevance of the content itself or the quality of the final answer.\n",
        "\n",
        "</div>\n",
        "</details>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "447d3e8f",
      "metadata": {},
      "source": [
        "## ‚úâÔ∏è Evaluation and Feedback\n",
        "Thank you for studying the Alibaba Cloud Large Model ACP Certification course. If you think there are parts of the course that are well-written or need improvement, we look forward to your [evaluation and feedback through this questionnaire](https://survey.aliyun.com/apps/zhiliao/Mo5O9vuie).\n",
        "\n",
        "Your criticism and encouragement are both driving forces for our progress.  \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
