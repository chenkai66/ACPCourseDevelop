{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "39f10553-d93f-496b-a5c1-7e224fbb7f0f",
      "metadata": {},
      "source": [
        "# 2.4 Automated Evaluation of the Q&A Robot's Performance\n",
        "\n",
        "## üöÑ Preface\n",
        "\n",
        "The new Q&A robot may encounter some issues during actual use. For example, when a newcomer asks \"how to request leave,\" the robot might provide a generic response instead of answering based on the content of policy documents.\n",
        "\n",
        "Just as conventional software development requires testing, you should also establish an evaluation system in your Q&A robot project to ensure that similar issues can be quickly diagnosed. Additionally, after optimizing for a specific issue, you should test a batch of questions to confirm that the optimization positively impacts the overall performance of the Q&A robot.\n",
        "\n",
        "\n",
        "## üçÅ Course Objectives\n",
        "After completing this course, you will be able to:\n",
        "\n",
        "* How to automate evaluations for large language models (LLMs) applications.\n",
        "* How to evaluate RAG chatbot using Ragas.\n",
        "* How to identify and solve problems through Ragas scores.\n",
        "\n",
        "<!-- ## üìñ Course Outline\n",
        "In this chapter, we will first understand some current issues with RAG chatbot through a specific problem. Then, we will attempt to discover the issue by implementing a simple automated test ourselves. Finally, we will learn how to use the more mature RAG application testing framework, Ragas, to assess the performance of RAG chatbot.\n",
        "\n",
        "- 1.&nbsp;Evaluating RAG Application Performance\n",
        "    - 1.1 Issues with the Q&A robot\n",
        "    - 1.2 Checking RAG retrieval results to troubleshoot issues\n",
        "    - 1.3 Attempting to establish an automated testing mechanism\n",
        "\n",
        "- 2.&nbsp;Using Ragas to Evaluate Application Performance\n",
        "     - 2.1 Evaluating the quality of responses from the RAG chatbot\n",
        "       - 2.1.1 Quick start\n",
        "       - 2.1.2 Understanding the calculation process of answer correctness\n",
        "     - 2.2 Evaluating the recall effectiveness of retrieval\n",
        "       - 2.2.1 Quick start\n",
        "       - 2.2.2 Understanding the calculation process of context recall and context precision -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c917f6bc",
      "metadata": {},
      "source": [
        "## 1. Evaluating RAG Application Performance\n",
        "\n",
        "### 1.1 Issues with the Q&A Bot\n",
        "\n",
        "In the previous chapter, you completed the development of a Q&A bot, but you noticed that it currently performs poorly on employee query-related questions.\n",
        "\n",
        "For example, Zhang Wei is the first employee in the employee information table, but your Q&A bot cannot answer the question \"Which department does Zhang Wei belong to?\"\n",
        "\n",
        "<a href=\"https://img.alicdn.com/imgextra/i1/O1CN01MsuZGI1E0rrkVNNnO_!!6000000000290-0-tps-1626-278.jpg\" target=\"_blank\">\n",
        "<img src=\"https://img.alicdn.com/imgextra/i1/O1CN01MsuZGI1E0rrkVNNnO_!!6000000000290-0-tps-1626-278.jpg\" width=\"600\">\n",
        "</a>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "33d09a89",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your configured API Key is: sk-4b*****\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from config.load_key import load_key\n",
        "load_key()\n",
        "print(f\"Your configured API Key is: {os.environ[\"DASHSCOPE_API_KEY\"][:5]+\"*\"*5}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "947e89be",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Which department is Zhang Wei in?\n",
            "Answer: Zhang Wei is in the Teaching Research Department."
          ]
        }
      ],
      "source": [
        "from chatbot import rag\n",
        "query_engine = rag.create_query_engine(rag.load_index())\n",
        "print('Question: Which department is Zhang Wei in?')\n",
        "response = query_engine.query('Which department is Zhang Wei in?')\n",
        "print('Answer: ', end='')\n",
        "response.print_response_stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "793fa40e",
      "metadata": {},
      "source": [
        "### 1.2. Check RAG Retrieval Results for Problem Diagnosis\n",
        "To resolve this issue, you need to confirm whether the reference materials retrieved before your RAG chatbot answers the question contain relevant information about Zhang Wei.\n",
        "\n",
        "Using the following code, you can obtain the reference information retrieved by the RAG chatbot when answering this question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79691f6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "contexts = [node.get_content() for node in response.source_nodes]\n",
        "contexts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48bbcc22",
      "metadata": {},
      "source": [
        "This shows that the problem is caused by poor retrieval performance.\n",
        "\n",
        "> This chapter will focus on building automated testing, and this retrieval performance issue will be addressed in subsequent chapters.\n",
        "\n",
        "### 1.3. Attempt to Establish an Automated Testing Mechanism\n",
        "\n",
        "Although you can always find a way to locate the problem, it would be very time-consuming if you had to confirm every time whether it was due to retrieval errors or correct retrieval but incorrect model-generated answers. You should establish a testing mechanism that can automatically test a batch of questions you have prepared.\n",
        "\n",
        "In previous studies, you already know that large language models (LLMs) can be used to answer questions and check for errors. Similarly, LLMs can also be used to detect whether the responses from the Q&A bot accurately answer the question, as long as reference information is provided in the prompt and the response format is restricted.\n",
        "\n",
        "The `test_answer` function below can be used to check whether the Q&A bot's response effectively answers the question. You need to input the question and the Q&A bot‚Äôs response into the prompt and restrict the response format to: \"Only valid response or invalid response\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9a20f5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from chatbot import llm\n",
        "\n",
        "def test_answer(question, answer):\n",
        "    prompt = (\"You are a tester.\\n\"\n",
        "        \"You need to check whether the following answer effectively responds to the user's question.\\n\"\n",
        "        \"The reply can only be: Valid response or Invalid response. Do not provide any other information.\\n\"\n",
        "        \"------\"\n",
        "        f\"The answer is {answer}\"\n",
        "        \"------\"\n",
        "        f\"The question is: {question}\"\n",
        "    )\n",
        "    return llm.invoke(prompt,model_name=\"qwen-max\")\n",
        "\n",
        "\n",
        "test_answer(\"Which department is Zhang Wei in?\", \"According to the provided information, there is no mention of the department where Zhang Wei works. If you can provide more information about Zhang Wei, I may be able to help you find the answer.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7c22a73",
      "metadata": {},
      "source": [
        "The response provided does not effectively answer the question 'Which department is Zhang Wei in?' The large language models (LLMs)'s reply of 'invalid response' is consistent with expectations.\n",
        "\n",
        "In RAG chatbot, apart from the effectiveness of responses, you also need to ensure that the retrieved reference information is useful. The `test_contexts` function below can be used to check whether the retrieved reference information is effective. You need to pass the question and the retrieved reference information into the prompt and restrict the response format to: 'Only: Reference information is useful or Reference information is not useful'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e20f11a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_contexts(question, contexts):\n",
        "    prompt = (\"You are a tester.\\n\"\n",
        "        \"You need to check whether the following reference materials can help answer the question.\\n\"\n",
        "        \"The response can only be: The reference information is useful or The reference information is not useful. Do not provide any other information.\\n\"\n",
        "        \"------\"\n",
        "        f\"The reference material is {contexts}\"\n",
        "        \"------\"\n",
        "        f\"The question is: {question}\"\n",
        "    )\n",
        "    return llm.invoke(prompt,model_name=\"qwen-max\")\n",
        "test_contexts(\"Which department is Zhang Wei in?\", \"Core, providing administrative management and coordination support, optimizing administrative workflows. \\nAdministrative Department Qin Fei Cai Jing G705 034 Administration Administrative Specialist 13800000034 qinf@educompany.com Maintaining company archives and information systems, responsible for issuing company notices and announcements,\\n\\nSupport. \\nPerformance Management Department Han Shan Li Fei I902 041 Human Resources Performance Specialist 13800000041 hanshan@educompany.com Establishing and maintaining employee performance records, regularly organizing performance review meetings, coordinating feedback from various departments, formulating evaluation processes and standards, ensuring performance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e550e2",
      "metadata": {},
      "source": [
        "With the two methods above, you have already preliminarily set up a prototype of a large language models (LLMs) testing project. However, the current implementation is still incomplete. For instance:\n",
        "\n",
        "- Because large language models (LLMs) sometimes hallucination, their answers may appear convincingly real. In such cases, the `test_answer` method cannot effectively detect this issue.\n",
        "- The higher the proportion of relevant information in the retrieved references, the better (signal-to-noise ratio). However, our current testing method is relatively simple and does not take these factors into account.\n",
        "\n",
        "You might consider using some mature testing frameworks to further improve your testing project. For example, [Ragas](https://docs.ragas.io/en/stable), which is a testing framework specifically designed to evaluate the performance of RAG chatbot."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a0e7e9",
      "metadata": {},
      "source": [
        "## 2. Using Ragas to Evaluate Application Performance\n",
        "\n",
        "Ragas provides multiple metrics that can be used to evaluate the quality of question-answering across the entire chain of an application. For example:\n",
        "\n",
        "- Evaluation of overall response quality:\n",
        "  - Answer Correctness, used to assess the accuracy of answers generated by the RAG application.\n",
        "- Evaluation of the generation phase:\n",
        "  - Answer Relevancy, used to assess whether the answers generated by the RAG application are relevant to the question.\n",
        "  - Faithfulness, used to evaluate the factual consistency between the answers generated by the RAG application and the retrieved reference materials.\n",
        "- Evaluation of the recall phase:\n",
        "  - Context Precision, used to evaluate whether entries related to the correct answer in contexts are ranked high and have a high proportion (signal-to-noise ratio).\n",
        "  - Context Recall, used to evaluate how many relevant reference materials are retrieved; a higher score means fewer relevant references are missed.\n",
        "\n",
        "<a href=\"https://img.alicdn.com/imgextra/i4/O1CN01b2lVQp21JZCJy6Nfe_!!6000000006964-0-tps-739-420.jpg\" target=\"_blank\">\n",
        "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01b2lVQp21JZCJy6Nfe_!!6000000006964-0-tps-739-420.jpg\" width=\"500\">\n",
        "</a>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d819011",
      "metadata": {},
      "source": [
        "### 2.1 Evaluating the Response Quality of RAG Applications\n",
        "\n",
        "#### 2.1.1 Quick Start  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86554952",
      "metadata": {},
      "source": [
        "When evaluating the overall response quality of a RAG chatbot, using Ragas' Answer Correctness is an excellent metric. To calculate this metric, you need to prepare the following two types of data to evaluate the quality of the answer generated by the RAG chatbot:\n",
        "1. question (The question input to the RAG chatbot)\n",
        "2. ground_truth (The correct answer you already know)\n",
        "\n",
        "To illustrate the differences in evaluation metrics for different responses, we have prepared three sets of RAG chatbot responses to the question \"*Which department does Zhang Wei belong to?\":\n",
        "\n",
        "| question            | ground_truth     | answer                                                                 |\n",
        "|---------------------|------------------|------------------------------------------------------------------------|\n",
        "| Which department does Zhang Wei belong to?    | Zhang Wei belongs to the Teaching and Research Department. | Based on the provided information, there is no mention of the department Zhang Wei belongs to. If you can provide more information about Zhang Wei, I may be able to help you find the answer. (Invalid answer) | \n",
        "| Which department does Zhang Wei belong to?    | Zhang Wei belongs to the Teaching and Research Department. | Zhang Wei belongs to the Human Resources Department. (hallucination)                                                     |\n",
        "| Which department does Zhang Wei belong to?    | Zhang Wei belongs to the Teaching and Research Department. | Zhang Wei belongs to the Teaching and Research Department. (Correct)                                                       |\n",
        "\n",
        "We can then run the following code to calculate the score for response accuracy (i.e., Answer Correctness)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39940304",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.llms.tongyi import Tongyi\n",
        "from langchain_community.embeddings import DashScopeEmbeddings\n",
        "from datasets import Dataset\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import answer_correctness\n",
        "\n",
        "data_samples = {\n",
        "    'question': [\n",
        "        'Which department is Zhang Wei in?',\n",
        "        'Which department is Zhang Wei in?',\n",
        "        'Which department is Zhang Wei in?'\n",
        "    ],\n",
        "    'answer': [\n",
        "        'According to the provided information, there is no mention of the department where Zhang Wei works. If you can provide more information about Zhang Wei, I may be able to help you find the answer.',\n",
        "        'Zhang Wei is in the HR department',\n",
        "        'Zhang Wei is in the Teaching and Research Department'\n",
        "    ],\n",
        "    'ground_truth':[\n",
        "        'Zhang Wei is a member of the Teaching and Research Department',\n",
        "        'Zhang Wei is a member of the Teaching and Research Department',\n",
        "        'Zhang Wei is a member of the Teaching and Research Department'\n",
        "    ]\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(data_samples)\n",
        "score = evaluate(\n",
        "    dataset = dataset,\n",
        "    metrics=[answer_correctness],\n",
        "    llm=Tongyi(model_name=\"qwen-plus-0919\"),\n",
        "    embeddings=DashScopeEmbeddings(model=\"text-embedding-v3\")\n",
        ")\n",
        "score.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7aa2370",
      "metadata": {},
      "source": [
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>question</th>\n",
        "      <th>answer</th>\n",
        "      <th>ground_truth</th>\n",
        "      <th>answer_correctness</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>Which department does Zhang Wei belong to?</td>\n",
        "      <td>Based on the provided information, there is no mention of the department Zhang Wei belongs to. If you can provide more information about Zhang Wei, I may be able to help you find the answer.</td>\n",
        "      <td>Zhang Wei is a member of the Teaching and Research Department.</td>\n",
        "      <td>0.175227</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>Which department does Zhang Wei belong to?</td>\n",
        "      <td>Zhang Wei belongs to the Human Resources Department.</td>\n",
        "      <td>Zhang Wei is a member of the Teaching and Research Department.</td>\n",
        "      <td>0.193980</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>Which department does Zhang Wei belong to?</td>\n",
        "      <td>Zhang Wei belongs to the Teaching and Research Department.</td>\n",
        "      <td>Zhang Wei is a member of the Teaching and Research Department.</td>\n",
        "      <td>0.994619</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55141147",
      "metadata": {},
      "source": [
        "As you can see, Ragas's Answer Correctness metric accurately reflects the performance of the three responses, with the more factually accurate answers receiving higher scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd01530",
      "metadata": {},
      "source": [
        "#### 2.1.2 Understanding the Calculation Process of Answer Correctness\n",
        "\n",
        "Intuitively, the scoring of Answer Correctness aligns with your expectations. The scoring process utilizes a large language models (LLMs)s (LLMs) (in the code `llm=Tongyi(model_name=\"qwen-plus\")`) and an embedding model (in the code `embeddings=DashScopeEmbeddings(model=\"text-embedding-v3\")`), calculating the result based on the **semantic similarity** and **factual accuracy** between the answer and ground_truth.\n",
        "\n",
        "##### Semantic Similarity\n",
        "Semantic similarity is obtained by generating text vectors for the answer and ground_truth using the embedding model, then computing the similarity between the two text vectors. There are various methods to calculate vector similarity, such as cosine similarity, Euclidean distance, and Manhattan distance. Ragas uses the most common method, cosine similarity.\n",
        "\n",
        "##### Factual Accuracy\n",
        "\n",
        "Factual accuracy measures the differences in factual descriptions between the answer and ground_truth. For example, consider the following two descriptions:\n",
        "\n",
        "- answer: Zhang Wei is a colleague in the teaching and research department responsible for large language models (LLMs)s (LLMs) courses.\n",
        "- ground_truth: Zhang Wei is a colleague in the teaching and research department responsible for big data direction.\n",
        "\n",
        "There are factual differences between the answer and ground_truth (work direction), but there are also consistent aspects (work department). Such differences are difficult to quantify through simple calls to a large language models (LLMs)s (LLMs) or embedding model. Ragas generates respective lists of assertions for the answer and ground_truth using a large language models (LLMs)s (LLMs) and compares and calculates the elements within these assertion lists.\n",
        "\n",
        "The following diagram can help you understand how Ragas measures factual accuracy:\n",
        "\n",
        "<a href=\"https://img.alicdn.com/imgextra/i3/O1CN01NXmu4B1xpOGyZDDdB_!!6000000006492-0-tps-2382-1186.jpg\" target=\"_blank\">\n",
        "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01NXmu4B1xpOGyZDDdB_!!6000000006492-0-tps-2382-1186.jpg\" width=\"600\">\n",
        "</a>\n",
        "\n",
        "1. Generate respective lists of assertions for the answer and ground_truth using a large language models (LLMs)s (LLMs). For example:\n",
        "    - **Generate the assertion list for the answer**: Zhang Wei is a colleague in the teaching and research department responsible for large language models (LLMs)s (LLMs) courses. ---> [\"*Zhang Wei is in the teaching and research department*\", \"*Zhang Wei is responsible for large language models (LLMs)s (LLMs) courses*\"]\n",
        "    - **Generate the assertion list for ground_truth**: Zhang Wei is a colleague in the teaching and research department responsible for big data direction. ---> [\"*Zhang Wei is in the teaching and research department*\", \"*Zhang Wei is responsible for big data direction*\"]\n",
        "\n",
        "2. Traverse the assertion lists for the answer and ground_truth, initializing three lists: TP, FP, and FN.\n",
        "    - For the assertions generated from the **answer**:\n",
        "      - If the assertion matches one from the ground_truth, add it to the TP list. For example: \"*Zhang Wei is in the teaching and research department*\".\n",
        "      - If the assertion cannot be found in the ground_truth list, add it to the FP list. For example: \"*Zhang Wei is responsible for large language models (LLMs)s (LLMs) courses*\".\n",
        "    - For the assertions generated from the **ground_truth**:\n",
        "      - If the assertion cannot be found in the answer list, add it to the FN list. For example: \"*Zhang Wei is responsible for big data direction*\".\n",
        "      <!-- ![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/ybEnBBXZ6LoPnP13/img/86c4839b-bd48-4faf-9842-c42599181339.png) -->\n",
        "      > The judgment process in this step is entirely provided by the large language models (LLMs)s (LLMs).\n",
        "\n",
        "3. Count the number of elements in the TP, FP, and FN lists, and calculate the F1 score as follows:\n",
        "\n",
        "\n",
        "\n",
        "```shell\n",
        "f1 score = tp / (tp + 0.5 * (fp + fn)) if tp > 0 else 0\n",
        "```\n",
        "\n",
        "Taking the above text as an example: f1 score = 1/(1+0.5*(1+1)) = 0.5\n",
        "\n",
        "##### Score Summary\n",
        "After obtaining the scores for semantic similarity and factual accuracy, a weighted sum of the two can be calculated to obtain the final Answer Correctness score.  \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Answer Correctness score = 0.25 * Semantic Similarity score + 0.75 * Factual Accuracy score\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe5fa7e6",
      "metadata": {},
      "source": [
        "### 2.2 Evaluating the Effectiveness of Retrieval Recall\n",
        "\n",
        "#### 2.2.1 Quick Start\n",
        "\n",
        "The context precision and context recall metrics in Ragas can be used to evaluate the effectiveness of retrieval recall in RAG applications.\n",
        "\n",
        "- Context precision evaluates whether the entries in the retrieved reference information (contexts) that are relevant to the correct answers are ranked higher and have a high proportion (signal-to-noise ratio), **focusing on relevance**.\n",
        "- Context recall evaluates the factual consistency between contexts and ground_truth, **focusing on factual accuracy**.\n",
        "\n",
        "In practical applications, these two metrics can be used together.\n",
        "\n",
        "To calculate these metrics, your dataset should include the following information:\n",
        "\n",
        "- **question**, the question input to the RAG application.\n",
        "- **contexts**, the retrieved reference information.\n",
        "- **ground_truth**, the correct answer you already know.\n",
        "\n",
        "You can continue using the question \"*Which department is Zhang Wei from?*\" and prepare three sets of data. Run the code below to simultaneously calculate the scores for context precision and context recall.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb227cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.llms.tongyi import Tongyi\n",
        "from datasets import Dataset\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import context_recall, context_precision\n",
        "\n",
        "data_samples = {\n",
        "    'question': [\n",
        "        'Which department is Zhang Wei in?',\n",
        "        'Which department is Zhang Wei in?',\n",
        "        'Which department is Zhang Wei in?'\n",
        "    ],\n",
        "    'answer': [\n",
        "        'Based on the provided information, there is no mention of the department where Zhang Wei works. If you can provide more information about Zhang Wei, I may be able to help you find the answer.',\n",
        "        'Zhang Wei is in the HR department',\n",
        "        'Zhang Wei is in the Teaching and Research Department'\n",
        "    ],\n",
        "    'ground_truth': [\n",
        "        'Zhang Wei is a member of the Teaching and Research Department',\n",
        "        'Zhang Wei is a member of the Teaching and Research Department',\n",
        "        'Zhang Wei is a member of the Teaching and Research Department'\n",
        "    ],\n",
        "    'contexts': [\n",
        "        ['Provides administrative management and coordination support, optimizing administrative workflows.', 'Performance Management Department Han Shan Li Fei I902 041 Human Resources'],\n",
        "        ['Li Kai, Director of the Teaching and Research Department', 'Newton discovered the law of universal gravitation'],\n",
        "        ['Newton discovered the law of universal gravitation', 'Zhang Wei, engineer in the Teaching and Research Department, has recently been responsible for curriculum development'],\n",
        "    ],\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(data_samples)\n",
        "score = evaluate(\n",
        "    dataset=dataset,\n",
        "    metrics=[context_recall, context_precision],\n",
        "    llm=Tongyi(model_name=\"qwen-plus-0919\"))\n",
        "score.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b678b82",
      "metadata": {},
      "source": [
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>question</th>\n",
        "      <th>answer</th>\n",
        "      <th>ground_truth</th>\n",
        "      <th>contexts</th>\n",
        "      <th>context_recall</th>\n",
        "      <th>context_precision</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>Which department is Zhang Wei in?</td>\n",
        "      <td>Based on the information provided, there is no mention of the department where Zhang Wei works. If you can provide more information about Zhang Wei, I may be able to help you find the answer.</td>\n",
        "      <td>Zhang Wei is a member of the Teaching and Research Department.</td>\n",
        "      <td>[Providing administrative management and coordination support, optimizing administrative workflows. , Performance Management Department Han Shan Li Fei I902 041 ...</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>Which department is Zhang Wei in?</td>\n",
        "      <td>Zhang Wei is in the Human Resources Department.</td>\n",
        "      <td>Zhang Wei is a member of the Teaching and Research Department.</td>\n",
        "      <td>[Li Kai, Director of the Teaching and Research Department , Newton discovered the law of universal gravitation]</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>Which department is Zhang Wei in?</td>\n",
        "      <td>Zhang Wei is in the Teaching and Research Department.</td>\n",
        "      <td>Zhang Wei is a member of the Teaching and Research Department.</td>\n",
        "      <td>[Newton discovered the law of universal gravitation, Zhang Wei, an engineer in the Teaching and Research Department, has recently been responsible for curriculum development]</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.5</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1938bbd",
      "metadata": {},
      "source": [
        "From the data above, we can see that:\n",
        "- The answer in the last row of data is accurate.\n",
        "- The reference materials (contexts) retrieved during the process also contain the correct viewpoint, i.e., \"Zhang Wei belongs to the teaching and research department.\" This situation is reflected in the context recall score being 1.\n",
        "- However, not every piece of information in the contexts is relevant to the question and answer. For example, \"Newton discovered gravity.\" This situation is reflected in the context precision score being 0.5.\n",
        "\n",
        "#### 2.2.2 Understanding the calculation process of context recall and context precision\n",
        "\n",
        "##### Context Recall\n",
        "\n",
        "You have already learned from the previous text that context recall is a metric used to measure whether contexts are consistent with ground_truth.\n",
        "\n",
        "In Ragas, context recall is used to describe what proportion of viewpoints in ground_truth can be supported by contexts. The calculation process is as follows:\n",
        "\n",
        "1. A large language model (LLM) breaks down the ground_truth into n statements.\n",
        "\n",
        "   For example, from the ground_truth \"Zhang Wei is a member of the teaching and research department,\" a list of statements such as [\"Zhang Wei belongs to the teaching and research department\"] can be generated.\n",
        "2. The LLM determines whether each statement can find supporting evidence in the retrieved reference materials (contexts), or whether the context can support the viewpoint of the ground_truth.\n",
        "\n",
        "   For instance, this statement can find supporting evidence in the third row of data's contexts: \"*Zhang Wei, an engineer in the teaching and research department, has been responsible for course development recently.*\"\n",
        "3. Then, the proportion of statements in the ground_truth list that can find supporting evidence in the contexts is calculated as the context_recall score.\n",
        "\n",
        "   Here, the score is 1 = 1/1.\n",
        "\n",
        "##### Context Precision\n",
        "\n",
        "In Ragas, context precision not only measures what proportion of contexts are related to the ground_truth but also evaluates the ranking of contexts. The calculation process is more complex:\n",
        "\n",
        "1. Read through the contexts sequentially, and based on the question and ground_truth, determine whether context<sub>i</sub> is relevant. If relevant, it scores 1; otherwise, it scores 0.\n",
        "\n",
        "   For example, in the third row of data, context<sub>1</sub> (\"Newton discovered gravity\") is irrelevant, while context<sub>2</sub> is relevant.\n",
        "2. For each context, calculate the precision score by dividing the cumulative sum of scores of the current context and all preceding contexts (numerator) by the position of the context in the sequence (denominator).\n",
        "\n",
        "   For the third row of data, the precision score for context<sub>1</sub> is 0/1 = 0, and for context<sub>2</sub>, it is 1/2 = 0.5.\n",
        "3. Sum up the precision scores of all contexts and divide by the number of relevant contexts to obtain the context_precision.\n",
        "\n",
        "   For the third row of data, context_precision = (0 + 0.5) / 1 = 0.5.\n",
        "\n",
        "> If you cannot fully understand the calculation process above, it doesn't matter. You only need to know that this metric evaluates the ranking of contexts. If you're interested, we encourage you to read [Ragas's source code](https://github.com/explodinggradients/ragas/blob/cc31f65d4b7c7cd6bbf686b9073a0dfaacfbcbc5/src/ragas/metrics/_context_precision.py#L250)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d7cc24c",
      "metadata": {},
      "source": [
        "### 2.3 Other Recommended Metrics to Explore\n",
        "\n",
        "Ragas also provides many other metrics, which will not be introduced one by one here. You can visit the Ragas documentation to learn more about the applicable scenarios and working principles of these metrics.\n",
        "\n",
        "The metrics supported by Ragas can be accessed at: https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44edd4a3",
      "metadata": {},
      "source": [
        "## 3. How to Optimize Based on Ragas Metrics\n",
        "The ultimate goal of evaluation is not to obtain scores, but to determine the direction of optimization based on these scores. You have already learned the concepts and calculation methods of three metrics: answer correctness, context recall, and context precision. When you observe that the scores of certain metrics are low, corresponding optimization measures should be formulated.\n",
        "\n",
        "### 3.1 Context Recall\n",
        "The context recall metric evaluates the performance of a RAG application during the **retrieval** phase. If this metric has a low score, you can try optimizing from the following aspects:\n",
        "\n",
        "- **Check the Knowledge Base**\n",
        "\n",
        "    <img src=\"https://wanx.alicdn.com/wanx/1937257750879544/text_to_image_lite_v2/6e4ca1055a0c467992b6b719b443a33e_0.png?x-oss-process=image/watermark,image_aW1nL3dhdGVybWFyazIwMjQxMTEyLnBuZz94LW9zcy1wcm9jZXNzPWltYWdlL3Jlc2l6ZSxtX2ZpeGVkLHdfMzAzLGhfNTI=,t_80,g_se,x_10,y_10/format,webp\" width=\"300\">\n",
        "\n",
        "    The knowledge base is the source of a RAG application. If the content of the knowledge base is incomplete, it will lead to insufficient reference information being recalled, thereby affecting context recall. You can compare the content of the knowledge base with test samples and observe whether the content of the knowledge base can support each test sample (this process can also be assisted by large language models (LLMs)s (LLMs)s (LLMs)). If you find that some test samples lack relevant knowledge, you need to supplement the knowledge base.\n",
        "- **Replace the Embedding Model**\n",
        "\n",
        "    <img src=\"https://img.alicdn.com/imgextra/i1/O1CN01WjZDdd1FlPjxo2B44_!!6000000000527-0-tps-2476-1120.jpg\" width=\"750\">\n",
        "\n",
        "    If your knowledge base content is already very complete, consider replacing the embedding model. A good embedding model can understand the deep semantic meaning of text. If two sentences are deeply related, even if they don't appear to be related, they can still receive a high similarity score. For example, if the question is \"Who is responsible for curriculum development?\" and the corresponding text segment in the knowledge base is \"Zhang Wei is a member of the teaching and research department,\" despite fewer overlapping words, an excellent embedding model can still assign a high similarity score to these two sentences, thereby recalling the text segment \"Zhang Wei is a member of the teaching and research department.\"\n",
        "- **query rewriting**\n",
        "\n",
        "    <img src=\"https://img.alicdn.com/imgextra/i4/O1CN01Z28xyO1KXjzp6JNaq_!!6000000001174-0-tps-2352-1154.jpg\" width=\"750\">\n",
        "\n",
        "    As a developer, it is unrealistic to impose too many requirements on how users ask questions. Therefore, you might receive vague questions like: \"Teaching and Research Department,\" \"Leave Request,\" \"Project Management.\" If such questions are directly input into a RAG application, they are unlikely to recall effective text segments. You can design a prompt template by organizing common employee questions and use large language models (LLMs)s (LLMs)s (LLMs) to rewrite queries, improving the accuracy of recall.\n",
        "\n",
        "\n",
        "### 3.2 Context Precision\n",
        "Similar to context recall, the context precision metric also evaluates the performance of a RAG application during the **retrieval** phase, but it focuses more on whether related text segments have higher rankings. If this metric has a low score, you can try the optimization measures mentioned under context recall, and you can also attempt to add **reranking** during the retrieval phase to improve the ranking of related text segments.\n",
        "\n",
        "\n",
        "### 3.3 Answer Correctness\n",
        "The answer correctness metric evaluates the overall comprehensive performance of a RAG system. If this metric has a low score while the previous two metrics have high scores, it indicates that the RAG system performs well in the **retrieval** phase but encounters issues in the **generation** phase. You can try the methods learned in previous tutorials, such as optimizing prompts, adjusting hyperparameters (such as temperature) of large language models (LLMs)s (LLMs) generation, or replacing with a more powerful large language models (LLMs)s (LLMs), and even fine-tuning the large language models (LLMs)s (LLMs) (which will be introduced in later tutorials) to enhance the accuracy of generated answers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "448a70bb-47c0-4933-9ed5-acd3072d18cc",
      "metadata": {},
      "source": [
        "## ‚úÖ Summary of this section\n",
        "\n",
        "Through the study of this section, you have learned how to establish automated testing for RAG chatbots.\n",
        "\n",
        "Automated testing is an important means of engineering optimization. With quantified automated testing, it can help you shift from **feeling** better to metrics **quantization** that the application performs better when improving your RAG chatbot. This not only helps you evaluate the question-answering quality of the RAG chatbot faster and find optimization directions, but also quantifies the optimization results you have achieved.\n",
        "\n",
        "Of course, having automated testing does not mean that you no longer need human evaluation at all. It is recommended that in practical applications, you invite domain experts corresponding to the RAG chatbot to jointly build a test set that reflects the distribution of real-world scenario problems, and continuously update the test set.\n",
        "\n",
        "At the same time, since large language models (LLMs) cannot always achieve 100% accuracy, it is also recommended that you regularly sample and evaluate the accuracy of automated testing results during actual use, and try not to frequently change the LLMs and embedding models. For Ragas, you can improve its performance by adjusting the prompts in the default evaluation method (for example, supplementing reference examples related to your business domain) (for more details, please refer to the extended reading).  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7745583",
      "metadata": {},
      "source": [
        "## Further Reading\n",
        "\n",
        "### Changing the Prompt Template in Ragas\n",
        "Many evaluation metrics in Ragas are implemented based on large language models (LLMs). Similar to LlamaIndex, Ragas' default prompt template is in English but allows for customization. You can translate Ragas' default prompts for various metrics into Chinese, making the evaluation results more suitable for Chinese question-answering scenarios.\n",
        "\n",
        "We provide a Chinese prompt template in the ragas_prompt folder. You can refer to the following code to adapt the Chinese prompts to different metrics in Ragas.\n",
        "> Ragas includes examples in its prompts to help the large model understand how to make judgments or generate lists of opinions, etc. Therefore, you can also modify these examples to fit your business scenario.\n",
        "\n",
        "An LLM sees prompts as a sequence of tokens where different models (or versions of a model) can tokenize the same prompt in different ways. Since LLMs are trained on tokens (and not on raw text), the way prompts get tokenized has a direct impact on the quality of the generated response.\n",
        "\n",
        "\"Prompts\" now become the primary programming interface for generative AI apps, telling the models what to do and influencing the quality of returned responses. \"Prompt Engineering\" is a fast-growing field of study that focuses on the design and optimization of prompts to deliver consistent and quality responses at scale.\n",
        "\n",
        "In its more complex form like this example from LangChain it contains placeholders that can be replaced with data from a variety of sources (user input, system context, external data sources etc.) to generate a prompt dynamically. This allows us to create a library of reusable prompts that can be used to drive consistent user experiences programmatically at scale.\n",
        "\n",
        "Finally, the real value of templates lies in the ability to create and publish prompt libraries for vertical application domains - where the prompt template is now optimized to reflect application-specific context or examples that make the responses more relevant and accurate for the targeted user audience. The Prompts For Edu repository is a great example of this approach, curating a library of prompts for the education domain with emphasis on key objectives like lesson planning, curriculum design, student tutoring etc.\n",
        "\n",
        "What Why Evaluate the latest models. New model generations are likely to have improved features and quality - but may also incur higher costs. Evaluate them for impact, then make migration decisions. Separate instructions & context Check if your model/provider defines delimiters to distinguish instructions, primary and secondary content more clearly. This can help models assign weights more accurately to tokens. Be specific and clear Give more details about the desired context, outcome, length, format, style etc. This will improve both the quality and consistency of responses. Capture recipes in reusable templates. Be descriptive, use examples Models may respond better to a \"show and tell\" approach. Start with a zero-shot approach where you give it an instruction (but no examples) then try few-shot as a refinement, providing a few examples of the desired output. Use analogies. Use cues to jumpstart completions Nudge it towards a desired outcome by giving it some leading words or phrases that it can use as a starting point for the response. Double Down Sometimes you may need to repeat yourself to the model. Give instructions before and after your primary content, use an instruction and a cue, etc. Iterate & validate to see what works. Order Matters The order in which you present information to the model may impact the output, even in the learning examples, thanks to recency bias. Try different options to see what works best. Give the model an ‚Äúout‚Äù Give the model a fallback completion response it can provide if it cannot complete the task for any reason. This can reduce chances of models generating false or fabricated responses.\n",
        "\n",
        "Above, you see how the prompt is constructed using a template. In the template there's a number of variables, denoted by {{variable}}, that will be replaced with actual values from a company API.\n",
        "\n",
        "In the search applications lesson, we briefly learned how to integrate your own data into Large Language Models (LLMs). In this lesson, we will delve further into the concepts of grounding your data in your LLM application, the mechanics of the process and the methods for storing data, including both embeddings and text.\n",
        "\n",
        "Large Language Models - These are the models referred throughout this course such as GPT-3.5, GPT-4, Llama-2, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5077509",
      "metadata": {},
      "source": [
        "| Ragas comes with a prompt template |\n",
        "|------------------------------------|\n",
        "| <a href=\"https://img.alicdn.com/imgextra/i4/O1CN016BngIW1nRtnidE0hK_!!6000000005087-0-tps-2710-1334.jpg\" target=\"_blank\"><img src=\"https://img.alicdn.com/imgextra/i4/O1CN016BngIW1nRtnidE0hK_!!6000000005087-0-tps-2710-1334.jpg\" width=\"900\"></a> |\n",
        "\n",
        "| After modifying the prompt template |\n",
        "|------------------------------------|\n",
        "| <a href=\"https://img.alicdn.com/imgextra/i2/O1CN01m7wDt21fhSKQ7d8CT_!!6000000004038-0-tps-2548-1278.jpg\" target=\"_blank\"><img src=\"https://img.alicdn.com/imgextra/i2/O1CN01m7wDt21fhSKQ7d8CT_!!6000000004038-0-tps-2548-1278.jpg\" width=\"900\"></a> |  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8d949c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Chinese prompt templates\n",
        "from ragas_prompt.chinese_prompt import ContextRecall, ContextPrecision, AnswerCorrectness\n",
        "\n",
        "# Customize prompt settings for each metric\n",
        "context_recall.context_recall_prompt.instruction = ContextRecall.context_recall_prompt[\"instruction\"]\n",
        "context_recall.context_recall_prompt.output_format_instruction = ContextRecall.context_recall_prompt[\"output_format_instruction\"]\n",
        "context_recall.context_recall_prompt.examples = ContextRecall.context_recall_prompt[\"examples\"]\n",
        "\n",
        "context_precision.context_precision_prompt.instruction = ContextPrecision.context_precision_prompt[\"instruction\"]\n",
        "context_precision.context_precision_prompt.output_format_instruction = ContextPrecision.context_precision_prompt[\"output_format_instruction\"]\n",
        "context_precision.context_precision_prompt.examples = ContextPrecision.context_precision_prompt[\"examples\"]\n",
        "\n",
        "answer_correctness.correctness_prompt.instruction = AnswerCorrectness.correctness_prompt[\"instruction\"]\n",
        "answer_correctness.correctness_prompt.output_format_instruction = AnswerCorrectness.correctness_prompt[\"output_format_instruction\"]\n",
        "answer_correctness.correctness_prompt.examples = AnswerCorrectness.correctness_prompt[\"examples\"]\n",
        "\n",
        "data_samples = {\n",
        "    'question': [\n",
        "        'Which department is Zhang Wei in?',\n",
        "        'Which department is Zhang Wei in?',\n",
        "        'Which department is Zhang Wei in?'\n",
        "    ],\n",
        "    'answer': [\n",
        "        'Based on the provided information, there is no mention of the department where Zhang Wei works. If you can provide more information about Zhang Wei, I may be able to help you find the answer.',\n",
        "        'Zhang Wei is in the HR department',\n",
        "        'Zhang Wei is in the Teaching and Research Department'\n",
        "    ],\n",
        "    'ground_truth': [\n",
        "        'Zhang Wei is a member of the Teaching and Research Department',\n",
        "        'Zhang Wei is a member of the Teaching and Research Department',\n",
        "        'Zhang Wei is a member of the Teaching and Research Department'\n",
        "    ],\n",
        "    'contexts': [\n",
        "        ['Provides administrative management and coordination support, optimizing administrative workflows.', 'Performance Management Department Han Shan Li Fei I902 041 Human Resources'],\n",
        "        ['Li Kai, Director of the Teaching and Research Department', 'Newton discovered the law of universal gravitation'],\n",
        "        ['Newton discovered the law of universal gravitation', 'Zhang Wei, engineer in the Teaching and Research Department, has recently been responsible for curriculum development.'],\n",
        "    ],\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(data_samples)\n",
        "\n",
        "score = evaluate(\n",
        "    dataset=dataset,\n",
        "    metrics=[answer_correctness, context_recall, context_precision],\n",
        "    llm=Tongyi(model_name=\"qwen-plus-0919\"),\n",
        "    embeddings=DashScopeEmbeddings(model=\"text-embedding-v3\"))\n",
        "\n",
        "score.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c1dc0f4",
      "metadata": {},
      "source": [
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>question</th>\n",
        "      <th>answer</th>\n",
        "      <th>ground_truth</th>\n",
        "      <th>contexts</th>\n",
        "      <th>answer_correctness</th>\n",
        "      <th>context_recall</th>\n",
        "      <th>context_precision</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>Which department is Zhang Wei in?</td>\n",
        "      <td>Based on the provided information, there is no mention of the department where Zhang Wei works. If you can provide more information about Zhang Wei, I may be able to help you find the answer.</td>\n",
        "      <td>Zhang Wei is a member of the Teaching and Research Department.</td>\n",
        "      <td>[Providing administrative management and coordination support, optimizing administrative workflows. , Performance Management Department Han Shan Li Fei I902 041 ...</td>\n",
        "      <td>0.175227</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>Which department is Zhang Wei in?</td>\n",
        "      <td>Zhang Wei is in the Human Resources Department.</td>\n",
        "      <td>Zhang Wei is a member of the Teaching and Research Department.</td>\n",
        "      <td>[Li Kai, Director of the Teaching and Research Department , Newton discovered the law of universal gravitation]</td>\n",
        "      <td>0.193980</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>Which department is Zhang Wei in?</td>\n",
        "      <td>Zhang Wei is in the Teaching and Research Department.</td>\n",
        "      <td>Zhang Wei is a member of the Teaching and Research Department.</td>\n",
        "      <td>[Newton discovered the law of universal gravitation, Zhang Wei, an engineer in the Teaching and Research Department, has recently been responsible for curriculum development]</td>\n",
        "      <td>0.994619</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.5</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b35c832",
      "metadata": {},
      "source": [
        "### More Evaluation Metrics\n",
        "In addition to RAG, there are many applications or tasks of large language models (LLMs) or natural language processing (NLP), such as Agents, natural language to SQL, machine translation, summarization, etc. Ragas provides many metrics that can be used to evaluate these tasks.\n",
        "\n",
        "\n",
        "| Evaluation Metric            | Use Case | Metric Meaning                                                                 |\n",
        "|---------------------|----------|--------------------------------------------------------------------------|\n",
        "| [ToolCallAccuracy](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/agents/#example)    | Agent    | Evaluates the LLM's performance in identifying and invoking tools required to complete specific tasks. This metric is obtained by comparing reference tool calls with tool calls made by the LLM, with a value range of 0-1. |\n",
        "| [DataCompyScore](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/sql/)      | natural language to SQL   | Evaluates the difference between the results obtained from database queries using SQL statements generated by the LLM and the correct results. The value ranges from 0 to 1.                     |\n",
        "| [LLMSQLEquivalence](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/sql/#non-execution-based-metrics)   | natural language to SQL   | Unlike the previous metric, this does not require actual database retrieval; it only evaluates the differences between SQL statements generated by the LLM and the correct SQL statements. The value ranges from 0 to 1.   |\n",
        "| [BleuScore](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/traditional/#bleu-score)           | General     | Evaluates the similarity between responses and correct answers based on n-grams. Initially designed for evaluating machine translation systems, this metric does not require the use of an LLM during evaluation, and its value ranges from 0 to 1. In the [2.7 tutorial](2_7_Fine_Tuning_LLMs_for_Improved_Accuracy_and_Efficiency.ipynb), you will learn how to fine-tune LLMs, and BleuScore can be used to measure the benefits brought by fine-tuning.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0ed74ea",
      "metadata": {},
      "source": [
        "## üî• Post-class Quiz\n",
        "\n",
        "### üîç Single-choice Question\n",
        "<details>\n",
        "<summary style=\"cursor: pointer; padding: 12px;  border: 1px solid #dee2e6; border-radius: 6px;\">\n",
        "<b>What does the Context Precision metric measure? ‚ùì</b>\n",
        "\n",
        "- A. Evaluation of overall response quality\n",
        "- B. Evaluation of whether retrieved text segments relevant to the question are ranked higher\n",
        "- C. Whether the generated answer is related to the retrieved text segments\n",
        "- D. Whether the generated answer is relevant to the question\n",
        "\n",
        "**[Click to view the answer]**\n",
        "</summary>\n",
        "\n",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
        "\n",
        "‚úÖ **Reference Answer: B**  \n",
        "üìù **Explanation**:  \n",
        "- Context Precision directly evaluates the ranking quality of retrieval results, not the relevance of the content itself or the quality of the final answer.\n",
        "\n",
        "</div>\n",
        "</details>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "447d3e8f",
      "metadata": {},
      "source": [
        "## ‚úâÔ∏è Evaluation and Feedback\n",
        "Thank you for studying the Alibaba Cloud Large Model ACP Certification course. If you think there are parts of the course that are well-written or need improvement, we look forward to your [evaluation and feedback through this questionnaire](https://survey.aliyun.com/apps/zhiliao/Mo5O9vuie).\n",
        "\n",
        "Your criticism and encouragement are both driving forces for our progress.  \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
