{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b3feff-775c-412d-bc63-39764d9c6f89",
   "metadata": {},
   "source": [
    "# 2.7 Enhancing Model Capabilities through Fine-tuning  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e93f3e3",
   "metadata": {},
   "source": [
    "## 🚄 Preface\n",
    "\n",
    "In the previous lessons, we introduced how to build a Q&A bot and attempted to enhance its capabilities by optimizing prompt, constructing RAG chatbot, and extending plugins. However, you may have noticed that you've been \"patching\" around the model—these methods essentially enhance the model's performance through external tools, while the model's inherent knowledge boundaries and reasoning abilities remain fundamentally unchanged. This section will take you into the \"internal training ground\" of large language models (LLMs), directly improving the model’s underlying capabilities through fine-tuning techniques.\n",
    "\n",
    "When facing in-depth needs in specific domains, such as precise parsing of elementary school math problems, relying on prompt engineering and RAG chatbot often falls short. For details like operator precedence rules or unit conversion logic in word problems, the model needs to establish a structured knowledge system. This is where fine-tuning shows its unique advantages—by \"targeted feeding\" the model with math problem-solving examples generated by DeepSeek-R2, you can enable the model to learn DeepSeek-R2's knowledge in mathematics, grasp mathematical thinking paradigms, and even independently discover problem-solving patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2783c73",
   "metadata": {},
   "source": [
    "## 🍁 Course Objectives\n",
    "\n",
    "After completing this course, you will be able to:\n",
    "\n",
    "* Learn and understand the core principles and implementation logic of fine-tuning large language models (LLMs).\n",
    "* Combine training principles to master the methodology for optimizing key training parameters.\n",
    "* Independently complete the fine-tuning of models, learn about potential issues that may arise, and practice various solutions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98334953",
   "metadata": {},
   "source": [
    "## 0. Environment Preparation\n",
    "\n",
    "Since fine-tuning models requires high hardware performance, it is recommended to use Platform for AI's Data Science Workshop to create an instance equipped with a GPU, allowing you to complete the fine-tuning tasks more efficiently.\n",
    "\n",
    "> If you do not have a local GPU environment or your GPU memory is less than 30GB, it is not recommended to run this course locally, as the code may fail to execute.\n",
    "\n",
    "Please refer to \"[1_0_Setup_Computing_Environment](https://edu.aliyun.com/course/3130200/lesson/343310285)\" under `Step 1: Create a PAI DSW Instance` to create a new instance, with the following instructions:\n",
    "\n",
    "1. Ensure that the new instance has a **different name** from any previously created instance, such as: acp_gpu  \n",
    "2. For **resource specifications**, select `ecs.gn7i-c8g1.2xlarge` (this specification includes **one A10 GPU with 30GB of memory**).\n",
    "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01L3iYeb1MRuEvXhhcD_!!6000000001432-2-tps-2984-1582.png\" width=\"800\">  \n",
    "3. For the **image**, choose `modelscope:1.21.0-pytorch2.4.0-gpu-py310-cu124-ubuntu22.04` (you need to switch the \"Image Configuration\" -> \"Chip Type\" to GPU).\n",
    "\n",
    "After the instance is successfully created and its status is `Running`, enter the following command in the `Terminal` to obtain the ACP course code:\n",
    "\n",
    "    ```bash\n",
    "    git clone https://github.com/AlibabaCloudDocs/aliyun_acp_learning.git\n",
    "    ```\n",
    "\n",
    "Reopen this chapter in the `Notebook` of the newly created GPU instance and continue learning the subsequent content.<br>\n",
    "\n",
    "Install the following dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "031f61d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: ms-swift[llm]==2.4.2.post2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# The following dependencies need to be installed\n",
    "%pip install accelerate==1.0.1 rouge-score==0.1.2 nltk==3.9.1 ms-swift[llm]==2.4.2.post2 evalscope==0.5.5rc1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfaa0e0",
   "metadata": {},
   "source": [
    "## 1. Task Design\n",
    "\n",
    "How to solve mathematical problems has always been an important direction in the development of large language models (LLMs), and it just so happens that your intelligent assistant also needs to have basic computational capabilities. To facilitate fine-tuning of the model, you can select a small-parameter open-source model `qwen2.5-1.5b-instruct` as your base model.\n",
    "\n",
    "First, you need to download the model and load it into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f1a505",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T03:25:11.402851Z",
     "iopub.status.busy": "2025-07-15T03:25:11.402538Z",
     "iopub.status.idle": "2025-07-15T04:12:33.686848Z",
     "shell.execute_reply": "2025-07-15T04:12:33.686389Z",
     "shell.execute_reply.started": "2025-07-15T03:25:11.402832Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading [config.json]: 100%|█████████████████| 660/660 [00:02<00:00, 224B/s]\n",
      "Downloading [configuration.json]: 100%|███████| 2.00/2.00 [00:01<00:00, 1.18B/s]\n",
      "Downloading [generation_config.json]: 100%|██████| 242/242 [00:01<00:00, 209B/s]\n",
      "Downloading [LICENSE]: 100%|███████████████| 11.1k/11.1k [00:01<00:00, 5.90kB/s]\n",
      "Downloading [merges.txt]: 100%|████████████| 1.59M/1.59M [01:12<00:00, 23.2kB/s]\n",
      "Downloading [model.safetensors]:   7%|▍     | 207M/2.88G [06:11<42:41, 1.12MB/s]2025-07-15 11:32:46,766 - modelscope - WARNING - Downloading: ./model/._____temp/model.safetensors failed, reason: ('Connection broken: IncompleteRead(56724836 bytes read, 111047324 more expected)', IncompleteRead(56724836 bytes read, 111047324 more expected)) will retry\n",
      "Downloading [model.safetensors]:   9%|▌     | 271M/2.88G [06:19<09:17, 5.03MB/s]2025-07-15 11:32:55,032 - modelscope - WARNING - Downloading: ./model/._____temp/model.safetensors failed, reason: ('Connection broken: IncompleteRead(55711028 bytes read, 112061132 more expected)', IncompleteRead(55711028 bytes read, 112061132 more expected)) will retry\n",
      "Downloading [model.safetensors]:  11%|▋     | 333M/2.88G [06:27<08:48, 5.18MB/s]2025-07-15 11:33:04,146 - modelscope - WARNING - Downloading: ./model/._____temp/model.safetensors failed, reason: ('Connection broken: IncompleteRead(56106500 bytes read, 111665660 more expected)', IncompleteRead(56106500 bytes read, 111665660 more expected)) will retry\n",
      "Downloading [model.safetensors]:  13%|▊     | 392M/2.88G [06:33<06:50, 6.53MB/s]2025-07-15 11:33:08,830 - modelscope - WARNING - Downloading: ./model/._____temp/model.safetensors failed, reason: ('Connection broken: IncompleteRead(65176388 bytes read, 102595772 more expected)', IncompleteRead(65176388 bytes read, 102595772 more expected)) will retry\n",
      "Downloading [model.safetensors]: 3.09GB [45:28, 1.22MB/s]                       \n",
      "Downloading [README.md]: 100%|█████████████| 4.80k/4.80k [00:02<00:00, 1.70kB/s]\n",
      "Downloading [tokenizer.json]: 100%|████████| 6.71M/6.71M [00:06<00:00, 1.15MB/s]\n",
      "Downloading [tokenizer_config.json]: 100%|█| 7.13k/7.13k [00:01<00:00, 5.51kB/s]\n",
      "Downloading [vocab.json]: 100%|█████████████| 2.65M/2.65M [00:05<00:00, 495kB/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[INFO:swift] Successfully registered `/usr/local/lib/python3.10/site-packages/swift/llm/data/dataset_info.json`\n",
      "2025-07-15 12:12:31,882\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "[INFO:swift] Loading the model using model_dir: ./model\n",
      "[INFO:swift] model_kwargs: {'device_map': 'cpu'}\n",
      "[INFO:swift] model.max_model_len: 32768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialization completed\n"
     ]
    }
   ],
   "source": [
    "# Download model parameters to the ./model directory\n",
    "!mkdir ./model\n",
    "!modelscope download --model qwen/Qwen2.5-1.5B-Instruct --local_dir './model'\n",
    "\n",
    "from swift.llm import (\n",
    "    get_model_tokenizer, get_template, ModelType,\n",
    "    get_default_template_type\n",
    ")\n",
    "import torch\n",
    "\n",
    "# You can modify the query (model input) according to your needs\n",
    "\n",
    "# Obtain model information\n",
    "model_type = ModelType.qwen2_5_1_5b_instruct\n",
    "template_type = get_default_template_type(model_type)\n",
    "# Set the local model location\n",
    "model_id_or_path = \"./model\"\n",
    "# Initialize the model and input/output formatting template\n",
    "kwargs = {}\n",
    "model, tokenizer = get_model_tokenizer(model_type, torch.float32, model_id_or_path=model_id_or_path, model_kwargs={'device_map': 'cpu'}, **kwargs)\n",
    "model.generation_config.max_new_tokens = 128\n",
    "template = get_template(template_type, tokenizer, default_system='')\n",
    "print(\"Model initialization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1063c961",
   "metadata": {},
   "source": [
    "You can directly try its effect on math problems (the answer is: 648 kilograms of radishes can be harvested):  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d446abd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T04:12:33.687820Z",
     "iopub.status.busy": "2025-07-15T04:12:33.687613Z",
     "iopub.status.idle": "2025-07-15T04:12:51.944379Z",
     "shell.execute_reply": "2025-07-15T04:12:51.943953Z",
     "shell.execute_reply.started": "2025-07-15T04:12:33.687803Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from swift.llm import inference\n",
    "from IPython.display import Latex, display\n",
    "\n",
    "math_question = \"In a triangular vegetable field with a base of 18 meters and a height of 6 meters, radishes are planted. If 12 kilograms of radishes are harvested per square meter, how many kilograms of radishes can be harvested from this field?\"\n",
    "query = math_question\n",
    "response, _ = inference(model, template, query)\n",
    "print(query)\n",
    "print(\"The correct answer is: 648 kilograms of radishes can be harvested\")\n",
    "print('-----------LLM response-------------')\n",
    "display(Latex(response))\n",
    "print('------------End of response--------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f6e2d",
   "metadata": {},
   "source": [
    "It can be observed that your model does not seem to be able to accurately compute this simple mathematical problem. The model knows the formula for the area of a triangle but fails to use this knowledge to accurately calculate the weight of the radish.\n",
    "\n",
    "Of course, the effect of using RAG is the same. From previous learning, you know that RAG is more like an open-book exam. However, you have never seen an open-book math exam improve scores because the core of improving math ability lies in enhancing students' logical reasoning and computational skills rather than knowledge retrieval.\n",
    "\n",
    "Therefore, to directly enhance the ability of your Q&A bot on simple mathematical problems, you must use model fine-tuning to improve the model’s logical reasoning ability. (Computational ability can be enhanced by introducing a \"calculator\" plugin.)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc8d595",
   "metadata": {},
   "source": [
    "## 2. Fine-tuning Principles\n",
    "\n",
    "### 2.1 How Models Learn\n",
    "\n",
    "#### 2.1.1 Machine Learning - Finding Patterns Through Data\n",
    "\n",
    "In traditional programming work, you usually know the explicit rules and write these rules into functions, such as: $f(x) = ax$.\n",
    "\n",
    "Here, a is a known deterministic value (also called a parameter or weight). This function represents a simple algorithmic model that can compute (predict) the output $y$ based on the input $x$.\n",
    "\n",
    "However, in real-world scenarios, it's more likely that you don't know the explicit rules (parameters) beforehand but may have some observed phenomena (data).\n",
    "\n",
    "The goal of machine learning is to help you use this data (training set) to try and find (learn) these parameter values, a process known as training the model.\n",
    "\n",
    "#### 2.1.2 loss function & Cost Function - Quantifying Model Performance\n",
    "\n",
    "To find the most suitable parameters, you need a way to measure whether the currently tested parameters are appropriate.\n",
    "\n",
    "For better understanding, assume you now need to evaluate whether the parameter a in the model $f(x) = ax$ is suitable.\n",
    "\n",
    "##### loss function\n",
    "\n",
    "You can assess the model's performance on a single data point $x_i, y_i$ by subtracting the predicted result $f(x_i)$ from the actual result $y_i$ for each sample $x_i$ in the training set. The function used to evaluate this error is called the loss function (or error function): $L(y_i, f(x_i)) = y_i - ax_i$.\n",
    "\n",
    "Directly calculating the difference might yield positive or negative values, which could cancel each other out when aggregating losses, underestimating the total loss. To address this issue, you can consider squaring the difference as the loss: $L(y_i, f(x_i)) = (y_i - ax_i)^2$. Additionally, squaring amplifies the impact of errors, helping you identify the most suitable model parameters.\n",
    "\n",
    "> In practical applications, different models may use different calculation methods as the loss function.\n",
    "\n",
    "##### Cost Function\n",
    "\n",
    "To evaluate the model's overall performance across the entire training set, you can calculate the average loss of all samples (i.e., mean squared error). This function, used to assess the model's overall performance across all training samples, is called the Cost Function (or cost function).\n",
    "\n",
    "For a training set with m samples, the cost function can be expressed as: $J(a) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - ax_i)^2$.\n",
    "\n",
    "> In practical applications, different models may also choose different calculation methods as the Cost Function.\n",
    "\n",
    "With the Cost Function, the task of finding suitable model parameters can be equated to finding the minimum value of the Cost Function (i.e., the optimal solution). Finding the minimum value of the Cost Function means that the corresponding parameter a value is the most suitable model parameter value.\n",
    "\n",
    "If you plot the Cost Function, the task of finding the optimal solution essentially involves finding the lowest point on the curve or surface.\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN0149XTTS1WUKSTtpeoh_!!6000000002791-2-tps-2314-1682.png\" style=\"width: 400px; display: block; margin-left: auto; margin-right: auto\"/>\n",
    "</div>\n",
    "\n",
    "> In real projects, people often interchangeably use the terms cost function and loss function. In subsequent content and code, we will follow this engineering convention and refer to the cost function as the loss function (loss function).\n",
    "\n",
    "#### 2.1.3 Gradient Descent Algorithm - Automatically Finding the Optimal Solution\n",
    "\n",
    "In the previous curve, you can visually identify the lowest point. However, in practical applications, models typically have many parameters, and their Cost Functions are often complex surfaces in high-dimensional spaces, making it impossible to find the optimal solution through direct observation. Therefore, you need an automated method to find the optimal parameter configuration.\n",
    "\n",
    "Gradient descent is one of the most common methods. A typical implementation of gradient descent starts by randomly selecting a starting point on the surface (or curve), then continuously making small adjustments to the parameters until the lowest point (corresponding to the optimal parameter configuration) is found.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://img.alicdn.com/imgextra/i2/O1CN01ihhR9Y1IbkFZTQ3bV_!!6000000000912-1-tps-1080-810.gif\" style=\"width: 400px;margin-left: auto; margin-right: auto\"/>\n",
    "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01meUISA1dHgq2mqm6V_!!6000000003711-1-tps-1080-810.gif\" style=\"width: 400px;margin-left: auto; margin-right: auto\"/>\n",
    "</div>\n",
    "\n",
    "When training a model, you need the training program to automatically adjust the parameters so that the value of the Cost Function approaches the lowest point. Therefore, the gradient descent algorithm must automatically control two aspects: the direction of parameter adjustment and the magnitude of parameter adjustment.\n",
    "\n",
    "##### Direction of Parameter Adjustment\n",
    "\n",
    "If the Cost Function is a U-shaped curve, you can intuitively see that the parameter adjustment should move in the direction where the absolute value of the slope decreases, i.e., towards a flatter area.\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://img.alicdn.com/imgextra/i2/O1CN01ME3u6G203FVsQsmLe_!!6000000006793-2-tps-1608-1244.png\" style=\"width: 400px; display: block; margin-left: auto; margin-right: auto\"/>\n",
    "</div>\n",
    "\n",
    "If the Cost Function is a surface in a three-dimensional coordinate system, the direction of parameter adjustment should similarly move towards flatter areas. However, at a certain point on the surface, there are multiple possible descending directions. To find the lowest point as quickly as possible, you should move in the steepest direction.\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://img.alicdn.com/imgextra/i2/O1CN01Uh8OxI1mqnkBHqMjH_!!6000000005006-1-tps-664-684.gif\" style=\"width: 400px; display: block; margin-left: auto; margin-right: auto\"/>\n",
    "</div>\n",
    "\n",
    "In mathematics, the gradient points in the direction of the steepest ascent from a point on the surface, and its opposite direction is the steepest descent.\n",
    "\n",
    "To find the lowest point on the surface in the shortest time, the direction of parameter adjustment should be along the opposite direction of the gradient, i.e., the green arrow direction in the two figures above.\n",
    "\n",
    "> For a curve f(a) in a two-dimensional coordinate system, the gradient at a point is the slope at that point. \n",
    "> For a surface f(a,b) in a three-dimensional coordinate system, the gradient at a point is a two-dimensional vector composed of the slope values in the a and b axis directions. This indicates the rate of change of the function in each input variable direction and points in the direction of the fastest growth. Calculating the slope of a point on the surface in a particular axis direction is also referred to as taking the partial derivative.\n",
    "\n",
    "##### Magnitude of Parameter Adjustment\n",
    "\n",
    "After determining the direction of parameter adjustment, the magnitude of the adjustment needs to be determined.\n",
    "\n",
    "Adjusting parameters with a fixed step size is the easiest approach, but this may prevent you from ever finding the lowest point, causing oscillation near the lowest point instead.\n",
    "\n",
    "For example, in the figure below, adjusting parameters with a fixed step size of 1.5 results in oscillation around the lowest value, unable to further approach the lowest point.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01y7FatQ27bKI9CYCJ1_!!6000000007815-1-tps-938-646.gif\" style=\"width: 400px; display: block; margin-left: auto; margin-right: auto\"/>\n",
    "</div>\n",
    "\n",
    "To avoid this issue, the adjustment magnitude should be reduced as you approach the lowest point. The closer you get to the lowest point, the smaller the slope becomes. Therefore, instead of using a fixed step size, you can use the slope at the current position as the adjustment magnitude.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01h45Ifb1xRZhXXIXEC_!!6000000006440-1-tps-892-618.gif\" style=\"width: 400px; display: block; margin-left: auto; margin-right: auto\"/>\n",
    "</div>\n",
    "\n",
    "However, some Cost Function curves are very steep, and directly using the slope may still cause oscillation around the lowest point. To address this, you can multiply the slope by a coefficient to regulate the step size. This coefficient is called the learning rate.\n",
    "\n",
    "The choice of learning rate is particularly important for training effectiveness and efficiency:\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; gap: 2px; padding: 15px; background:rgba(0,0,0,0)\">\n",
    "    <!-- Column 1 -->\n",
    "    <div style=\"flex: 1; padding: 10px; border: 1px solid #ddd; border-radius: 5px\">\n",
    "        <p style=\"margin-top: 10px\">An appropriate learning rate allows you to find suitable parameters in a relatively short time.</p>\n",
    "        <img src=\"https://img.alicdn.com/imgextra/i3/O1CN01NrvVfj1sCqtKHLyia_!!6000000005731-2-tps-1680-1224.png\" style=\"width: 100%; height: auto; border-radius: 3px\"/>\n",
    "    </div>\n",
    "    <!-- Column 2 -->\n",
    "    <div style=\"flex: 1; padding: 10px; border: 1px solid #ddd; border-radius: 5px\">\n",
    "        <p style=\"margin-bottom: 10px\">An excessively low learning rate, while capable of finding suitable parameters, leads to greater time and resource consumption.</p>\n",
    "        <img src=\"https://img.alicdn.com/imgextra/i1/O1CN015dbcz61MCn8LkN2Ta_!!6000000001399-2-tps-1728-1300.png\" style=\"width: 100%; height: auto; border-radius: 3px\"/>\n",
    "    </div>\n",
    "    <!-- Column 3 -->\n",
    "    <div style=\"flex: 1; padding: 10px; border: 1px solid #ddd; border-radius: 5px\">\n",
    "        <p style=\"margin-bottom: 10px\">An excessively high learning rate may cause you to skip the optimal solution, ultimately failing to find the lowest point.</p>\n",
    "        <img src=\"https://img.alicdn.com/imgextra/i1/O1CN01l4leTB1LKI0BcVs16_!!6000000001280-2-tps-1658-1262.png\" style=\"width: 100%; height: auto; border-radius: 3px\"/>\n",
    "    </div>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "A smaller learning rate, although it will consume a lot of computational resources and time, actually helps you approach the lowest point more closely. In practical model training engineering, attempts are also made to dynamically adjust the learning rate. For example, in Model Studio's model fine-tuning feature, there is a [learning rate adjustment strategy](https://help.aliyun.com/zh/model-studio/user-guide/using-fine-tuning-on-console#7864d6a606ztg), which allows you to configure linear decay of the learning rate or decay according to a curve. Alibaba Cloud's PAI also provides an [AutoML](https://help.aliyun.com/zh/pai/user-guide/automl/) tool that can help you automatically find a more suitable learning rate.\n",
    "\n",
    "#### 2.1.4 More parameters used in model training engineering\n",
    "\n",
    "##### batch size\n",
    "\n",
    "In the process of finding the lowest point of the Cost Function, each calculation of the gradient (the slope in each direction) and then updating the model parameters based on that gradient, preparing for the next calculation and update, is called a training step.\n",
    "\n",
    "In previous introductions, each training step calculates the gradient at a certain point and then updates the parameters. You can also set the batch size to n, averaging the gradients based on n samples (mini-batch) to update the parameters.\n",
    "\n",
    "A larger batch size can accelerate the training process, but it will also consume more resources, and an excessively large batch size may lead to issues such as reduced model generalization performance.\n",
    "\n",
    "Choosing an appropriate batch size is a balancing act, depending on available hardware resources, training time, and desired model performance. In practice, experiments are often needed to determine the most suitable batch size for a specific task.\n",
    "\n",
    "##### eval steps\n",
    "\n",
    "Because the training set is usually very large, people typically do not use the validation set for evaluation after a full iteration over the training set. Instead, they choose to evaluate using the validation set after a certain number of training steps. This interval is usually controlled by the eval_steps parameter.\n",
    "\n",
    "##### epoch\n",
    "\n",
    "A complete iteration over the training set is called an epoch. In actual training, you cannot guarantee finding the optimal solution (lowest point) of the Cost Function within one epoch. Therefore, many training frameworks support configuring the number of training epochs, such as the num_train_epochs parameter provided in the swift training framework.\n",
    "\n",
    "A too small epoch value may result in not finding the optimal model parameters by the end of training. A too large epoch value can lead to excessively long training times and resource waste.\n",
    "\n",
    "A common method for finding a suitable epoch is early stopping: before starting training, you do not preset an epoch value (or set a larger value). During training, you periodically evaluate the model's performance using the validation set. When the model's performance on the validation set no longer improves (or starts to decline), training is automatically stopped.\n",
    "\n",
    "Of course, early stopping is not the only solution. There are many other methods in the industry to determine a suitable epoch value, such as dynamically adjusting the learning rate based on changes in validation set loss to indirectly affect the number of training epochs.\n",
    "\n",
    "#### 2.1.5 Neural Network - Universal Complex Function Approximator\n",
    "\n",
    "**Problems faced in machine learning:**\n",
    "\n",
    "In text generation tasks, the input $x$ and output $y$ generally have very high dimensions, making it impossible to directly discern the underlying patterns. What should you do?\n",
    "\n",
    "Smart mathematicians found a **universal function approximator — neural network (multi-layer)**, which has become the foundation of current complex machine learning tasks.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://img.alicdn.com/imgextra/i1/O1CN01QRD5MH1rwMdJHBzxi_!!6000000005695-2-tps-1080-533.png\" style=\"width: 400px; display: block; margin-left: auto; margin-right: auto\"/>\n",
    "</div>\n",
    "\n",
    "One layer of a neural network is generally expressed as $Y=σ(W⋅X)$, where the uppercase input $X$ and output $Y$ indicate they are multi-dimensional, $σ$ is the activation function, and $W$ represents the parameters of the assumed function $f$. A k-layer neural network can be expressed as $Y=σ(W_k ⋯ σ(W_2 ⋅σ(W_1⋅X)))$.\n",
    "\n",
    "The activation function is a key component in neural networks that introduces non-linear transformations and determines whether neurons are activated and transmit information. For example, the most commonly used activation function RELU can be written as:\n",
    "\n",
    "**$RELU(input) = max( 0, input)= \\begin{cases} input & \\text{if } input > 0 \\\\ 0 & \\text{if } input ≤ 0 \\end{cases}$**\n",
    "\n",
    "When $input≤0$, the neuron is not activated; when $input>0$, the neuron is activated and begins transmitting information to the output.\n",
    "\n",
    "Expanding one layer of a neural network can be written as follows (assuming $X$ is a $3×2$ dimensional matrix and $Y$ is a $2×2$ dimensional matrix):\n",
    "\n",
    "$σ(W_{2×3}⋅X_{3×2})= σ(\\left[ \\begin{matrix} w_{1,1} & w_{1,2} & w_{1,3} \\\\ w_{2,1} & w_{2,2} & w_{2,3} \\end{matrix} \\right]×\\left[ \\begin{matrix} x_{1,1}& x_{1,2}\\\\ x_{2,1}& x_{2,2} \\\\ x_{3,1}& x_{3,2} \\end{matrix} \\right])$\n",
    "\n",
    "$= σ(\\left[\\begin{matrix}\n",
    "w_{1,1}×x_{1,1}+w_{1,2}×x_{2,1}+w_{1,3}×x_{3,1}&\n",
    "w_{1,1}×x_{1,2}+ w_{1,2}×x_{2,2}+w_{1,3}×x_{3,2} \\\\\n",
    "w_{2,1}×x_{1,1}+ w_{1,2}×x_{2,1}+w_{1,3}×x_{3,1}&\n",
    "w_{2,1}×x_{1,2}+ w_{2,2}×x_{2,2}+w_{2,3}×x_{3,2} \\end{matrix} \\right])$\n",
    "\n",
    "$= \\left[ \\begin{matrix} max(0, \\sum\\limits_{k=1}^{3}w_{1,k}×x_{k,1})& max(0, \\sum\\limits_{k=1}^{3}w_{1,k}×x_{k,2})\\\\ max(0, \\sum\\limits_{k=1}^{3}w_{2,k}×x_{k,1})& max(0, \\sum\\limits_{k=1}^{3}w_{2,k}×x_{k,2}) \\end{matrix} \\right]= \\left[ \\begin{matrix} y_{1,1}& y_{1,2}\\\\ y_{2,1}& y_{2,2} \\end{matrix} \\right]=Y_{2×2}$\n",
    "\n",
    "Fortunately, the gradient descent method remains effective on high-dimensional, complex functions.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN011caxP31GiUrEv1aGH_!!6000000000656-2-tps-847-779.png\" style=\"width: 400px; display: block; margin-left: auto; margin-right: auto\"/>\n",
    "</div>\n",
    "\n",
    "Now you have the ace combination:\n",
    "\n",
    "**A tool capable of approximating any complex function — neural network + a method capable of fitting data patterns and learning function parameters — gradient descent method**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f0801",
   "metadata": {},
   "source": [
    "### 2.2 Efficient Fine-Tuning Techniques\n",
    "\n",
    "#### 2.2.1 Pre-training and Fine-Tuning\n",
    "\n",
    "From previous learning, you've already understood the core of model training: finding the optimal combination of parameters.\n",
    "\n",
    "The model you initially downloaded is a pre-trained set of parameters — the result of extensive training on large-scale data.\n",
    "\n",
    "Fine-tuning refers to further adjusting these parameters to better suit your specific task (e.g., solving math problems or answering questions in a specialized domain).\n",
    "\n",
    "Let’s take the `qwen2.5-1.5b-instruct` model as an example to understand the time and hardware requirements for full training from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "#### GPU Memory Requirements\n",
    "\n",
    "*   Memory occupied by 1.5 billion parameters (assuming FP32 precision, with each parameter taking 4 bytes):  \n",
    "    $ \\frac{1.5 \\times 10^9 \\times 4}{2^{30}} \\approx 5.59 \\text{ GB} $\n",
    "\n",
    "*   In practice, training a model typically requires **7–8 times** the memory of its parameter size due to gradients, optimizer states, and intermediate activations. This brings the total GPU memory requirement to around **45 GB**, which exceeds most consumer-grade GPUs and even many cloud-based experimental environments.\n",
    "\n",
    "---\n",
    "\n",
    "#### Training Time Estimation\n",
    "\n",
    "*   Example calculation:  \n",
    "    - Total training tokens = **200 billion**, or about 250 thousands copies of Shakespeare's complete works\n",
    "    - Batch size (using 8 GPUs in parallel) = **2,000 tokens per batch**  \n",
    "    - Throughput = **150 tokens/GPU/sec × 8 GPUs = 1,200 tokens/sec**\n",
    "\n",
    "*   Estimated training time =  \n",
    "    $$\n",
    "    \\frac{\\text{Total Tokens}}{\\text{Batch Size} \\times \\text{Tokens per Second} \\times 86400} \\approx 10 \\text{ days}\n",
    "    $$\n",
    "\n",
    "*   Real-world considerations:  \n",
    "    Include data preprocessing, checkpoint saving, and distributed communication overhead. Actual training time may increase by **20–50%**. If the dataset grows to **1 trillion tokens**, the training duration could extend to **several months**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Training Cost Overview\n",
    "\n",
    "*   For short-term training (e.g., 10 days), renting cloud GPU instances on a pay-as-you-go basis is often more cost-effective than purchasing dedicated hardware upfront.\n",
    "\n",
    "*   Training cost formula:  \n",
    "    $$\n",
    "    \\text{Training Cost} = \\text{GPU hourly rate} \\times \\text{Training time (in hours)}\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In summary, **reducing server unit price** and **shortening training time** can effectively reduce training costs, where **reducing memory requirements** can effectively lower server unit price, and reducing **total training data volume** can shorten training time.\n",
    "\n",
    "<br/>\n",
    "\n",
    "In the actual model training process, there is also a challenge: **the high cost of obtaining labeled data**, **especially for specific tasks** (e.g., medical image analysis or niche language processing). You can try step-by-step training of the model through \"pre-training\" and \"fine-tuning\", where:\n",
    "\n",
    "*   **Pre-training**: Training the model on a large-scale **general dataset** so that it can learn broad foundational knowledge or feature representations. This knowledge is usually general and not aimed at any specific task. Pre-training is not task-specific but provides a powerful initial model for various downstream tasks. Typical pre-trained models: Qwen2.5-Max, DeepSeek-V3, GPT-4, etc.\n",
    "    \n",
    "*   **Fine-tuning**: Further training the model using a **small-scale dataset** specific to a task based on the pre-trained model. The goal is to make the model adapt to specific downstream tasks (e.g., medical, legal, and other professional domain needs).\n",
    "    \n",
    "\n",
    "The table below shows the main differences between pre-training and fine-tuning:\n",
    "\n",
    "<div style=\"width: 20%\">\n",
    "    \n",
    "|  **Feature**  |  **Pre-training**  |  **Fine-tuning**  |\n",
    "| --- | --- | --- |\n",
    "|  Objective  |  $ $ Learning general features  |  Adapting to specific tasks  |\n",
    "|  Data  |  Large-scale general data  |  Small-scale task-related data  |\n",
    "|  Training method  |  Self-supervised/Unsupervised  |  Supervised  |\n",
    "|  Parameter updates  |  All parameters trainable  |  Partial or all parameters trainable  |\n",
    "|  Application scenarios  |  Base model construction  |  Specific task optimization  |\n",
    "\n",
    "</div>\n",
    "\n",
    "It is worth mentioning that **pre-training generally learns through self-supervised learning**, with data coming from massive texts on the internet (e.g., Wikipedia, books, web pages), allowing the model to find patterns or \"guess answers\" on its own. This learning method does not require manual annotation, saving a lot of labor costs, making it naturally suitable for learning from massive data.\n",
    "\n",
    "On the other hand, **fine-tuning is done through supervised learning**, requiring small-scale annotated data for specific tasks (e.g., annotated reviews for sentiment classification, annotated medical texts), and directly teaching the model to complete tasks using annotated data. Due to the high cost of manual annotation, this learning method is difficult to scale to massive data, thus being more suitable for model training with clear scenario goals, typically requiring only a few thousand to tens of thousands of samples.\n",
    "\n",
    "Therefore, you can quickly and cost-effectively build your large model application in the following ways:\n",
    "\n",
    "Step 1: Directly choose a pre-trained model (e.g., Qwen, DeepSeek, GPT), which can save the comprehensive cost of training a model from scratch.\n",
    "\n",
    "Step 2: Fine-tune the model according to your actual scenario, usually only needing to build a few thousand annotated data applicable to the actual scenario, because the total number of training tokens is greatly reduced, effectively shortening the training time, thereby further reducing the training cost.\n",
    "\n",
    "Fine-tuning can shorten training time, but can fine-tuning the model also reduce memory requirements?\n",
    "\n",
    "The number of model parameters is the main reason affecting memory requirements. From the perspective of adjusting the size of the parameter count, fine-tuning can be divided into **full-parameter fine-tuning** and **parameter-efficient fine-tuning**.\n",
    "\n",
    "**Full-parameter fine-tuning (Full Fine Tuning)** is a model optimization method that fine-tunes all parameters based on the pre-trained model, meaning that in the above model structure, any parameter will be adjusted. This method avoids consuming the large amount of computational resources required to retrain all parameters of the model from scratch while avoiding performance degradation due to some parameters not being fine-tuned. However, large model training costs are high, requiring substantial computational resources and large amounts of data. Even with full-parameter fine-tuning, high training costs are often still needed.\n",
    "\n",
    "**Efficient fine-tuning techniques (PEFT)** significantly reduce the computational cost of large model fine-tuning by adjusting a small number of parameters while maintaining performance close to full-parameter training. Typical methods include Adapter Tuning, Prompt Tuning, and LoRA. Among them, LoRA, which only needs to train small parameter matrices (i.e., low-rank matrices, requiring only 0.1%-1% of the original model's parameters), has become the preferred solution for resource-constrained scenarios. The following focuses on how LoRA achieves parameter-efficient fine-tuning with extremely low parameter counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f59f270",
   "metadata": {},
   "source": [
    "#### 2.2.2 LoRA Fine-tuning\n",
    "\n",
    "LoRA (Low-Rank Adaptation) fine-tuning is currently the most commonly used method for model adaptation. It does not rely on the internal architecture of the model but instead abstracts and decomposes the parameters that need updating during fine-tuning into two much smaller low-rank matrices $A_{d \\times r}$ and $B_{r \\times d}$. The original model weights remain frozen, i.e.,  \n",
    "$$\n",
    "W^{fine-tuned}_{d \\times d} = A_{d \\times r} \\cdot B_{r \\times d} + W^{pre-trained}_{d \\times d}\n",
    "$$\n",
    "\n",
    "If you're unfamiliar with the concept of low-rank decomposition, let's revisit a simple neural network formulation. Assume the input vector $X$ has dimension 5 and the output vector $Y$ has dimension 4. Then, the weight matrix $W$ would be of size $5 \\times 4$, denoted as $W \\in \\mathbb{R}^{5 \\times 4}$, containing a total of 20 parameters.\n",
    "\n",
    "A single-layer neural network can be expressed as:  \n",
    "$$\n",
    "Y_{5 \\times 1} = \\sigma(W_{5 \\times 4} \\cdot X_{4 \\times 1})\n",
    "$$\n",
    "\n",
    "The rank of a matrix intuitively represents its effective information content. For example, although the following matrix has 2 rows and 3 columns, all rows are linearly dependent — one row can represent the others — so its rank is 1:\n",
    "$$\n",
    "\\text{rank}\\left( \n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "2 & 4 & 6\n",
    "\\end{bmatrix}\n",
    "\\right) = 1\n",
    "$$\n",
    "\n",
    "In model fine-tuning, it can be assumed that most of the useful information updates (high-rank) have already been learned during pre-training, while the additional effective information introduced by fine-tuning is minimal (low-rank). This can be written mathematically as:\n",
    "\n",
    "$$\n",
    "W_{5 \\times 4}^{pre-trained} - W_{5 \\times 4}^{initial} = \\Delta W_{5 \\times 4}^{pre-trained}, \\quad \\text{rank}(\\Delta W_{5 \\times 4}^{pre-trained}) = 5\n",
    "$$\n",
    "$$\n",
    "W_{5 \\times 4}^{fine-tuned} - W_{5 \\times 4}^{pre-trained} = \\Delta W_{5 \\times 4}^{fine-tuning}, \\quad \\text{rank}(\\Delta W_{5 \\times 4}^{fine-tuning}) \\leq 2\n",
    "$$\n",
    "\n",
    "Since low-rank matrices contain sparse information, they can be efficiently decomposed into two much smaller matrices. Assuming $\\text{rank}(\\Delta W_{5 \\times 4}^{fine-tuning}) = 1$, we can write:\n",
    "\n",
    "$$\n",
    "\\Delta W_{5 \\times 4}^{fine-tuning} =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2 & -1 \\\\\n",
    "2 & 0 & 4 & -2 \\\\\n",
    "3 & 0 & 6 & -3 \\\\\n",
    "4 & 0 & 8 & -4 \\\\\n",
    "5 & 0 & 10 & -5\n",
    "\\end{bmatrix}_{5 \\times 4}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "4 \\\\\n",
    "5\n",
    "\\end{bmatrix}_{5 \\times 1}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2 & -1\n",
    "\\end{bmatrix}_{1 \\times 4}\n",
    "$$\n",
    "\n",
    "To illustrate this further, consider the base model `qwen2.5-1.5b-instruct`, where we assume $r = 8$ and $d = 1024$. Below is a comparison of parameter counts:\n",
    "\n",
    "$$\n",
    "W^{fine-tuned}_{d \\times d} = A_{d \\times r} \\cdot B_{r \\times d} + W^{pre-trained}_{d \\times d}\n",
    "$$\n",
    "\n",
    "| **Method** | **Parameter Calculation Formula** | **Number of Parameters** | **Savings Ratio** |\n",
    "| --- | --- | --- | --- |\n",
    "| Full-parameter fine-tuning | $W_{d \\times d}$, $1024 \\times 1024$ | 1,048,576 | $0\\%$ |\n",
    "| LoRA fine-tuning | $A_{d \\times r}$ and $B_{r \\times d}$, $1024 \\times 8 + 8 \\times 1024$ | 16,384 | $98.44\\%$ |\n",
    "\n",
    "During inference, the matrices $A_{d \\times r}$, $B_{r \\times d}$, and $W^{pre-trained}_{d \\times d}$ can be merged to reconstruct $W^{fine-tuned}_{d \\times d}$ either in advance or dynamically.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<a href=\"https://img.alicdn.com/imgextra/i3/O1CN01NtGavS1TTvIIeZxO1_!!6000000002384-2-tps-804-712.png\" target=\"_blank\">\n",
    "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01NtGavS1TTvIIeZxO1_!!6000000002384-2-tps-804-712.png\" style=\"width: 600px;background:white;display: block; margin-left: auto; margin-right: auto\"/>\n",
    "</a>\n",
    "<br>Image source: LORA's paper - LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\n",
    "</div>\n",
    "\n",
    "When using LoRA for fine-tuning, the main tunable hyperparameter is the assumed low-rank $r$. A larger $r$ allows the model to capture more complex feature changes but increases training difficulty, requiring more memory and training epochs.\n",
    "\n",
    "Empirically, the value of $r$ is closely related to the amount of training data:\n",
    "\n",
    "- **For small datasets (1k–10k samples):** It is recommended to use $r \\leq 16$ to prevent overfitting and excessive training time.\n",
    "- **For large datasets (100k+ samples):** Try $r \\geq 32$ to better explore underlying patterns in the data.\n",
    "\n",
    "#### 2.2.3 LoRA Fine-tuning Effectiveness\n",
    "\n",
    "The authors of LoRA compared various fine-tuning methods across two datasets (the x-axis shows the number of trainable parameters, and the y-axis indicates training effectiveness). As shown below, LoRA provides the best cost-effectiveness trade-off.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://img.alicdn.com/imgextra/i1/O1CN01RGquUv1ZlDuoik8zU_!!6000000003234-2-tps-1944-662.png\" style=\"width: 700px;background:white;display: block; margin-left: auto; margin-right: auto\"/>\n",
    "</div>\n",
    "\n",
    "It is clear that not all methods benefit from having more trainable parameters — **more parameters do not necessarily lead to better performance.** However, **the LoRA method demonstrates superior scalability and task performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1204e108",
   "metadata": {},
   "source": [
    "## 3. Fine-Tuning Practice\n",
    "\n",
    "### 3.1 Model Training Status and Metrics\n",
    "\n",
    "Training a model is very similar to the human learning and exam process.\n",
    "\n",
    "A model must undergo the test of three sets of questions, generating two metrics to determine the state of model training. These are:\n",
    "\n",
    "Three sets of questions:\n",
    "\n",
    "*   **Training set**: The practice workbook with detailed answer explanations. The model will repeatedly practice and generate **training loss** based on the loss function. The smaller the training loss, the better the model performs on the provided practice workbook. Combined with the gradient descent method discussed in section 2.1 on how models learn, the model updates its parameters based on the training loss.\n",
    "    \n",
    "*   **Validation set**: Simulated exam questions. After the model has learned for a period of time, it will be tested once and generate **validation loss** based on the loss function. Validation loss is used to evaluate the effectiveness of model training. The smaller the validation loss, the better the model performs in the simulated exam.\n",
    "    \n",
    "*   **Test set**: Real exam questions. The accuracy of the model on the test set is used to evaluate the final performance of the model.\n",
    "\n",
    "The three states of model training:\n",
    "\n",
    "*   **Unchanged or increasing training loss**: This indicates **training failure**. You can think of it as the model not learning anything from the training set (practice workbook), indicating that there is an issue with the model's learning method.\n",
    "    \n",
    "*   **Both training loss and validation loss are decreasing**: This indicates that the model is **underfitting**. You can imagine that the model is making progress on the training set (practice workbook) and its performance on the validation set (simulated exam) is also improving, but there is still more room for improvement. At this point, you should let the model continue learning.\n",
    "    \n",
    "*   **Decreasing training loss but increasing validation loss**: This indicates that the model is **overfitting**. You can think of it as the model simply memorizing the training set (practice workbook). When taking the exam, it struggles with unseen questions. In this scenario, methods to suppress the model’s tendency to memorize should be applied, such as providing it with 20 more workbooks so that it cannot remember all the questions and is forced to learn the underlying patterns in the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23363db8",
   "metadata": {},
   "source": [
    "### 3.2 Baseline Model Examination\n",
    "\n",
    "Before starting the model fine-tuning, let's first take a look at how the baseline model performs on the test set.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a54a02d5",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-07-15T04:55:06.106090Z",
     "iopub.status.busy": "2025-07-15T04:55:06.105776Z",
     "iopub.status.idle": "2025-07-15T04:58:09.714944Z",
     "shell.execute_reply": "2025-07-15T04:58:09.714489Z",
     "shell.execute_reply.started": "2025-07-15T04:55:06.106071Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "Xiaoli is reading a 240-page storybook. On the first day, she read (1/8) of the entire book, and on the second day, she read (1/5) of the entire book. How many pages did she read in total over the two days?\n",
      "The correct answer is: {{ans:78}}\n",
      "-----------Model Response----------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "To determine how many pages Xiaoli read over the two days, we need to calculate the number of pages she read each day and then sum these amounts.\n",
       "\n",
       "First, let's find out how many pages Xiaoli read on the first day:\n",
       "\\[\n",
       "\\text{Pages read on the first day} = \\frac{1}{8} \\times 240 = 30\n",
       "\\]\n",
       "\n",
       "Next, let's find out how many pages Xiaoli read on the second day:\n",
       "\\[\n",
       "\\text{Pages read on the second day} = \\frac{1}{5} \\times 240 = 48"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------End of Response----------------\n",
      "Model answered incorrectly\n",
      "========================================================================================\n",
      "A rectangular dining table is 150 centimeters long, and its width is 100 centimeters shorter than its length. What is the perimeter of this rectangular dining table in centimeters?\n",
      "The correct answer is: {{ans:400}}\n",
      "-----------Model Response----------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "To find the perimeter of a rectangle, we use the formula \\( P = 2 \\times (L + W) \\), where \\( L \\) is the length and \\( W \\) is the width.\n",
       "\n",
       "Given:\n",
       "- The length (\\( L \\)) of the rectangular dining table is 150 cm.\n",
       "- The width (\\( W \\)) is 100 cm less than the length, so \\( W = L - 100 \\).\n",
       "\n",
       "First, calculate the width:\n",
       "\\[ W = 150 \\text{ cm} - 100 \\text{ cm} = 50"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------End of Response----------------\n",
      "Model answered incorrectly\n",
      "========================================================================================\n",
      "The physical education room purchased a batch of small balls. When divided into groups of 3, 4, or 5, there were no remainders each time. What is the minimum number of balls in this batch?\n",
      "The correct answer is: {{ans:60}}\n",
      "-----------Model Response----------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "To solve the problem, we need to find the smallest positive integer \\( n \\) that satisfies the following conditions:\n",
       "1. \\( n \\) is divisible by 3.\n",
       "2. \\( n \\) is divisible by 4.\n",
       "3. \\( n \\) is divisible by 5.\n",
       "\n",
       "This means that \\( n \\) must be a common multiple of 3, 4, and 5. To find the smallest such \\( n \\), we calculate the least common multiple (LCM) of these numbers.\n",
       "\n",
       "First, we find the prime factorizations of 3, 4, and 5:\n",
       "- The prime factor"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------End of Response----------------\n",
      "Model answered incorrectly\n",
      "========================================================================================\n",
      "Grandpa is 62 years old this year, and Dongdong is 8 years old. How many times older will Grandpa be than Dongdong next year?\n",
      "The correct answer is: {{ans:7}}\n",
      "-----------Model Response----------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "To determine how many times older Grandpa will be than Dongdong next year, we first need to find their ages for that future time.\n",
       "\n",
       "1. **Calculate Grandpa's age next year:**\n",
       "   - Grandpa is currently 62 years old.\n",
       "   - Next year, Grandpa will be \\( 62 + 1 = 63 \\) years old.\n",
       "\n",
       "2. **Calculate Dongdong's age next year:**\n",
       "   - Dongdong is currently 8 years old.\n",
       "   - Next year, Dongdong will be \\( 8 + 1 = 9 \\) years old.\n",
       "\n",
       "3. **Determine"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------End of Response----------------\n",
      "Model answered incorrectly\n",
      "========================================================================================\n",
      "Ms. Wang has 400 yuan. She bought 15 tubes of badminton shuttlecocks, each costing 25 yuan. How much money does she have left?\n",
      "The correct answer is: {{ans:25}}\n",
      "-----------Model Response----------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "To determine how much money Ms. Wang has left after buying the badminton shuttlecocks, we need to follow these steps:\n",
       "\n",
       "1. Calculate the total cost of the badminton shuttlecocks.\n",
       "2. Subtract the total cost from the initial amount of money Ms. Wang had.\n",
       "\n",
       "First, let's calculate the total cost of the badminton shuttlecocks:\n",
       "\\[\n",
       "\\text{Total cost} = \\text{Number of tubes} \\times \\text{Cost per tube} = 15 \\times 25 = 375 \\text{ yuan}\n",
       "\\]\n",
       "\n",
       "Next, we subtract the total"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------End of Response----------------\n",
      "Model answered correctly\n",
      "========================================================================================\n",
      "The ratio of the number of chickens to ducks is 5:3. If there are 25 chickens, how many ducks are there?\n",
      "The correct answer is: {{ans:15}}\n",
      "-----------Model Response----------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "To solve this problem, we need to understand that the ratio of the number of chickens to ducks is given as 5:3. This means for every 5 chickens, there are 3 ducks.\n",
       "\n",
       "Given:\n",
       "- The number of chickens = 25\n",
       "\n",
       "We can set up a proportion based on the ratio:\n",
       "\n",
       "\\[\n",
       "\\frac{\\text{Number of Chickens}}{\\text{Number of Ducks}} = \\frac{5}{3}\n",
       "\\]\n",
       "\n",
       "Let's denote the number of ducks by \\( D \\). Therefore,\n",
       "\n",
       "\\[\n",
       "\\frac{25}{D} = \\frac{5}{3}\n",
       "\\]\n",
       "\n",
       "Cross"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------End of Response----------------\n",
      "Model answered incorrectly\n",
      "========================================================================================\n",
      "It is known that a No. 4 bus departs every 10 minutes, and a No. 6 bus departs every 15 minutes. After both buses depart simultaneously, they will depart simultaneously again after at least ___ minutes. How many?\n",
      "The correct answer is: {{ans:30}}\n",
      "-----------Model Response----------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "To determine how long it will take for both the No. 4 bus and the No. 6 bus to depart simultaneously again, we need to find the least common multiple (LCM) of their departure intervals.\n",
       "\n",
       "The No. 4 bus departs every 10 minutes, and the No. 6 bus departs every 15 minutes. The LCM of two numbers is the smallest number that is a multiple of both of them.\n",
       "\n",
       "First, let's list the multiples of each interval:\n",
       "- Multiples of 10: \\(10, 20, 30, 40, "
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------End of Response----------------\n",
      "Model answered correctly\n",
      "========================================================================================\n",
      "A road construction project started on May 9th and was completed on June 10th. How many days did the construction take?\n",
      "The correct answer is: {{ans:33}}\n",
      "-----------Model Response----------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "To determine how many days the construction took, we need to count the number of days from May 9th to June 10th.\n",
       "\n",
       "1. **Identify the start date**: The construction began on May 9th.\n",
       "2. **Identify the end date**: The construction ended on June 10th.\n",
       "3. **Count the days**:\n",
       "   - From May 9th to May 31st (inclusive): There are \\(31 - 9 + 1 = 23\\) days in this period.\n",
       "   - From June 1st to June 10th (inclusive): There"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------End of Response----------------\n",
      "Model answered incorrectly\n",
      "========================================================================================\n",
      "A football costs 32 yuan, and a volleyball costs 18 yuan. The school bought 8 footballs and 6 volleyballs. How much is the total cost?\n",
      "The correct answer is: {{ans:364}}\n",
      "-----------Model Response----------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "To find the total cost of the footballs and volleyballs, we need to calculate the cost for each type separately and then add them together.\n",
       "\n",
       "First, let's calculate the cost of the footballs:\n",
       "- Each football costs 32 yuan.\n",
       "- The school bought 8 footballs.\n",
       "\n",
       "\\[\n",
       "\\text{Cost of footballs} = 32 \\times 8\n",
       "\\]\n",
       "\n",
       "Let's perform the multiplication:\n",
       "\n",
       "\\[\n",
       "32 \\times 8 = 256\n",
       "\\]\n",
       "\n",
       "So, the cost of the footballs is 256 yuan.\n",
       "\n",
       "Next, let's calculate the cost of the volleyballs"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------End of Response----------------\n",
      "Model answered incorrectly\n",
      "========================================================================================\n",
      "The sports field track is 400 meters per lap. Mingming wants to run 1 kilometer. He has already run one lap. How many more meters does he need to run to reach one kilometer?\n",
      "The correct answer is: {{ans:600}}\n",
      "-----------Model Response----------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "To determine how many more meters Mingming needs to run to reach one kilometer, we start by noting that:\n",
       "\n",
       "- The total distance of one kilometer is \\(1000\\) meters.\n",
       "- The current distance Mingming has run is \\(400\\) meters (one lap).\n",
       "\n",
       "We subtract the distance already covered from the total distance to find out how much further he needs to run:\n",
       "\\[\n",
       "1000 \\text{ meters} - 400 \\text{ meters} = 600 \\text{ meters}\n",
       "\\]\n",
       "\n",
       "Therefore, Mingming needs to run an additional **600 meters**"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------End of Response----------------\n",
      "Model answered correctly\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Model scored: **30** points in the exam"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from IPython.display import Markdown\n",
    "\n",
    "sum, score = 0, 0\n",
    "for line in open(\"./resources/2_7/test.jsonl\"):\n",
    "    # Read math questions from the test set\n",
    "    math_question = json.loads(line)\n",
    "    query = math_question[\"messages\"][1][\"content\"]\n",
    "    # Inference using the baseline model\n",
    "    response, _ = inference(model, template, query)\n",
    "    # Get the correct answer\n",
    "    ans = math_question[\"messages\"][2][\"content\"]\n",
    "    pos = ans.find(\"ans\")\n",
    "    end_pos = ans[pos:].find('}}')\n",
    "    ans = ans[pos - 2: end_pos + pos + 2]\n",
    "    # Format output\n",
    "    print((\"========================================================================================\"))\n",
    "    print(query.split(\"#Math Problem#\\n\")[1])\n",
    "    print(\"The correct answer is: \" + ans)\n",
    "    print(\"-----------Model Response----------------\")\n",
    "    display(Latex(response))\n",
    "    print(\"-----------End of Response----------------\")\n",
    "    # Calculate model score\n",
    "    if ans in response or ans[6:-2] in response:\n",
    "        score += 1\n",
    "        print(\"Model answered correctly\")\n",
    "    else: print(\"Model answered incorrectly\")\n",
    "    sum += 1\n",
    "# Summary\n",
    "display(Markdown(\"Model scored: **\" + str(int(100*score/sum)) + \"** points in the exam\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa331f",
   "metadata": {},
   "source": [
    "The baseline model often abandons reasoning midway during exams, struggling to provide correct answers. This performance not only confirms that the question difficulty exceeds its processing capability but also reveals the fundamental reason why prompt engineering is ineffective — the model itself lacks the necessary problem-solving ability. Fine-tuning the model is the only solution.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf59e5e",
   "metadata": {},
   "source": [
    "### 3.3 Model Fine-tuning\n",
    "\n",
    "Here, we use the [ms-swift](https://github.com/modelscope/ms-swift/tree/main) (Modelscope Scalable lightWeight Infrastructure for Fine-Tuning) framework, an open-source framework specifically developed by Alibaba's ModelScope community for model training. This framework supports the training (pre-training, fine-tuning, alignment), inference, evaluation, and deployment of over 350 large language models (LLMs) and more than 90 multi-modal LLMs (MLLMs).\n",
    "\n",
    "Moreover, the ms-swift framework is very convenient to use. Each time it calculates the validation loss (evaluation loss), the framework automatically saves the current model parameters (model_checkpoint) of the training phase and automatically saves the parameters with the smallest validation loss at the end of the training, which corresponds to the (best_model_checkpoint) in the figure below.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN0150XsFO1xM4z7CUMNr_!!6000000006428-2-tps-2288-136.png\" style=\"width: 70%;display: block; margin-left: auto; margin-right: auto\"/>\n",
    "</div>\n",
    "\n",
    "In the subsequent multiple experiments, we will focus on adjusting three parameters: learning_rate, LoRA (lora_rank), and the number of dataset training epochs (num_train_epochs). We will also replace the dataset to demonstrate how to perform LoRA fine-tuning. Adjustments to other parameters are made to facilitate the presentation of experimental results, such as increasing the batch size (batch_size) to shorten training time, which you do not need to pay too much attention to.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91a945d",
   "metadata": {},
   "source": [
    "#### 3.3.1 First Experiment (Takes 1 minute)\n",
    "\n",
    "In the initial experiment, it is recommended that you first fine-tuning according to the following parameter settings, and use a dataset of 100 problem solutions generated by DeepSeek-R1 for training, so as to improve the training effect through parameter optimization in subsequent experimental stages:\n",
    "\n",
    "| Parameter | Parameter Value |\n",
    "| --- | --- |\n",
    "| learning rate (learning_rate) | 0.1 |\n",
    "| LoRA Rank (lora_rank) | 4 |\n",
    "| Number of Training Epochs (num_train_epochs) | 1 |\n",
    "| Dataset Location (dataset) | Dataset Location: current directory/resources/2_4/train_100.jsonl |\n",
    "| You can adjust all parameters freely, but due to display effects and memory constraints, there are the following limitations: | batch_size <= 16 (memory constraint) <br>max_length <= 512 (maximum length of each training data, memory constraint) <br>lora_rank <= 64 (LoRA rank, memory constraint) <br>eval_step <= 20 (for convenience of display) |\n",
    "\n",
    "Start the experiment:<br/>\n",
    "The fine-tuning module of the ms-swift framework uses LoRA fine-tuning by default, so there is no need to explicitly declare the fine-tuning method in the experiment.<br/>\n",
    "At the same time, the framework will intelligently reduce the actual learning rate during the training process to ensure that the model does not always skip the optimal solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5988325c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-07-15T05:39:43.785953Z",
     "iopub.status.busy": "2025-07-15T05:39:43.785613Z",
     "iopub.status.idle": "2025-07-15T05:40:08.196115Z",
     "shell.execute_reply": "2025-07-15T05:40:08.195552Z",
     "shell.execute_reply.started": "2025-07-15T05:39:43.785931Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: LOG_LEVEL=INFO\n",
      "run sh: `/usr/local/bin/python /usr/local/lib/python3.10/site-packages/swift/cli/sft.py --learning_rate 0.1 --lora_rank 4 --num_train_epochs 1 --dataset ./resources/2_7/train_100.jsonl --batch_size 8 --max_length 512 --eval_step 1 --model_type qwen2_5-1_5b-instruct --model_id_or_path ./model`\n",
      "[INFO:swift] Successfully registered `/usr/local/lib/python3.10/site-packages/swift/llm/data/dataset_info.json`\n",
      "[INFO:swift] Start time of running main: 2025-07-15 13:39:51.537119\n",
      "[INFO:swift] Setting template_type: qwen2_5\n",
      "[INFO:swift] Setting args.lazy_tokenize: False\n",
      "[INFO:swift] Setting args.dataloader_num_workers: 1\n",
      "[INFO:swift] output_dir: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951\n",
      "[INFO:swift] args: SftArguments(model_type='qwen2_5-1_5b-instruct', model_id_or_path='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model', model_revision='master', full_determinism=False, sft_type='lora', freeze_parameters=[], freeze_vit=False, freeze_parameters_ratio=0.0, additional_trainable_parameters=[], tuner_backend='peft', template_type='qwen2_5', output_dir='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951', add_output_dir_suffix=True, ddp_backend=None, ddp_find_unused_parameters=None, ddp_broadcast_buffers=None, ddp_timeout=1800, seed=42, resume_from_checkpoint=None, resume_only_model=False, ignore_data_skip=False, dtype='bf16', packing=False, train_backend='transformers', tp=1, pp=1, min_lr=None, sequence_parallel=False, model_kwargs={}, loss_name=None, dataset=['./resources/2_7/train_100.jsonl'], val_dataset=[], dataset_seed=42, dataset_test_ratio=0.01, use_loss_scale=False, loss_scale_config_path='/usr/local/lib/python3.10/site-packages/swift/llm/agent/default_loss_scale_config.json', system=None, tools_prompt='react_en', max_length=512, truncation_strategy='delete', check_dataset_strategy='none', streaming=False, streaming_val_size=0, streaming_buffer_size=16384, model_name=[None, None], model_author=[None, None], quant_method=None, quantization_bit=0, hqq_axis=0, hqq_dynamic_config_path=None, bnb_4bit_comp_dtype='bf16', bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, rescale_image=-1, target_modules=['q_proj', 'k_proj', 'v_proj'], target_regex=None, modules_to_save=[], lora_rank=4, lora_alpha=32, lora_dropout=0.05, lora_bias_trainable='none', lora_dtype='AUTO', lora_lr_ratio=None, use_rslora=False, use_dora=False, init_lora_weights='true', fourier_n_frequency=2000, fourier_scaling=300.0, rope_scaling=None, boft_block_size=4, boft_block_num=0, boft_n_butterfly_factor=1, boft_dropout=0.0, vera_rank=256, vera_projection_prng_key=0, vera_dropout=0.0, vera_d_initial=0.1, adapter_act='gelu', adapter_length=128, use_galore=False, galore_target_modules=None, galore_rank=128, galore_update_proj_gap=50, galore_scale=1.0, galore_proj_type='std', galore_optim_per_parameter=False, galore_with_embedding=False, galore_quantization=False, galore_proj_quant=False, galore_proj_bits=4, galore_proj_group_size=256, galore_cos_threshold=0.4, galore_gamma_proj=2, galore_queue_size=5, adalora_target_r=8, adalora_init_r=12, adalora_tinit=0, adalora_tfinal=0, adalora_deltaT=1, adalora_beta1=0.85, adalora_beta2=0.85, adalora_orth_reg_weight=0.5, ia3_feedforward_modules=[], llamapro_num_new_blocks=4, llamapro_num_groups=None, neftune_noise_alpha=None, neftune_backend='transformers', lisa_activated_layers=0, lisa_step_interval=20, reft_layer_key=None, reft_layers=None, reft_rank=4, reft_intervention_type='LoreftIntervention', reft_args=None, use_liger=False, gradient_checkpointing=True, deepspeed=None, batch_size=8, eval_batch_size=8, auto_find_batch_size=False, num_train_epochs=1, max_steps=-1, optim='adamw_torch', adam_beta1=0.9, adam_beta2=0.95, adam_epsilon=1e-08, learning_rate=0.1, weight_decay=0.1, gradient_accumulation_steps=2, max_grad_norm=1, predict_with_generate=False, lr_scheduler_type='cosine', lr_scheduler_kwargs={}, warmup_ratio=0.05, warmup_steps=0, eval_steps=1, save_steps=1, save_only_model=False, save_total_limit=2, logging_steps=5, acc_steps=1, dataloader_num_workers=1, dataloader_pin_memory=True, dataloader_drop_last=False, push_to_hub=False, hub_model_id=None, hub_token=None, hub_private_repo=False, hub_strategy='every_save', test_oom_error=False, disable_tqdm=False, lazy_tokenize=False, preprocess_num_proc=1, use_flash_attn=None, ignore_args_error=False, check_model_is_latest=True, logging_dir='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951/runs', report_to=['tensorboard'], acc_strategy='token', save_on_each_node=False, evaluation_strategy='steps', save_strategy='steps', save_safetensors=True, gpu_memory_fraction=None, include_num_input_tokens_seen=False, local_repo_path=None, custom_register_path=None, custom_dataset_info=None, device_map_config=None, device_max_memory=[], max_new_tokens=2048, do_sample=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, fsdp='', fsdp_config=None, sequence_parallel_size=1, model_layer_cls_name=None, metric_warmup_step=0, fsdp_num=1, per_device_train_batch_size=None, per_device_eval_batch_size=None, eval_strategy=None, self_cognition_sample=0, train_dataset_mix_ratio=0.0, train_dataset_mix_ds=['ms-bench'], train_dataset_sample=-1, val_dataset_sample=None, safe_serialization=None, only_save_model=None, neftune_alpha=None, deepspeed_config_path=None, model_cache_dir=None, lora_dropout_p=None, lora_target_modules=[], lora_target_regex=None, lora_modules_to_save=[], boft_target_modules=[], boft_modules_to_save=[], vera_target_modules=[], vera_modules_to_save=[], ia3_target_modules=[], ia3_modules_to_save=[], custom_train_dataset_path=[], custom_val_dataset_path=[], device_map_config_path=None, push_hub_strategy=None)\n",
      "[INFO:swift] Global seed set to 42\n",
      "device_count: 1\n",
      "rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n",
      "[INFO:swift] Loading the model using model_dir: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model\n",
      "[INFO:swift] model_kwargs: {'device_map': 'cuda:0'}\n",
      "[INFO:swift] model.max_model_len: 32768\n",
      "[INFO:swift] model.hf_device_map: {'': device(type='cuda', index=0)}\n",
      "[INFO:swift] model_config: Qwen2Config {\n",
      "  \"_name_or_path\": \"/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO:swift] model.generation_config: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO:swift] target_modules: ['q_proj', 'k_proj', 'v_proj']\n",
      "[INFO:swift] modules_to_save: []\n",
      "[INFO:swift] lora_config: get_wrapped_class.<locals>.PeftWrapper(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=4, target_modules={'k_proj', 'v_proj', 'q_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=[], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_dtype=None, lorap_lr_ratio=None, lorap_emb_lr=1e-06)\n",
      "[INFO:swift] [base_model.model.model.embed_tokens.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.o_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.gate_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.up_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.down_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.input_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.post_attention_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] ...\n",
      "[INFO:swift] PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(151936, 1536)\n",
      "        (layers): ModuleList(\n",
      "          (0-27): 28 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2SdpaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "              (rotary_emb): Qwen2RotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "              (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (rotary_emb): Qwen2RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[INFO:swift] PeftModelForCausalLM: 1544.4598M Params (0.7455M Trainable [0.0483%]), 0.0019M Buffers.\n",
      "[INFO:swift] Setting model.config.use_cache: False\n",
      "[INFO:swift] system: You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "[INFO:swift] args.lazy_tokenize: False\n",
      "Generating train split: 100 examples [00:00, 53828.34 examples/s]\n",
      "Map: 100%|███████████████████████████| 100/100 [00:00<00:00, 7976.39 examples/s]\n",
      "Filter: 100%|███████████████████████| 100/100 [00:00<00:00, 18345.38 examples/s]\n",
      "[INFO:swift] train_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 99\n",
      "})\n",
      "[INFO:swift] val_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[INFO:swift] [INPUT_IDS] [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 2, 5097, 4956, 785, 2550, 1969, 25879, 311, 279, 1590, 4226, 9099, 2878, 5867, 596, 25, 24048, 3417, 1283, 32711, 624, 2, 8815, 22079, 4956, 40151, 1728, 10788, 220, 16, 20, 20044, 315, 39944, 33420, 311, 67508, 330, 44923, 60217, 1335, 323, 1283, 83609, 220, 16, 23, 60217, 11, 1340, 1483, 220, 16, 19, 13, 19, 20044, 315, 39944, 33420, 13, 2585, 1657, 20044, 315, 39944, 33420, 1558, 1817, 330, 44923, 50800, 1, 990, 30, 151645, 198, 151644, 77091, 198, 785, 3084, 315, 39944, 33420, 1483, 369, 1817, 330, 44923, 50800, 1, 284, 10657, 33420, 1483, 1683, 115, 5624, 315, 8453, 60217, 284, 220, 16, 19, 13, 19, 20044, 1683, 115, 220, 16, 23, 284, 220, 15, 13, 23, 20044, 817, 50800, 382, 2979, 596, 25, 15, 13, 23, 3417, 151645]\n",
      "[INFO:swift] [INPUT] <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "#Output#\n",
      "The output must conform to the final answer placed within {{ans:xxx}} after reasoning.\n",
      "#Math Problem#\n",
      "Grandma bought 15 meters of silk rope to weave \"Chinese knots,\" and after weaving 18 knots, she used 14.4 meters of silk rope. How many meters of silk rope does each \"Chinese knot\" use?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The length of silk rope used for each \"Chinese knot\" = Total rope used ÷ Number of Chinese knots = 14.4 meters ÷ 18 = 0.8 meters per knot.\n",
      "\n",
      "{{ans:0.8}}<|im_end|>\n",
      "[INFO:swift] [LABELS_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 785, 3084, 315, 39944, 33420, 1483, 369, 1817, 330, 44923, 50800, 1, 284, 10657, 33420, 1483, 1683, 115, 5624, 315, 8453, 60217, 284, 220, 16, 19, 13, 19, 20044, 1683, 115, 220, 16, 23, 284, 220, 15, 13, 23, 20044, 817, 50800, 382, 2979, 596, 25, 15, 13, 23, 3417, 151645]\n",
      "[INFO:swift] [LABELS] [-100 * 104]The length of silk rope used for each \"Chinese knot\" = Total rope used ÷ Number of Chinese knots = 14.4 meters ÷ 18 = 0.8 meters per knot.\n",
      "\n",
      "{{ans:0.8}}<|im_end|>\n",
      "Map: 100%|████████████████████████████████████| 99/99 [00:00<00:00, 1622.00it/s]\n",
      "Map: 100%|██████████████████████████████████████| 1/1 [00:00<00:00, 1670.37it/s]\n",
      "[INFO:swift] Dataset Token Length: 207.111111±68.192373, min=91.000000, max=486.000000, size=99\n",
      "[INFO:swift] Dataset Token Length: 171.000000±0.000000, min=171.000000, max=171.000000, size=1\n",
      "[INFO:swift] training_args: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "acc_strategy=token,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': False, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.95,\n",
      "adam_epsilon=1e-08,\n",
      "additional_saved_files=[],\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=42,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=1,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=1,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      ",\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.1,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951/runs,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "loss_name=None,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "metric_warmup_step=0,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=1,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_dataset_sample=-1,\n",
      "train_sampler_random=True,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      ")\n",
      "[INFO:swift] The SftArguments will be saved in: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951/sft_args.json\n",
      "[INFO:swift] The Seq2SeqTrainingArguments will be saved in: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951/training_args.json\n",
      "[INFO:swift] The logging file will be saved in: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951/logging.jsonl\n",
      "[2025-07-15 13:39:54,124] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Train:   0%|                                              | 0/6 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.85834581, 'acc': 0.78527278, 'grad_norm': 0.84269738, 'learning_rate': 0.1, 'memory(GiB)': 9.48, 'train_speed(iter/s)': 0.351349, 'epoch': 0.15, 'global_step/max_steps': '1/6', 'percentage': '16.67%', 'elapsed_time': '1s', 'remaining_time': '9s'}\n",
      "Train:  17%|██████▎                               | 1/6 [00:01<00:09,  1.88s/it]\n",
      "{'eval_loss': 0.52560061, 'eval_acc': 0.84210526, 'eval_runtime': 0.093, 'eval_samples_per_second': 10.753, 'eval_steps_per_second': 10.753, 'epoch': 0.15, 'global_step/max_steps': '1/6', 'percentage': '16.67%', 'elapsed_time': '1s', 'remaining_time': '9s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 607.78it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951/checkpoint-1\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Train:  33%|████████████▋                         | 2/6 [00:03<00:06,  1.55s/it]\n",
      "{'eval_loss': 9.64134216, 'eval_acc': 0.05263158, 'eval_runtime': 0.083, 'eval_samples_per_second': 12.045, 'eval_steps_per_second': 12.045, 'epoch': 0.31, 'global_step/max_steps': '2/6', 'percentage': '33.33%', 'elapsed_time': '3s', 'remaining_time': '6s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 651.09it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951/checkpoint-2\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Train:  50%|███████████████████                   | 3/6 [00:04<00:04,  1.42s/it]\n",
      "{'eval_loss': 18.33996773, 'eval_acc': 0.0, 'eval_runtime': 0.0823, 'eval_samples_per_second': 12.146, 'eval_steps_per_second': 12.146, 'epoch': 0.46, 'global_step/max_steps': '3/6', 'percentage': '50.00%', 'elapsed_time': '4s', 'remaining_time': '4s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 645.38it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951/checkpoint-3\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Train:  67%|█████████████████████████▎            | 4/6 [00:05<00:02,  1.38s/it]\n",
      "{'eval_loss': 17.12577248, 'eval_acc': 0.0, 'eval_runtime': 0.0885, 'eval_samples_per_second': 11.304, 'eval_steps_per_second': 11.304, 'epoch': 0.62, 'global_step/max_steps': '4/6', 'percentage': '66.67%', 'elapsed_time': '5s', 'remaining_time': '2s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 649.37it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951/checkpoint-4\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 11.46051788, 'acc': 0.21079765, 'grad_norm': 4527874048.0, 'learning_rate': 0.00954915, 'memory(GiB)': 16.09, 'train_speed(iter/s)': 0.590507, 'epoch': 0.77, 'global_step/max_steps': '5/6', 'percentage': '83.33%', 'elapsed_time': '7s', 'remaining_time': '1s'}\n",
      "Train:  83%|███████████████████████████████▋      | 5/6 [00:07<00:01,  1.50s/it]\n",
      "{'eval_loss': 15.85587597, 'eval_acc': 0.0, 'eval_runtime': 0.0889, 'eval_samples_per_second': 11.247, 'eval_steps_per_second': 11.247, 'epoch': 0.77, 'global_step/max_steps': '5/6', 'percentage': '83.33%', 'elapsed_time': '7s', 'remaining_time': '1s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 666.93it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951/checkpoint-5\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Train: 100%|██████████████████████████████████████| 6/6 [00:09<00:00,  1.60s/it]\n",
      "{'eval_loss': 16.07638741, 'eval_acc': 0.0, 'eval_runtime': 0.0879, 'eval_samples_per_second': 11.38, 'eval_steps_per_second': 11.38, 'epoch': 0.92, 'global_step/max_steps': '6/6', 'percentage': '100.00%', 'elapsed_time': '9s', 'remaining_time': '0s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 626.76it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951/checkpoint-6\n",
      "{'train_runtime': 9.439, 'train_samples_per_second': 10.488, 'train_steps_per_second': 0.636, 'train_loss': 10.6209645, 'epoch': 0.92, 'global_step/max_steps': '6/6', 'percentage': '100.00%', 'elapsed_time': '9s', 'remaining_time': '0s'}\n",
      "Train: 100%|██████████████████████████████████████| 6/6 [00:09<00:00,  1.57s/it]\n",
      "[INFO:swift] last_model_checkpoint: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951/checkpoint-6\n",
      "[INFO:swift] best_model_checkpoint: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951/checkpoint-1\n",
      "[INFO:swift] images_dir: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v2-20250715-133951/images\n",
      "[INFO:swift] End time of running main: 2025-07-15 13:40:06.347029\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env LOG_LEVEL=INFO\n",
    "!swift sft \\\n",
    "--learning_rate '0.1' \\\n",
    "--lora_rank 4 \\\n",
    "--num_train_epochs 1 \\\n",
    "--dataset './resources/2_7/train_100.jsonl' \\\n",
    "--batch_size '8' \\\n",
    "--max_length 512 \\\n",
    "--eval_step 1 \\\n",
    "--model_type 'qwen2_5-1_5b-instruct' \\\n",
    "--model_id_or_path './model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52b2e2e",
   "metadata": {},
   "source": [
    "| Training loss image | Evaluation loss image |\n",
    "| --- | --- |\n",
    "|<img src=\"https://img.alicdn.com/imgextra/i2/O1CN0122CqML1xiykiTglmo_!!6000000006478-2-tps-667-451.png\" style=\"width: 500px;display: block; margin-left: auto; margin-right: auto\"/> | <img src=\"https://img.alicdn.com/imgextra/i4/O1CN01AxXE0V1JqEORoVBdi_!!6000000001079-2-tps-667-451.png\" style=\"width: 500px;display: block; margin-left: auto; margin-right: auto\"/> |\n",
    "\n",
    "| **Observation metrics (training loss, validation loss):** | Training loss increases, validation loss increases |\n",
    "| --- | --- |\n",
    "| **Training status:** | **Training failed** |\n",
    "| **Cause analysis:** | It is highly likely that the learning rate is too high, causing the model parameters to oscillate repeatedly near the optimal solution and fail to find the optimal solution, resulting in training failure.<img src=\"https://img.alicdn.com/imgextra/i1/O1CN01l4leTB1LKI0BcVs16_!!6000000001280-2-tps-1658-1262.png\" style=\"width: 300px;display: block; margin-left: auto; margin-right: auto\"/>|\n",
    "| **Adjustment logic:** | Significantly reduce the learning rate to $0.00005$, allowing the model to \"learn cautiously\" with smaller steps. |\n",
    "\n",
    "#### 3.3.2 Second Experiment (requires 2 minutes)\n",
    "\n",
    "<div style=\"width: 30%\">\n",
    "    \n",
    "| Parameter | Old parameter value | New parameter value |\n",
    "| --- | --- | --- |\n",
    "| Learning rate (learning_rate) | 0.1 $ $ | 0.00005 |\n",
    "    \n",
    "</div>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "053c0d3d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-07-15T05:40:24.914984Z",
     "iopub.status.busy": "2025-07-15T05:40:24.914644Z",
     "iopub.status.idle": "2025-07-15T05:40:49.090159Z",
     "shell.execute_reply": "2025-07-15T05:40:49.089605Z",
     "shell.execute_reply.started": "2025-07-15T05:40:24.914963Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: LOG_LEVEL=INFO\n",
      "run sh: `/usr/local/bin/python /usr/local/lib/python3.10/site-packages/swift/cli/sft.py --learning_rate 0.00005 --lora_rank 4 --num_train_epochs 1 --dataset ./resources/2_7/train_100.jsonl --batch_size 8 --max_length 512 --eval_step 1 --model_type qwen2_5-1_5b-instruct --model_id_or_path ./model`\n",
      "[INFO:swift] Successfully registered `/usr/local/lib/python3.10/site-packages/swift/llm/data/dataset_info.json`\n",
      "[INFO:swift] Start time of running main: 2025-07-15 13:40:32.669175\n",
      "[INFO:swift] Setting template_type: qwen2_5\n",
      "[INFO:swift] Setting args.lazy_tokenize: False\n",
      "[INFO:swift] Setting args.dataloader_num_workers: 1\n",
      "[INFO:swift] output_dir: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032\n",
      "[INFO:swift] args: SftArguments(model_type='qwen2_5-1_5b-instruct', model_id_or_path='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model', model_revision='master', full_determinism=False, sft_type='lora', freeze_parameters=[], freeze_vit=False, freeze_parameters_ratio=0.0, additional_trainable_parameters=[], tuner_backend='peft', template_type='qwen2_5', output_dir='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032', add_output_dir_suffix=True, ddp_backend=None, ddp_find_unused_parameters=None, ddp_broadcast_buffers=None, ddp_timeout=1800, seed=42, resume_from_checkpoint=None, resume_only_model=False, ignore_data_skip=False, dtype='bf16', packing=False, train_backend='transformers', tp=1, pp=1, min_lr=None, sequence_parallel=False, model_kwargs={}, loss_name=None, dataset=['./resources/2_7/train_100.jsonl'], val_dataset=[], dataset_seed=42, dataset_test_ratio=0.01, use_loss_scale=False, loss_scale_config_path='/usr/local/lib/python3.10/site-packages/swift/llm/agent/default_loss_scale_config.json', system=None, tools_prompt='react_en', max_length=512, truncation_strategy='delete', check_dataset_strategy='none', streaming=False, streaming_val_size=0, streaming_buffer_size=16384, model_name=[None, None], model_author=[None, None], quant_method=None, quantization_bit=0, hqq_axis=0, hqq_dynamic_config_path=None, bnb_4bit_comp_dtype='bf16', bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, rescale_image=-1, target_modules=['q_proj', 'k_proj', 'v_proj'], target_regex=None, modules_to_save=[], lora_rank=4, lora_alpha=32, lora_dropout=0.05, lora_bias_trainable='none', lora_dtype='AUTO', lora_lr_ratio=None, use_rslora=False, use_dora=False, init_lora_weights='true', fourier_n_frequency=2000, fourier_scaling=300.0, rope_scaling=None, boft_block_size=4, boft_block_num=0, boft_n_butterfly_factor=1, boft_dropout=0.0, vera_rank=256, vera_projection_prng_key=0, vera_dropout=0.0, vera_d_initial=0.1, adapter_act='gelu', adapter_length=128, use_galore=False, galore_target_modules=None, galore_rank=128, galore_update_proj_gap=50, galore_scale=1.0, galore_proj_type='std', galore_optim_per_parameter=False, galore_with_embedding=False, galore_quantization=False, galore_proj_quant=False, galore_proj_bits=4, galore_proj_group_size=256, galore_cos_threshold=0.4, galore_gamma_proj=2, galore_queue_size=5, adalora_target_r=8, adalora_init_r=12, adalora_tinit=0, adalora_tfinal=0, adalora_deltaT=1, adalora_beta1=0.85, adalora_beta2=0.85, adalora_orth_reg_weight=0.5, ia3_feedforward_modules=[], llamapro_num_new_blocks=4, llamapro_num_groups=None, neftune_noise_alpha=None, neftune_backend='transformers', lisa_activated_layers=0, lisa_step_interval=20, reft_layer_key=None, reft_layers=None, reft_rank=4, reft_intervention_type='LoreftIntervention', reft_args=None, use_liger=False, gradient_checkpointing=True, deepspeed=None, batch_size=8, eval_batch_size=8, auto_find_batch_size=False, num_train_epochs=1, max_steps=-1, optim='adamw_torch', adam_beta1=0.9, adam_beta2=0.95, adam_epsilon=1e-08, learning_rate=5e-05, weight_decay=0.1, gradient_accumulation_steps=2, max_grad_norm=1, predict_with_generate=False, lr_scheduler_type='cosine', lr_scheduler_kwargs={}, warmup_ratio=0.05, warmup_steps=0, eval_steps=1, save_steps=1, save_only_model=False, save_total_limit=2, logging_steps=5, acc_steps=1, dataloader_num_workers=1, dataloader_pin_memory=True, dataloader_drop_last=False, push_to_hub=False, hub_model_id=None, hub_token=None, hub_private_repo=False, hub_strategy='every_save', test_oom_error=False, disable_tqdm=False, lazy_tokenize=False, preprocess_num_proc=1, use_flash_attn=None, ignore_args_error=False, check_model_is_latest=True, logging_dir='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032/runs', report_to=['tensorboard'], acc_strategy='token', save_on_each_node=False, evaluation_strategy='steps', save_strategy='steps', save_safetensors=True, gpu_memory_fraction=None, include_num_input_tokens_seen=False, local_repo_path=None, custom_register_path=None, custom_dataset_info=None, device_map_config=None, device_max_memory=[], max_new_tokens=2048, do_sample=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, fsdp='', fsdp_config=None, sequence_parallel_size=1, model_layer_cls_name=None, metric_warmup_step=0, fsdp_num=1, per_device_train_batch_size=None, per_device_eval_batch_size=None, eval_strategy=None, self_cognition_sample=0, train_dataset_mix_ratio=0.0, train_dataset_mix_ds=['ms-bench'], train_dataset_sample=-1, val_dataset_sample=None, safe_serialization=None, only_save_model=None, neftune_alpha=None, deepspeed_config_path=None, model_cache_dir=None, lora_dropout_p=None, lora_target_modules=[], lora_target_regex=None, lora_modules_to_save=[], boft_target_modules=[], boft_modules_to_save=[], vera_target_modules=[], vera_modules_to_save=[], ia3_target_modules=[], ia3_modules_to_save=[], custom_train_dataset_path=[], custom_val_dataset_path=[], device_map_config_path=None, push_hub_strategy=None)\n",
      "[INFO:swift] Global seed set to 42\n",
      "device_count: 1\n",
      "rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n",
      "[INFO:swift] Loading the model using model_dir: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model\n",
      "[INFO:swift] model_kwargs: {'device_map': 'cuda:0'}\n",
      "[INFO:swift] model.max_model_len: 32768\n",
      "[INFO:swift] model.hf_device_map: {'': device(type='cuda', index=0)}\n",
      "[INFO:swift] model_config: Qwen2Config {\n",
      "  \"_name_or_path\": \"/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO:swift] model.generation_config: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO:swift] target_modules: ['q_proj', 'k_proj', 'v_proj']\n",
      "[INFO:swift] modules_to_save: []\n",
      "[INFO:swift] lora_config: get_wrapped_class.<locals>.PeftWrapper(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=4, target_modules={'k_proj', 'q_proj', 'v_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=[], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_dtype=None, lorap_lr_ratio=None, lorap_emb_lr=1e-06)\n",
      "[INFO:swift] [base_model.model.model.embed_tokens.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.o_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.gate_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.up_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.down_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.input_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.post_attention_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] ...\n",
      "[INFO:swift] PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(151936, 1536)\n",
      "        (layers): ModuleList(\n",
      "          (0-27): 28 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2SdpaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "              (rotary_emb): Qwen2RotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "              (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (rotary_emb): Qwen2RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[INFO:swift] PeftModelForCausalLM: 1544.4598M Params (0.7455M Trainable [0.0483%]), 0.0019M Buffers.\n",
      "[INFO:swift] Setting model.config.use_cache: False\n",
      "[INFO:swift] system: You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "[INFO:swift] args.lazy_tokenize: False\n",
      "Map: 100%|███████████████████████████| 100/100 [00:00<00:00, 8107.29 examples/s]\n",
      "[INFO:swift] train_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 99\n",
      "})\n",
      "[INFO:swift] val_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[INFO:swift] [INPUT_IDS] [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 2, 5097, 4956, 785, 2550, 1969, 25879, 311, 279, 1590, 4226, 9099, 2878, 5867, 596, 25, 24048, 3417, 1283, 32711, 624, 2, 8815, 22079, 4956, 40151, 1728, 10788, 220, 16, 20, 20044, 315, 39944, 33420, 311, 67508, 330, 44923, 60217, 1335, 323, 1283, 83609, 220, 16, 23, 60217, 11, 1340, 1483, 220, 16, 19, 13, 19, 20044, 315, 39944, 33420, 13, 2585, 1657, 20044, 315, 39944, 33420, 1558, 1817, 330, 44923, 50800, 1, 990, 30, 151645, 198, 151644, 77091, 198, 785, 3084, 315, 39944, 33420, 1483, 369, 1817, 330, 44923, 50800, 1, 284, 10657, 33420, 1483, 1683, 115, 5624, 315, 8453, 60217, 284, 220, 16, 19, 13, 19, 20044, 1683, 115, 220, 16, 23, 284, 220, 15, 13, 23, 20044, 817, 50800, 382, 2979, 596, 25, 15, 13, 23, 3417, 151645]\n",
      "[INFO:swift] [INPUT] <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "#Output#\n",
      "The output must conform to the final answer placed within {{ans:xxx}} after reasoning.\n",
      "#Math Problem#\n",
      "Grandma bought 15 meters of silk rope to weave \"Chinese knots,\" and after weaving 18 knots, she used 14.4 meters of silk rope. How many meters of silk rope does each \"Chinese knot\" use?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The length of silk rope used for each \"Chinese knot\" = Total rope used ÷ Number of Chinese knots = 14.4 meters ÷ 18 = 0.8 meters per knot.\n",
      "\n",
      "{{ans:0.8}}<|im_end|>\n",
      "[INFO:swift] [LABELS_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 785, 3084, 315, 39944, 33420, 1483, 369, 1817, 330, 44923, 50800, 1, 284, 10657, 33420, 1483, 1683, 115, 5624, 315, 8453, 60217, 284, 220, 16, 19, 13, 19, 20044, 1683, 115, 220, 16, 23, 284, 220, 15, 13, 23, 20044, 817, 50800, 382, 2979, 596, 25, 15, 13, 23, 3417, 151645]\n",
      "[INFO:swift] [LABELS] [-100 * 104]The length of silk rope used for each \"Chinese knot\" = Total rope used ÷ Number of Chinese knots = 14.4 meters ÷ 18 = 0.8 meters per knot.\n",
      "\n",
      "{{ans:0.8}}<|im_end|>\n",
      "Map: 100%|████████████████████████████████████| 99/99 [00:00<00:00, 1591.49it/s]\n",
      "Map: 100%|██████████████████████████████████████| 1/1 [00:00<00:00, 1644.83it/s]\n",
      "[INFO:swift] Dataset Token Length: 207.111111±68.192373, min=91.000000, max=486.000000, size=99\n",
      "[INFO:swift] Dataset Token Length: 171.000000±0.000000, min=171.000000, max=171.000000, size=1\n",
      "[INFO:swift] training_args: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "acc_strategy=token,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': False, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.95,\n",
      "adam_epsilon=1e-08,\n",
      "additional_saved_files=[],\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=42,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=1,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=1,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      ",\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032/runs,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "loss_name=None,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "metric_warmup_step=0,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=1,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_dataset_sample=-1,\n",
      "train_sampler_random=True,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      ")\n",
      "[INFO:swift] The SftArguments will be saved in: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032/sft_args.json\n",
      "[INFO:swift] The Seq2SeqTrainingArguments will be saved in: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032/training_args.json\n",
      "[INFO:swift] The logging file will be saved in: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032/logging.jsonl\n",
      "[2025-07-15 13:40:35,106] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Train:   0%|                                              | 0/6 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.85834581, 'acc': 0.78527278, 'grad_norm': 0.84274334, 'learning_rate': 5e-05, 'memory(GiB)': 9.48, 'train_speed(iter/s)': 0.352995, 'epoch': 0.15, 'global_step/max_steps': '1/6', 'percentage': '16.67%', 'elapsed_time': '1s', 'remaining_time': '9s'}\n",
      "Train:  17%|██████▎                               | 1/6 [00:01<00:09,  1.87s/it]\n",
      "{'eval_loss': 0.52560061, 'eval_acc': 0.84210526, 'eval_runtime': 0.0925, 'eval_samples_per_second': 10.807, 'eval_steps_per_second': 10.807, 'epoch': 0.15, 'global_step/max_steps': '1/6', 'percentage': '16.67%', 'elapsed_time': '1s', 'remaining_time': '9s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 593.42it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032/checkpoint-1\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Train:  33%|████████████▋                         | 2/6 [00:03<00:06,  1.55s/it]\n",
      "{'eval_loss': 0.51793379, 'eval_acc': 0.84210526, 'eval_runtime': 0.083, 'eval_samples_per_second': 12.044, 'eval_steps_per_second': 12.044, 'epoch': 0.31, 'global_step/max_steps': '2/6', 'percentage': '33.33%', 'elapsed_time': '3s', 'remaining_time': '6s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 640.45it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032/checkpoint-2\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Train:  50%|███████████████████                   | 3/6 [00:04<00:04,  1.42s/it]\n",
      "{'eval_loss': 0.51218975, 'eval_acc': 0.84210526, 'eval_runtime': 0.082, 'eval_samples_per_second': 12.188, 'eval_steps_per_second': 12.188, 'epoch': 0.46, 'global_step/max_steps': '3/6', 'percentage': '50.00%', 'elapsed_time': '4s', 'remaining_time': '4s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 639.77it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032/checkpoint-3\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Train:  67%|█████████████████████████▎            | 4/6 [00:05<00:02,  1.38s/it]\n",
      "{'eval_loss': 0.51007485, 'eval_acc': 0.84210526, 'eval_runtime': 0.0829, 'eval_samples_per_second': 12.064, 'eval_steps_per_second': 12.064, 'epoch': 0.62, 'global_step/max_steps': '4/6', 'percentage': '66.67%', 'elapsed_time': '5s', 'remaining_time': '2s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 638.69it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032/checkpoint-4\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.78638732, 'acc': 0.80049992, 'grad_norm': 0.90170425, 'learning_rate': 4.77e-06, 'memory(GiB)': 16.09, 'train_speed(iter/s)': 0.590364, 'epoch': 0.77, 'global_step/max_steps': '5/6', 'percentage': '83.33%', 'elapsed_time': '7s', 'remaining_time': '1s'}\n",
      "Train:  83%|███████████████████████████████▋      | 5/6 [00:07<00:01,  1.50s/it]\n",
      "{'eval_loss': 0.50981921, 'eval_acc': 0.84210526, 'eval_runtime': 0.0829, 'eval_samples_per_second': 12.059, 'eval_steps_per_second': 12.059, 'epoch': 0.77, 'global_step/max_steps': '5/6', 'percentage': '83.33%', 'elapsed_time': '7s', 'remaining_time': '1s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 677.81it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032/checkpoint-5\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Train: 100%|██████████████████████████████████████| 6/6 [00:09<00:00,  1.60s/it]\n",
      "{'eval_loss': 0.50845259, 'eval_acc': 0.85263158, 'eval_runtime': 0.085, 'eval_samples_per_second': 11.763, 'eval_steps_per_second': 11.763, 'epoch': 0.92, 'global_step/max_steps': '6/6', 'percentage': '100.00%', 'elapsed_time': '9s', 'remaining_time': '0s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 655.77it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032/checkpoint-6\n",
      "{'train_runtime': 9.4376, 'train_samples_per_second': 10.49, 'train_steps_per_second': 0.636, 'train_loss': 0.8020139, 'epoch': 0.92, 'global_step/max_steps': '6/6', 'percentage': '100.00%', 'elapsed_time': '9s', 'remaining_time': '0s'}\n",
      "Train: 100%|██████████████████████████████████████| 6/6 [00:09<00:00,  1.57s/it]\n",
      "[INFO:swift] last_model_checkpoint: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032/checkpoint-6\n",
      "[INFO:swift] best_model_checkpoint: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032/checkpoint-6\n",
      "[INFO:swift] images_dir: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v3-20250715-134032/images\n",
      "[INFO:swift] End time of running main: 2025-07-15 13:40:47.329116\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env LOG_LEVEL=INFO\n",
    "!swift sft \\\n",
    "--learning_rate '0.00005' \\\n",
    "--lora_rank 4 \\\n",
    "--num_train_epochs 1 \\\n",
    "--dataset './resources/2_7/train_100.jsonl' \\\n",
    "--batch_size '8' \\\n",
    "--max_length 512 \\\n",
    "--eval_step 1 \\\n",
    "--model_type 'qwen2_5-1_5b-instruct' \\\n",
    "--model_id_or_path './model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af933c4",
   "metadata": {},
   "source": [
    "| Training loss image | Evaluation loss image |\n",
    "| --- | --- |\n",
    "|<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01DgtNVX1EDgzHYamOE_!!6000000000318-2-tps-680-451.png\" style=\"width: 500px;display: block; margin-left: auto; margin-right: auto\"/> | <img src=\"https://img.alicdn.com/imgextra/i3/O1CN01621v4k1ErzqC24Z1b_!!6000000000406-2-tps-689-451.png\" style=\"width: 500px;display: block; margin-left: auto; margin-right: auto\"/> |\n",
    "\n",
    "| **Observation Metrics (Training Loss, Validation Loss):** | Training loss decreases, validation loss also decreases |\n",
    "| --- | --- |\n",
    "| **Training Status:** | **Underfitting** |\n",
    "| **Cause Analysis:** | Underfitting is a very common phenomenon during training. It indicates that, with the parameters unchanged, simply allowing the model to train longer can lead to successful training. Of course, modifying the parameters can also accelerate the training process. |\n",
    "| **Adjustment Logic:** | 1. Let the model train longer: Increase the number of dataset learning cycles `epoch` to 50. <br/> 2. Adjust `batch_size` to the maximum value of 16 to speed up model training. |\n",
    "\n",
    "#### 3.3.3 Third Experiment (Takes 10 minutes)\n",
    "\n",
    "<div style=\"width: 50%\">\n",
    "\n",
    "| Parameter | Old Parameter Value | New Parameter Value |\n",
    "| :--- | :--- | :--- |\n",
    "| Number of Training Epochs (num_train_epochs) | 1 | 50 |\n",
    "| batch_size | 8 | 16 |\n",
    "| eval_step | 1 | 20 (Optimized output display) |\n",
    "\n",
    "</div> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "647638fe",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-07-15T05:43:16.687835Z",
     "iopub.status.busy": "2025-07-15T05:43:16.687479Z",
     "iopub.status.idle": "2025-07-15T05:44:31.312976Z",
     "shell.execute_reply": "2025-07-15T05:44:31.312405Z",
     "shell.execute_reply.started": "2025-07-15T05:43:16.687813Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: LOG_LEVEL=INFO\n",
      "run sh: `/usr/local/bin/python /usr/local/lib/python3.10/site-packages/swift/cli/sft.py --learning_rate 0.00005 --lora_rank 4 --num_train_epochs 50 --dataset ./resources/2_7/train_100.jsonl --batch_size 16 --max_length 512 --eval_step 20 --model_type qwen2_5-1_5b-instruct --model_id_or_path ./model`\n",
      "[INFO:swift] Successfully registered `/usr/local/lib/python3.10/site-packages/swift/llm/data/dataset_info.json`\n",
      "[INFO:swift] Start time of running main: 2025-07-15 13:43:24.447460\n",
      "[INFO:swift] Setting template_type: qwen2_5\n",
      "[INFO:swift] Setting args.lazy_tokenize: False\n",
      "[INFO:swift] Setting args.dataloader_num_workers: 1\n",
      "[INFO:swift] output_dir: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v4-20250715-134324\n",
      "[INFO:swift] args: SftArguments(model_type='qwen2_5-1_5b-instruct', model_id_or_path='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model', model_revision='master', full_determinism=False, sft_type='lora', freeze_parameters=[], freeze_vit=False, freeze_parameters_ratio=0.0, additional_trainable_parameters=[], tuner_backend='peft', template_type='qwen2_5', output_dir='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v4-20250715-134324', add_output_dir_suffix=True, ddp_backend=None, ddp_find_unused_parameters=None, ddp_broadcast_buffers=None, ddp_timeout=1800, seed=42, resume_from_checkpoint=None, resume_only_model=False, ignore_data_skip=False, dtype='bf16', packing=False, train_backend='transformers', tp=1, pp=1, min_lr=None, sequence_parallel=False, model_kwargs={}, loss_name=None, dataset=['./resources/2_7/train_100.jsonl'], val_dataset=[], dataset_seed=42, dataset_test_ratio=0.01, use_loss_scale=False, loss_scale_config_path='/usr/local/lib/python3.10/site-packages/swift/llm/agent/default_loss_scale_config.json', system=None, tools_prompt='react_en', max_length=512, truncation_strategy='delete', check_dataset_strategy='none', streaming=False, streaming_val_size=0, streaming_buffer_size=16384, model_name=[None, None], model_author=[None, None], quant_method=None, quantization_bit=0, hqq_axis=0, hqq_dynamic_config_path=None, bnb_4bit_comp_dtype='bf16', bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, rescale_image=-1, target_modules=['q_proj', 'k_proj', 'v_proj'], target_regex=None, modules_to_save=[], lora_rank=4, lora_alpha=32, lora_dropout=0.05, lora_bias_trainable='none', lora_dtype='AUTO', lora_lr_ratio=None, use_rslora=False, use_dora=False, init_lora_weights='true', fourier_n_frequency=2000, fourier_scaling=300.0, rope_scaling=None, boft_block_size=4, boft_block_num=0, boft_n_butterfly_factor=1, boft_dropout=0.0, vera_rank=256, vera_projection_prng_key=0, vera_dropout=0.0, vera_d_initial=0.1, adapter_act='gelu', adapter_length=128, use_galore=False, galore_target_modules=None, galore_rank=128, galore_update_proj_gap=50, galore_scale=1.0, galore_proj_type='std', galore_optim_per_parameter=False, galore_with_embedding=False, galore_quantization=False, galore_proj_quant=False, galore_proj_bits=4, galore_proj_group_size=256, galore_cos_threshold=0.4, galore_gamma_proj=2, galore_queue_size=5, adalora_target_r=8, adalora_init_r=12, adalora_tinit=0, adalora_tfinal=0, adalora_deltaT=1, adalora_beta1=0.85, adalora_beta2=0.85, adalora_orth_reg_weight=0.5, ia3_feedforward_modules=[], llamapro_num_new_blocks=4, llamapro_num_groups=None, neftune_noise_alpha=None, neftune_backend='transformers', lisa_activated_layers=0, lisa_step_interval=20, reft_layer_key=None, reft_layers=None, reft_rank=4, reft_intervention_type='LoreftIntervention', reft_args=None, use_liger=False, gradient_checkpointing=True, deepspeed=None, batch_size=16, eval_batch_size=16, auto_find_batch_size=False, num_train_epochs=50, max_steps=-1, optim='adamw_torch', adam_beta1=0.9, adam_beta2=0.95, adam_epsilon=1e-08, learning_rate=5e-05, weight_decay=0.1, gradient_accumulation_steps=1, max_grad_norm=1, predict_with_generate=False, lr_scheduler_type='cosine', lr_scheduler_kwargs={}, warmup_ratio=0.05, warmup_steps=0, eval_steps=20, save_steps=20, save_only_model=False, save_total_limit=2, logging_steps=5, acc_steps=1, dataloader_num_workers=1, dataloader_pin_memory=True, dataloader_drop_last=False, push_to_hub=False, hub_model_id=None, hub_token=None, hub_private_repo=False, hub_strategy='every_save', test_oom_error=False, disable_tqdm=False, lazy_tokenize=False, preprocess_num_proc=1, use_flash_attn=None, ignore_args_error=False, check_model_is_latest=True, logging_dir='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v4-20250715-134324/runs', report_to=['tensorboard'], acc_strategy='token', save_on_each_node=False, evaluation_strategy='steps', save_strategy='steps', save_safetensors=True, gpu_memory_fraction=None, include_num_input_tokens_seen=False, local_repo_path=None, custom_register_path=None, custom_dataset_info=None, device_map_config=None, device_max_memory=[], max_new_tokens=2048, do_sample=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, fsdp='', fsdp_config=None, sequence_parallel_size=1, model_layer_cls_name=None, metric_warmup_step=0, fsdp_num=1, per_device_train_batch_size=None, per_device_eval_batch_size=None, eval_strategy=None, self_cognition_sample=0, train_dataset_mix_ratio=0.0, train_dataset_mix_ds=['ms-bench'], train_dataset_sample=-1, val_dataset_sample=None, safe_serialization=None, only_save_model=None, neftune_alpha=None, deepspeed_config_path=None, model_cache_dir=None, lora_dropout_p=None, lora_target_modules=[], lora_target_regex=None, lora_modules_to_save=[], boft_target_modules=[], boft_modules_to_save=[], vera_target_modules=[], vera_modules_to_save=[], ia3_target_modules=[], ia3_modules_to_save=[], custom_train_dataset_path=[], custom_val_dataset_path=[], device_map_config_path=None, push_hub_strategy=None)\n",
      "[INFO:swift] Global seed set to 42\n",
      "device_count: 1\n",
      "rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n",
      "[INFO:swift] Loading the model using model_dir: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model\n",
      "[INFO:swift] model_kwargs: {'device_map': 'cuda:0'}\n",
      "[INFO:swift] model.max_model_len: 32768\n",
      "[INFO:swift] model.hf_device_map: {'': device(type='cuda', index=0)}\n",
      "[INFO:swift] model_config: Qwen2Config {\n",
      "  \"_name_or_path\": \"/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO:swift] model.generation_config: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO:swift] target_modules: ['q_proj', 'k_proj', 'v_proj']\n",
      "[INFO:swift] modules_to_save: []\n",
      "[INFO:swift] lora_config: get_wrapped_class.<locals>.PeftWrapper(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=4, target_modules={'v_proj', 'q_proj', 'k_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=[], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_dtype=None, lorap_lr_ratio=None, lorap_emb_lr=1e-06)\n",
      "[INFO:swift] [base_model.model.model.embed_tokens.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.o_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.gate_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.up_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.down_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.input_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.post_attention_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] ...\n",
      "[INFO:swift] PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(151936, 1536)\n",
      "        (layers): ModuleList(\n",
      "          (0-27): 28 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2SdpaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "              (rotary_emb): Qwen2RotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "              (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (rotary_emb): Qwen2RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[INFO:swift] PeftModelForCausalLM: 1544.4598M Params (0.7455M Trainable [0.0483%]), 0.0019M Buffers.\n",
      "[INFO:swift] Setting model.config.use_cache: False\n",
      "[INFO:swift] system: You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "[INFO:swift] args.lazy_tokenize: False\n",
      "Map: 100%|███████████████████████████| 100/100 [00:00<00:00, 8076.38 examples/s]\n",
      "[INFO:swift] train_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 99\n",
      "})\n",
      "[INFO:swift] val_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[INFO:swift] [INPUT_IDS] [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 2, 5097, 4956, 785, 2550, 1969, 25879, 311, 279, 1590, 4226, 9099, 2878, 5867, 596, 25, 24048, 3417, 1283, 32711, 624, 2, 8815, 22079, 4956, 40151, 1728, 10788, 220, 16, 20, 20044, 315, 39944, 33420, 311, 67508, 330, 44923, 60217, 1335, 323, 1283, 83609, 220, 16, 23, 60217, 11, 1340, 1483, 220, 16, 19, 13, 19, 20044, 315, 39944, 33420, 13, 2585, 1657, 20044, 315, 39944, 33420, 1558, 1817, 330, 44923, 50800, 1, 990, 30, 151645, 198, 151644, 77091, 198, 785, 3084, 315, 39944, 33420, 1483, 369, 1817, 330, 44923, 50800, 1, 284, 10657, 33420, 1483, 1683, 115, 5624, 315, 8453, 60217, 284, 220, 16, 19, 13, 19, 20044, 1683, 115, 220, 16, 23, 284, 220, 15, 13, 23, 20044, 817, 50800, 382, 2979, 596, 25, 15, 13, 23, 3417, 151645]\n",
      "[INFO:swift] [INPUT] <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "#Output#\n",
      "The output must conform to the final answer placed within {{ans:xxx}} after reasoning.\n",
      "#Math Problem#\n",
      "Grandma bought 15 meters of silk rope to weave \"Chinese knots,\" and after weaving 18 knots, she used 14.4 meters of silk rope. How many meters of silk rope does each \"Chinese knot\" use?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The length of silk rope used for each \"Chinese knot\" = Total rope used ÷ Number of Chinese knots = 14.4 meters ÷ 18 = 0.8 meters per knot.\n",
      "\n",
      "{{ans:0.8}}<|im_end|>\n",
      "[INFO:swift] [LABELS_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 785, 3084, 315, 39944, 33420, 1483, 369, 1817, 330, 44923, 50800, 1, 284, 10657, 33420, 1483, 1683, 115, 5624, 315, 8453, 60217, 284, 220, 16, 19, 13, 19, 20044, 1683, 115, 220, 16, 23, 284, 220, 15, 13, 23, 20044, 817, 50800, 382, 2979, 596, 25, 15, 13, 23, 3417, 151645]\n",
      "[INFO:swift] [LABELS] [-100 * 104]The length of silk rope used for each \"Chinese knot\" = Total rope used ÷ Number of Chinese knots = 14.4 meters ÷ 18 = 0.8 meters per knot.\n",
      "\n",
      "{{ans:0.8}}<|im_end|>\n",
      "Map: 100%|████████████████████████████████████| 99/99 [00:00<00:00, 1635.59it/s]\n",
      "Map: 100%|██████████████████████████████████████| 1/1 [00:00<00:00, 1676.38it/s]\n",
      "[INFO:swift] Dataset Token Length: 207.111111±68.192373, min=91.000000, max=486.000000, size=99\n",
      "[INFO:swift] Dataset Token Length: 171.000000±0.000000, min=171.000000, max=171.000000, size=1\n",
      "[INFO:swift] training_args: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "acc_strategy=token,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': False, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.95,\n",
      "adam_epsilon=1e-08,\n",
      "additional_saved_files=[],\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=42,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=1,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=20,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      ",\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v4-20250715-134324/runs,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "loss_name=None,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "metric_warmup_step=0,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=50,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v4-20250715-134324,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v4-20250715-134324,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=20,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_dataset_sample=-1,\n",
      "train_sampler_random=True,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      ")\n",
      "[INFO:swift] The SftArguments will be saved in: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v4-20250715-134324/sft_args.json\n",
      "[INFO:swift] The Seq2SeqTrainingArguments will be saved in: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v4-20250715-134324/training_args.json\n",
      "[INFO:swift] The logging file will be saved in: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v4-20250715-134324/logging.jsonl\n",
      "[2025-07-15 13:43:26,943] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Train:   0%|                                            | 0/350 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.85834426, 'acc': 0.7853592, 'grad_norm': 0.84241009, 'learning_rate': 2.78e-06, 'memory(GiB)': 15.86, 'train_speed(iter/s)': 0.350937, 'epoch': 0.14, 'global_step/max_steps': '1/350', 'percentage': '0.29%', 'elapsed_time': '1s', 'remaining_time': '10m 59s'}\n",
      "{'loss': 0.79115528, 'acc': 0.80082273, 'grad_norm': 0.92507082, 'learning_rate': 1.389e-05, 'memory(GiB)': 20.27, 'train_speed(iter/s)': 0.554174, 'epoch': 0.71, 'global_step/max_steps': '5/350', 'percentage': '1.43%', 'elapsed_time': '8s', 'remaining_time': '9m 16s'}\n",
      "{'loss': 0.74317207, 'acc': 0.80934076, 'grad_norm': 1.04584777, 'learning_rate': 2.778e-05, 'memory(GiB)': 17.21, 'train_speed(iter/s)': 0.652645, 'epoch': 1.43, 'global_step/max_steps': '10/350', 'percentage': '2.86%', 'elapsed_time': '14s', 'remaining_time': '8m 8s'}\n",
      "{'loss': 0.80570374, 'acc': 0.79645138, 'grad_norm': 1.07175124, 'learning_rate': 4.167e-05, 'memory(GiB)': 17.21, 'train_speed(iter/s)': 0.677988, 'epoch': 2.14, 'global_step/max_steps': '15/350', 'percentage': '4.29%', 'elapsed_time': '21s', 'remaining_time': '7m 52s'}\n",
      "{'loss': 0.73347702, 'acc': 0.80475693, 'grad_norm': 0.8867659, 'learning_rate': 5e-05, 'memory(GiB)': 17.21, 'train_speed(iter/s)': 0.667108, 'epoch': 2.86, 'global_step/max_steps': '20/350', 'percentage': '5.71%', 'elapsed_time': '29s', 'remaining_time': '7m 58s'}\n",
      "Train:   6%|██                                 | 20/350 [00:29<08:12,  1.49s/it]\n",
      "{'eval_loss': 0.47165161, 'eval_acc': 0.85263158, 'eval_runtime': 0.0883, 'eval_samples_per_second': 11.322, 'eval_steps_per_second': 11.322, 'epoch': 2.86, 'global_step/max_steps': '20/350', 'percentage': '5.71%', 'elapsed_time': '29s', 'remaining_time': '8m 0s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 612.22it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v4-20250715-134324/checkpoint-20\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.70895538, 'acc': 0.81015015, 'grad_norm': 0.68780118, 'learning_rate': 4.995e-05, 'memory(GiB)': 17.21, 'train_speed(iter/s)': 0.675782, 'epoch': 3.57, 'global_step/max_steps': '25/350', 'percentage': '7.14%', 'elapsed_time': '36s', 'remaining_time': '7m 48s'}\n",
      "{'loss': 0.77151394, 'acc': 0.78393736, 'grad_norm': 0.7558338, 'learning_rate': 4.984e-05, 'memory(GiB)': 17.21, 'train_speed(iter/s)': 0.693784, 'epoch': 4.29, 'global_step/max_steps': '30/350', 'percentage': '8.57%', 'elapsed_time': '42s', 'remaining_time': '7m 31s'}\n",
      "{'loss': 0.66649079, 'acc': 0.8269206, 'grad_norm': 1.11850452, 'learning_rate': 4.968e-05, 'memory(GiB)': 17.21, 'train_speed(iter/s)': 0.699045, 'epoch': 5.0, 'global_step/max_steps': '35/350', 'percentage': '10.00%', 'elapsed_time': '49s', 'remaining_time': '7m 21s'}\n",
      "{'loss': 0.67703266, 'acc': 0.8153389, 'grad_norm': 0.69384021, 'learning_rate': 4.946e-05, 'memory(GiB)': 17.21, 'train_speed(iter/s)': 0.685454, 'epoch': 5.71, 'global_step/max_steps': '40/350', 'percentage': '11.43%', 'elapsed_time': '57s', 'remaining_time': '7m 24s'}\n",
      "Train:  11%|████                               | 40/350 [00:57<07:59,  1.55s/it]\n",
      "{'eval_loss': 0.42604062, 'eval_acc': 0.84210526, 'eval_runtime': 0.0891, 'eval_samples_per_second': 11.218, 'eval_steps_per_second': 11.218, 'epoch': 5.71, 'global_step/max_steps': '40/350', 'percentage': '11.43%', 'elapsed_time': '57s', 'remaining_time': '7m 25s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 617.45it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v4-20250715-134324/checkpoint-40\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Train:  13%|████▍                              | 44/350 [01:02<07:21,  1.44s/it]^C\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env LOG_LEVEL=INFO\n",
    "!swift sft \\\n",
    "--learning_rate '0.00005' \\\n",
    "--lora_rank 4 \\\n",
    "--num_train_epochs 50 \\\n",
    "--dataset './resources/2_7/train_100.jsonl' \\\n",
    "--batch_size '16' \\\n",
    "--max_length 512 \\\n",
    "--eval_step 20 \\\n",
    "--model_type 'qwen2_5-1_5b-instruct' \\\n",
    "--model_id_or_path './model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8933c14b",
   "metadata": {},
   "source": [
    "| Training loss image | Evaluation loss image |\n",
    "| --- | --- |\n",
    "|<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01xsw3a31YarKvsEKCR_!!6000000003076-2-tps-671-451.png\" style=\"width: 500px;display: block; margin-left: auto; margin-right: auto\"/> | <img src=\"https://img.alicdn.com/imgextra/i3/O1CN01b2v3fK1jOSNo73Q3y_!!6000000004538-2-tps-680-451.png\" style=\"width: 500px;display: block; margin-left: auto; margin-right: auto\"/> |\n",
    "\n",
    "| **Observation Metrics (Training Loss, Validation Loss):** | Training loss decreases, validation loss first decreases then increases |\n",
    "| --- | --- |\n",
    "| **Training Status:** | **overfitting** |\n",
    "| **Cause Analysis:** | overfitting is also a very common phenomenon during training. It indicates that the model is \"memorizing questions\" and not learning the knowledge in the dataset. We can reduce the number of epochs or increase the amount of data to make the model \"forget the questions.\" |\n",
    "| **Adjustment Logic:** | 1. Reduce the number of epochs to 5. <br/> 2. Expand the number of problem solutions generated by DeepSeek-R1 to 1000 entries. Dataset location: current directory/resources/2_4/train_1k.jsonl <br/> 3. After increasing the amount of data, increase the rank of LoRA to 16 based on previous learning. |\n",
    "\n",
    "In general, with the scale of today's large language models (LLMs), fine-tuning requires at least **1000+** high-quality training dataset entries. When below this threshold, the model tends to \"memorize questions\" after a few rounds of training instead of learning the inherent knowledge within the data.\n",
    "\n",
    "#### 3.3.4 Fourth Experiment (Expected to take 5 minutes)\n",
    "\n",
    "| Parameter | Old Value | New Value |\n",
    "| --- | --- | --- |\n",
    "| Change Dataset | 100 entries | 1000+ entries |\n",
    "| Number of Training Epochs (num_train_epochs) | 50 | 3 |\n",
    "| LoRA Rank (lora_rank) | 4 | 8 (For reasons why this was increased, refer to the LoRA introduction). | \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a77d9f59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T08:24:49.164123Z",
     "iopub.status.busy": "2025-07-15T08:24:49.163788Z",
     "iopub.status.idle": "2025-07-15T08:30:20.768915Z",
     "shell.execute_reply": "2025-07-15T08:30:20.768351Z",
     "shell.execute_reply.started": "2025-07-15T08:24:49.164103Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: LOG_LEVEL=INFO\n",
      "run sh: `/usr/local/bin/python /usr/local/lib/python3.10/site-packages/swift/cli/sft.py --learning_rate 0.00005 --lora_rank 8 --num_train_epochs 3 --dataset ./resources/2_7/train_1k.jsonl --batch_size 16 --max_length 512 --eval_step 20 --model_type qwen2_5-1_5b-instruct --model_id_or_path ./model`\n",
      "[INFO:swift] Successfully registered `/usr/local/lib/python3.10/site-packages/swift/llm/data/dataset_info.json`\n",
      "[INFO:swift] Start time of running main: 2025-07-15 16:24:56.914902\n",
      "[INFO:swift] Setting template_type: qwen2_5\n",
      "[INFO:swift] Setting args.lazy_tokenize: False\n",
      "[INFO:swift] Setting args.dataloader_num_workers: 1\n",
      "[INFO:swift] output_dir: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456\n",
      "[INFO:swift] args: SftArguments(model_type='qwen2_5-1_5b-instruct', model_id_or_path='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model', model_revision='master', full_determinism=False, sft_type='lora', freeze_parameters=[], freeze_vit=False, freeze_parameters_ratio=0.0, additional_trainable_parameters=[], tuner_backend='peft', template_type='qwen2_5', output_dir='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456', add_output_dir_suffix=True, ddp_backend=None, ddp_find_unused_parameters=None, ddp_broadcast_buffers=None, ddp_timeout=1800, seed=42, resume_from_checkpoint=None, resume_only_model=False, ignore_data_skip=False, dtype='bf16', packing=False, train_backend='transformers', tp=1, pp=1, min_lr=None, sequence_parallel=False, model_kwargs={}, loss_name=None, dataset=['./resources/2_7/train_1k.jsonl'], val_dataset=[], dataset_seed=42, dataset_test_ratio=0.01, use_loss_scale=False, loss_scale_config_path='/usr/local/lib/python3.10/site-packages/swift/llm/agent/default_loss_scale_config.json', system=None, tools_prompt='react_en', max_length=512, truncation_strategy='delete', check_dataset_strategy='none', streaming=False, streaming_val_size=0, streaming_buffer_size=16384, model_name=[None, None], model_author=[None, None], quant_method=None, quantization_bit=0, hqq_axis=0, hqq_dynamic_config_path=None, bnb_4bit_comp_dtype='bf16', bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, rescale_image=-1, target_modules=['q_proj', 'k_proj', 'v_proj'], target_regex=None, modules_to_save=[], lora_rank=8, lora_alpha=32, lora_dropout=0.05, lora_bias_trainable='none', lora_dtype='AUTO', lora_lr_ratio=None, use_rslora=False, use_dora=False, init_lora_weights='true', fourier_n_frequency=2000, fourier_scaling=300.0, rope_scaling=None, boft_block_size=4, boft_block_num=0, boft_n_butterfly_factor=1, boft_dropout=0.0, vera_rank=256, vera_projection_prng_key=0, vera_dropout=0.0, vera_d_initial=0.1, adapter_act='gelu', adapter_length=128, use_galore=False, galore_target_modules=None, galore_rank=128, galore_update_proj_gap=50, galore_scale=1.0, galore_proj_type='std', galore_optim_per_parameter=False, galore_with_embedding=False, galore_quantization=False, galore_proj_quant=False, galore_proj_bits=4, galore_proj_group_size=256, galore_cos_threshold=0.4, galore_gamma_proj=2, galore_queue_size=5, adalora_target_r=8, adalora_init_r=12, adalora_tinit=0, adalora_tfinal=0, adalora_deltaT=1, adalora_beta1=0.85, adalora_beta2=0.85, adalora_orth_reg_weight=0.5, ia3_feedforward_modules=[], llamapro_num_new_blocks=4, llamapro_num_groups=None, neftune_noise_alpha=None, neftune_backend='transformers', lisa_activated_layers=0, lisa_step_interval=20, reft_layer_key=None, reft_layers=None, reft_rank=4, reft_intervention_type='LoreftIntervention', reft_args=None, use_liger=False, gradient_checkpointing=True, deepspeed=None, batch_size=16, eval_batch_size=16, auto_find_batch_size=False, num_train_epochs=3, max_steps=-1, optim='adamw_torch', adam_beta1=0.9, adam_beta2=0.95, adam_epsilon=1e-08, learning_rate=5e-05, weight_decay=0.1, gradient_accumulation_steps=1, max_grad_norm=1, predict_with_generate=False, lr_scheduler_type='cosine', lr_scheduler_kwargs={}, warmup_ratio=0.05, warmup_steps=0, eval_steps=20, save_steps=20, save_only_model=False, save_total_limit=2, logging_steps=5, acc_steps=1, dataloader_num_workers=1, dataloader_pin_memory=True, dataloader_drop_last=False, push_to_hub=False, hub_model_id=None, hub_token=None, hub_private_repo=False, hub_strategy='every_save', test_oom_error=False, disable_tqdm=False, lazy_tokenize=False, preprocess_num_proc=1, use_flash_attn=None, ignore_args_error=False, check_model_is_latest=True, logging_dir='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/runs', report_to=['tensorboard'], acc_strategy='token', save_on_each_node=False, evaluation_strategy='steps', save_strategy='steps', save_safetensors=True, gpu_memory_fraction=None, include_num_input_tokens_seen=False, local_repo_path=None, custom_register_path=None, custom_dataset_info=None, device_map_config=None, device_max_memory=[], max_new_tokens=2048, do_sample=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, fsdp='', fsdp_config=None, sequence_parallel_size=1, model_layer_cls_name=None, metric_warmup_step=0, fsdp_num=1, per_device_train_batch_size=None, per_device_eval_batch_size=None, eval_strategy=None, self_cognition_sample=0, train_dataset_mix_ratio=0.0, train_dataset_mix_ds=['ms-bench'], train_dataset_sample=-1, val_dataset_sample=None, safe_serialization=None, only_save_model=None, neftune_alpha=None, deepspeed_config_path=None, model_cache_dir=None, lora_dropout_p=None, lora_target_modules=[], lora_target_regex=None, lora_modules_to_save=[], boft_target_modules=[], boft_modules_to_save=[], vera_target_modules=[], vera_modules_to_save=[], ia3_target_modules=[], ia3_modules_to_save=[], custom_train_dataset_path=[], custom_val_dataset_path=[], device_map_config_path=None, push_hub_strategy=None)\n",
      "[INFO:swift] Global seed set to 42\n",
      "device_count: 1\n",
      "rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n",
      "[INFO:swift] Loading the model using model_dir: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model\n",
      "[INFO:swift] model_kwargs: {'device_map': 'cuda:0'}\n",
      "[INFO:swift] model.max_model_len: 32768\n",
      "[INFO:swift] model.hf_device_map: {'': device(type='cuda', index=0)}\n",
      "[INFO:swift] model_config: Qwen2Config {\n",
      "  \"_name_or_path\": \"/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO:swift] model.generation_config: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO:swift] target_modules: ['q_proj', 'k_proj', 'v_proj']\n",
      "[INFO:swift] modules_to_save: []\n",
      "[INFO:swift] lora_config: get_wrapped_class.<locals>.PeftWrapper(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'k_proj', 'v_proj', 'q_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=[], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_dtype=None, lorap_lr_ratio=None, lorap_emb_lr=1e-06)\n",
      "[INFO:swift] [base_model.model.model.embed_tokens.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.o_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.gate_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.up_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.down_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.input_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.post_attention_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] ...\n",
      "[INFO:swift] PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(151936, 1536)\n",
      "        (layers): ModuleList(\n",
      "          (0-27): 28 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2SdpaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "              (rotary_emb): Qwen2RotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "              (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (rotary_emb): Qwen2RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[INFO:swift] PeftModelForCausalLM: 1545.2052M Params (1.4909M Trainable [0.0965%]), 0.0019M Buffers.\n",
      "[INFO:swift] Setting model.config.use_cache: False\n",
      "[INFO:swift] system: You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "[INFO:swift] args.lazy_tokenize: False\n",
      "Map: 100%|████████████████████████| 1113/1113 [00:00<00:00, 15681.99 examples/s]\n",
      "[INFO:swift] train_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 1102\n",
      "})\n",
      "[INFO:swift] val_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 11\n",
      "})\n",
      "[INFO:swift] [INPUT_IDS] [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 2, 5097, 4956, 785, 2550, 1969, 387, 12966, 448, 279, 1590, 4226, 1283, 32711, 624, 2, 8815, 3491, 4956, 32, 5636, 8086, 2083, 374, 4752, 264, 5636, 13, 576, 949, 807, 614, 5798, 374, 220, 18, 3039, 279, 949, 807, 614, 537, 5798, 13, 2619, 525, 220, 16, 17, 15, 20044, 2115, 13, 2585, 1293, 374, 279, 5636, 30, 151645, 198, 151644, 77091, 198, 785, 949, 807, 614, 5798, 374, 220, 18, 3039, 279, 949, 807, 614, 537, 5798, 13, 576, 949, 807, 614, 537, 5798, 374, 220, 16, 17, 15, 20044, 13, 15277, 11, 279, 949, 807, 614, 5798, 374, 220, 18, 17568, 16, 17, 15, 28, 18, 21, 15, 20044, 13, 576, 2790, 3084, 315, 279, 5636, 374, 279, 2629, 315, 279, 949, 807, 614, 5798, 323, 279, 949, 807, 614, 537, 5798, 11, 429, 374, 11, 220, 18, 21, 15, 10, 16, 17, 15, 28, 19, 23, 15, 20044, 382, 2979, 596, 25, 19, 23, 15, 3417, 20044, 151645]\n",
      "[INFO:swift] [INPUT] <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "#Output#\n",
      "The output must be consistent with the final answer after reasoning.\n",
      "#Math problem#\n",
      "A road construction team is building a road. The part they have built is 3 times the part they have not built. There are 120 meters left. How long is the road?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The part they have built is 3 times the part they have not built. The part they have not built is 120 meters. Therefore, the part they have built is 3×120=360 meters. The total length of the road is the sum of the part they have built and the part they have not built, that is, 360+120=480 meters.\n",
      "\n",
      "{{ans:480}} meters<|im_end|>\n",
      "[INFO:swift] [LABELS_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 785, 949, 807, 614, 5798, 374, 220, 18, 3039, 279, 949, 807, 614, 537, 5798, 13, 576, 949, 807, 614, 537, 5798, 374, 220, 16, 17, 15, 20044, 13, 15277, 11, 279, 949, 807, 614, 5798, 374, 220, 18, 17568, 16, 17, 15, 28, 18, 21, 15, 20044, 13, 576, 2790, 3084, 315, 279, 5636, 374, 279, 2629, 315, 279, 949, 807, 614, 5798, 323, 279, 949, 807, 614, 537, 5798, 11, 429, 374, 11, 220, 18, 21, 15, 10, 16, 17, 15, 28, 19, 23, 15, 20044, 382, 2979, 596, 25, 19, 23, 15, 3417, 20044, 151645]\n",
      "[INFO:swift] [LABELS] [-100 * 88]The part they have built is 3 times the part they have not built. The part they have not built is 120 meters. Therefore, the part they have built is 3×120=360 meters. The total length of the road is the sum of the part they have built and the part they have not built, that is, 360+120=480 meters.\n",
      "\n",
      "{{ans:480}} meters<|im_end|>\n",
      "Map:   0%|                                             | 0/1102 [00:00<?, ?it/s][WARNING:swift] Current length of row(519) is larger than the max_length(512), deleted.\n",
      "Map:  31%|██████████▎                      | 344/1102 [00:00<00:00, 1720.90it/s][WARNING:swift] Current length of row(627) is larger than the max_length(512), deleted.\n",
      "Map:  78%|█████████████████████████▊       | 863/1102 [00:00<00:00, 1715.63it/s][WARNING:swift] Current length of row(529) is larger than the max_length(512), deleted.\n",
      "Map: 100%|████████████████████████████████| 1102/1102 [00:00<00:00, 1717.28it/s]\n",
      "Map: 100%|████████████████████████████████████| 11/11 [00:00<00:00, 1581.94it/s]\n",
      "[INFO:swift] Dataset Token Length: 205.472247±65.802901, min=71.000000, max=493.000000, size=1099\n",
      "[INFO:swift] Dataset Token Length: 230.000000±82.677907, min=152.000000, max=453.000000, size=11\n",
      "[INFO:swift] training_args: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "acc_strategy=token,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': False, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.95,\n",
      "adam_epsilon=1e-08,\n",
      "additional_saved_files=[],\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=42,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=1,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=20,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      ",\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/runs,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "loss_name=None,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "metric_warmup_step=0,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=20,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_dataset_sample=-1,\n",
      "train_sampler_random=True,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      ")\n",
      "[INFO:swift] The SftArguments will be saved in: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/sft_args.json\n",
      "[INFO:swift] The Seq2SeqTrainingArguments will be saved in: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/training_args.json\n",
      "[INFO:swift] The logging file will be saved in: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/logging.jsonl\n",
      "[2025-07-15 16:25:00,005] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Train:   0%|                                            | 0/207 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.94491345, 'acc': 0.7805627, 'grad_norm': 0.86230594, 'learning_rate': 4.55e-06, 'memory(GiB)': 13.97, 'train_speed(iter/s)': 0.379512, 'epoch': 0.01, 'global_step/max_steps': '1/207', 'percentage': '0.48%', 'elapsed_time': '1s', 'remaining_time': '5m 42s'}\n",
      "{'loss': 1.00211227, 'acc': 0.76114619, 'grad_norm': 0.95129859, 'learning_rate': 2.273e-05, 'memory(GiB)': 18.35, 'train_speed(iter/s)': 0.571229, 'epoch': 0.07, 'global_step/max_steps': '5/207', 'percentage': '2.42%', 'elapsed_time': '7s', 'remaining_time': '5m 14s'}\n",
      "{'loss': 1.01538553, 'acc': 0.75882297, 'grad_norm': 0.85796142, 'learning_rate': 4.545e-05, 'memory(GiB)': 18.35, 'train_speed(iter/s)': 0.625239, 'epoch': 0.14, 'global_step/max_steps': '10/207', 'percentage': '4.83%', 'elapsed_time': '15s', 'remaining_time': '4m 55s'}\n",
      "{'loss': 0.97670822, 'acc': 0.77076283, 'grad_norm': 0.61990637, 'learning_rate': 4.995e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.614269, 'epoch': 0.22, 'global_step/max_steps': '15/207', 'percentage': '7.25%', 'elapsed_time': '23s', 'remaining_time': '5m 0s'}\n",
      "{'loss': 0.90754662, 'acc': 0.78192263, 'grad_norm': 0.68963766, 'learning_rate': 4.974e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.625946, 'epoch': 0.29, 'global_step/max_steps': '20/207', 'percentage': '9.66%', 'elapsed_time': '30s', 'remaining_time': '4m 49s'}\n",
      "Train:  10%|███▍                               | 20/207 [00:30<04:29,  1.44s/it]\n",
      "{'eval_loss': 0.79668754, 'eval_acc': 0.80689655, 'eval_runtime': 0.4411, 'eval_samples_per_second': 24.94, 'eval_steps_per_second': 2.267, 'epoch': 0.29, 'global_step/max_steps': '20/207', 'percentage': '9.66%', 'elapsed_time': '31s', 'remaining_time': '4m 53s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 141.81it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/checkpoint-20\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.93364029, 'acc': 0.77008896, 'grad_norm': 0.55583781, 'learning_rate': 4.937e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.623246, 'epoch': 0.36, 'global_step/max_steps': '25/207', 'percentage': '12.08%', 'elapsed_time': '39s', 'remaining_time': '4m 44s'}\n",
      "{'loss': 0.86785679, 'acc': 0.78488336, 'grad_norm': 0.48232955, 'learning_rate': 4.885e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.636642, 'epoch': 0.43, 'global_step/max_steps': '30/207', 'percentage': '14.49%', 'elapsed_time': '46s', 'remaining_time': '4m 32s'}\n",
      "{'loss': 0.90643606, 'acc': 0.77088208, 'grad_norm': 0.64429784, 'learning_rate': 4.817e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.643412, 'epoch': 0.51, 'global_step/max_steps': '35/207', 'percentage': '16.91%', 'elapsed_time': '53s', 'remaining_time': '4m 22s'}\n",
      "{'loss': 0.82489862, 'acc': 0.79285378, 'grad_norm': 0.48644868, 'learning_rate': 4.735e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.654997, 'epoch': 0.58, 'global_step/max_steps': '40/207', 'percentage': '19.32%', 'elapsed_time': '1m 0s', 'remaining_time': '4m 10s'}\n",
      "Train:  19%|██████▊                            | 40/207 [01:00<03:40,  1.32s/it]\n",
      "{'eval_loss': 0.72858673, 'eval_acc': 0.82206897, 'eval_runtime': 0.4456, 'eval_samples_per_second': 24.684, 'eval_steps_per_second': 2.244, 'epoch': 0.58, 'global_step/max_steps': '40/207', 'percentage': '19.32%', 'elapsed_time': '1m 0s', 'remaining_time': '4m 12s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 146.03it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/checkpoint-40\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.82646036, 'acc': 0.78392358, 'grad_norm': 0.48713711, 'learning_rate': 4.638e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.661447, 'epoch': 0.65, 'global_step/max_steps': '45/207', 'percentage': '21.74%', 'elapsed_time': '1m 7s', 'remaining_time': '4m 1s'}\n",
      "{'loss': 0.81009846, 'acc': 0.79310923, 'grad_norm': 0.48110187, 'learning_rate': 4.527e-05, 'memory(GiB)': 18.52, 'train_speed(iter/s)': 0.658079, 'epoch': 0.72, 'global_step/max_steps': '50/207', 'percentage': '24.15%', 'elapsed_time': '1m 15s', 'remaining_time': '3m 55s'}\n",
      "{'loss': 0.82845917, 'acc': 0.78421898, 'grad_norm': 0.50807834, 'learning_rate': 4.404e-05, 'memory(GiB)': 18.52, 'train_speed(iter/s)': 0.666798, 'epoch': 0.8, 'global_step/max_steps': '55/207', 'percentage': '26.57%', 'elapsed_time': '1m 21s', 'remaining_time': '3m 45s'}\n",
      "{'loss': 0.81257038, 'acc': 0.78365822, 'grad_norm': 0.53394008, 'learning_rate': 4.268e-05, 'memory(GiB)': 18.52, 'train_speed(iter/s)': 0.667218, 'epoch': 0.87, 'global_step/max_steps': '60/207', 'percentage': '28.99%', 'elapsed_time': '1m 28s', 'remaining_time': '3m 37s'}\n",
      "Train:  29%|██████████▏                        | 60/207 [01:28<03:33,  1.45s/it]\n",
      "{'eval_loss': 0.67445129, 'eval_acc': 0.82689655, 'eval_runtime': 0.4496, 'eval_samples_per_second': 24.467, 'eval_steps_per_second': 2.224, 'epoch': 0.87, 'global_step/max_steps': '60/207', 'percentage': '28.99%', 'elapsed_time': '1m 29s', 'remaining_time': '3m 39s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 144.61it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/checkpoint-60\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.82997561, 'acc': 0.78921413, 'grad_norm': 0.46454033, 'learning_rate': 4.121e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.652504, 'epoch': 0.94, 'global_step/max_steps': '65/207', 'percentage': '31.40%', 'elapsed_time': '1m 38s', 'remaining_time': '3m 35s'}\n",
      "{'loss': 0.83646584, 'acc': 0.78187466, 'grad_norm': 0.54692262, 'learning_rate': 3.963e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654486, 'epoch': 1.01, 'global_step/max_steps': '70/207', 'percentage': '33.82%', 'elapsed_time': '1m 45s', 'remaining_time': '3m 27s'}\n",
      "{'loss': 0.72185478, 'acc': 0.80801868, 'grad_norm': 0.48976514, 'learning_rate': 3.796e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656791, 'epoch': 1.09, 'global_step/max_steps': '75/207', 'percentage': '36.23%', 'elapsed_time': '1m 53s', 'remaining_time': '3m 19s'}\n",
      "{'loss': 0.71702008, 'acc': 0.81226988, 'grad_norm': 0.5168612, 'learning_rate': 3.621e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.661185, 'epoch': 1.16, 'global_step/max_steps': '80/207', 'percentage': '38.65%', 'elapsed_time': '2m 0s', 'remaining_time': '3m 10s'}\n",
      "Train:  39%|█████████████▌                     | 80/207 [02:00<02:58,  1.41s/it]\n",
      "{'eval_loss': 0.63354737, 'eval_acc': 0.82965517, 'eval_runtime': 0.452, 'eval_samples_per_second': 24.335, 'eval_steps_per_second': 2.212, 'epoch': 1.16, 'global_step/max_steps': '80/207', 'percentage': '38.65%', 'elapsed_time': '2m 0s', 'remaining_time': '3m 11s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 143.49it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/checkpoint-80\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.74307652, 'acc': 0.79941597, 'grad_norm': 0.52821022, 'learning_rate': 3.438e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.658327, 'epoch': 1.23, 'global_step/max_steps': '85/207', 'percentage': '41.06%', 'elapsed_time': '2m 8s', 'remaining_time': '3m 3s'}\n",
      "{'loss': 0.722855, 'acc': 0.80380878, 'grad_norm': 0.53239554, 'learning_rate': 3.25e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.658931, 'epoch': 1.3, 'global_step/max_steps': '90/207', 'percentage': '43.48%', 'elapsed_time': '2m 15s', 'remaining_time': '2m 56s'}\n",
      "{'loss': 0.77421484, 'acc': 0.79261169, 'grad_norm': 0.56103373, 'learning_rate': 3.056e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655208, 'epoch': 1.38, 'global_step/max_steps': '95/207', 'percentage': '45.89%', 'elapsed_time': '2m 24s', 'remaining_time': '2m 49s'}\n",
      "{'loss': 0.76570792, 'acc': 0.79564762, 'grad_norm': 0.56992978, 'learning_rate': 2.859e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.653086, 'epoch': 1.45, 'global_step/max_steps': '100/207', 'percentage': '48.31%', 'elapsed_time': '2m 32s', 'remaining_time': '2m 42s'}\n",
      "Train:  48%|████████████████▍                 | 100/207 [02:32<03:10,  1.78s/it]\n",
      "{'eval_loss': 0.60619062, 'eval_acc': 0.83310345, 'eval_runtime': 0.4547, 'eval_samples_per_second': 24.193, 'eval_steps_per_second': 2.199, 'epoch': 1.45, 'global_step/max_steps': '100/207', 'percentage': '48.31%', 'elapsed_time': '2m 32s', 'remaining_time': '2m 43s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.82it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/checkpoint-100\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.70075212, 'acc': 0.80333529, 'grad_norm': 0.57828873, 'learning_rate': 2.66e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.652374, 'epoch': 1.52, 'global_step/max_steps': '105/207', 'percentage': '50.72%', 'elapsed_time': '2m 39s', 'remaining_time': '2m 35s'}\n",
      "{'loss': 0.6739696, 'acc': 0.81750011, 'grad_norm': 0.58180612, 'learning_rate': 2.46e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.649779, 'epoch': 1.59, 'global_step/max_steps': '110/207', 'percentage': '53.14%', 'elapsed_time': '2m 48s', 'remaining_time': '2m 28s'}\n",
      "{'loss': 0.68936968, 'acc': 0.80851088, 'grad_norm': 0.51411748, 'learning_rate': 2.26e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.651362, 'epoch': 1.67, 'global_step/max_steps': '115/207', 'percentage': '55.56%', 'elapsed_time': '2m 55s', 'remaining_time': '2m 20s'}\n",
      "{'loss': 0.68918691, 'acc': 0.81339102, 'grad_norm': 0.59504557, 'learning_rate': 2.061e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655588, 'epoch': 1.74, 'global_step/max_steps': '120/207', 'percentage': '57.97%', 'elapsed_time': '3m 2s', 'remaining_time': '2m 12s'}\n",
      "Train:  58%|███████████████████▋              | 120/207 [03:02<02:00,  1.39s/it]\n",
      "{'eval_loss': 0.59331363, 'eval_acc': 0.83586207, 'eval_runtime': 0.4476, 'eval_samples_per_second': 24.577, 'eval_steps_per_second': 2.234, 'epoch': 1.74, 'global_step/max_steps': '120/207', 'percentage': '57.97%', 'elapsed_time': '3m 2s', 'remaining_time': '2m 12s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.12it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/checkpoint-120\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.71811342, 'acc': 0.80748529, 'grad_norm': 0.54783893, 'learning_rate': 1.866e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655847, 'epoch': 1.81, 'global_step/max_steps': '125/207', 'percentage': '60.39%', 'elapsed_time': '3m 9s', 'remaining_time': '2m 4s'}\n",
      "{'loss': 0.73707819, 'acc': 0.79635057, 'grad_norm': 0.52663636, 'learning_rate': 1.674e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654033, 'epoch': 1.88, 'global_step/max_steps': '130/207', 'percentage': '62.80%', 'elapsed_time': '3m 17s', 'remaining_time': '1m 57s'}\n",
      "{'loss': 0.65600762, 'acc': 0.81616964, 'grad_norm': 0.57028759, 'learning_rate': 1.488e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657533, 'epoch': 1.96, 'global_step/max_steps': '135/207', 'percentage': '65.22%', 'elapsed_time': '3m 24s', 'remaining_time': '1m 48s'}\n",
      "{'loss': 0.71841521, 'acc': 0.80170536, 'grad_norm': 0.52375323, 'learning_rate': 1.308e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.660681, 'epoch': 2.03, 'global_step/max_steps': '140/207', 'percentage': '67.63%', 'elapsed_time': '3m 30s', 'remaining_time': '1m 40s'}\n",
      "Train:  68%|██████████████████████▉           | 140/207 [03:30<01:33,  1.39s/it]\n",
      "{'eval_loss': 0.58691996, 'eval_acc': 0.83586207, 'eval_runtime': 0.453, 'eval_samples_per_second': 24.282, 'eval_steps_per_second': 2.207, 'epoch': 2.03, 'global_step/max_steps': '140/207', 'percentage': '67.63%', 'elapsed_time': '3m 31s', 'remaining_time': '1m 41s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 144.35it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/checkpoint-140\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.59971714, 'acc': 0.82738266, 'grad_norm': 0.53899395, 'learning_rate': 1.136e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.659489, 'epoch': 2.1, 'global_step/max_steps': '145/207', 'percentage': '70.05%', 'elapsed_time': '3m 38s', 'remaining_time': '1m 33s'}\n",
      "{'loss': 0.69226246, 'acc': 0.81069565, 'grad_norm': 0.52391773, 'learning_rate': 9.73e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657468, 'epoch': 2.17, 'global_step/max_steps': '150/207', 'percentage': '72.46%', 'elapsed_time': '3m 47s', 'remaining_time': '1m 26s'}\n",
      "{'loss': 0.6650672, 'acc': 0.81640682, 'grad_norm': 0.63706392, 'learning_rate': 8.19e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657315, 'epoch': 2.25, 'global_step/max_steps': '155/207', 'percentage': '74.88%', 'elapsed_time': '3m 54s', 'remaining_time': '1m 18s'}\n",
      "{'loss': 0.70260725, 'acc': 0.80763073, 'grad_norm': 0.57316685, 'learning_rate': 6.76e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655581, 'epoch': 2.32, 'global_step/max_steps': '160/207', 'percentage': '77.29%', 'elapsed_time': '4m 3s', 'remaining_time': '1m 11s'}\n",
      "Train:  77%|██████████████████████████▎       | 160/207 [04:03<01:11,  1.53s/it]\n",
      "{'eval_loss': 0.583386, 'eval_acc': 0.83724138, 'eval_runtime': 0.4481, 'eval_samples_per_second': 24.549, 'eval_steps_per_second': 2.232, 'epoch': 2.32, 'global_step/max_steps': '160/207', 'percentage': '77.29%', 'elapsed_time': '4m 3s', 'remaining_time': '1m 11s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.25it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/checkpoint-160\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.64955006, 'acc': 0.81449823, 'grad_norm': 0.64128309, 'learning_rate': 5.45e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654171, 'epoch': 2.39, 'global_step/max_steps': '165/207', 'percentage': '79.71%', 'elapsed_time': '4m 11s', 'remaining_time': '1m 3s'}\n",
      "{'loss': 0.74411821, 'acc': 0.79819484, 'grad_norm': 0.82861388, 'learning_rate': 4.27e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655666, 'epoch': 2.46, 'global_step/max_steps': '170/207', 'percentage': '82.13%', 'elapsed_time': '4m 18s', 'remaining_time': '56s'}\n",
      "{'loss': 0.66727304, 'acc': 0.81624107, 'grad_norm': 0.68424982, 'learning_rate': 3.22e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655586, 'epoch': 2.54, 'global_step/max_steps': '175/207', 'percentage': '84.54%', 'elapsed_time': '4m 25s', 'remaining_time': '48s'}\n",
      "{'loss': 0.65330763, 'acc': 0.81758499, 'grad_norm': 0.64320868, 'learning_rate': 2.3e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656312, 'epoch': 2.61, 'global_step/max_steps': '180/207', 'percentage': '86.96%', 'elapsed_time': '4m 33s', 'remaining_time': '40s'}\n",
      "Train:  87%|█████████████████████████████▌    | 180/207 [04:33<00:39,  1.48s/it]\n",
      "{'eval_loss': 0.58189297, 'eval_acc': 0.83655172, 'eval_runtime': 0.4523, 'eval_samples_per_second': 24.318, 'eval_steps_per_second': 2.211, 'epoch': 2.61, 'global_step/max_steps': '180/207', 'percentage': '86.96%', 'elapsed_time': '4m 33s', 'remaining_time': '41s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 146.08it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/checkpoint-180\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.71067743, 'acc': 0.80790815, 'grad_norm': 0.65382707, 'learning_rate': 1.54e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.652476, 'epoch': 2.68, 'global_step/max_steps': '185/207', 'percentage': '89.37%', 'elapsed_time': '4m 42s', 'remaining_time': '33s'}\n",
      "{'loss': 0.67813945, 'acc': 0.8152812, 'grad_norm': 0.5832352, 'learning_rate': 9.2e-07, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.653716, 'epoch': 2.75, 'global_step/max_steps': '190/207', 'percentage': '91.79%', 'elapsed_time': '4m 49s', 'remaining_time': '25s'}\n",
      "{'loss': 0.71567721, 'acc': 0.80633879, 'grad_norm': 0.5508393, 'learning_rate': 4.6e-07, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.652748, 'epoch': 2.83, 'global_step/max_steps': '195/207', 'percentage': '94.20%', 'elapsed_time': '4m 57s', 'remaining_time': '18s'}\n",
      "{'loss': 0.67012296, 'acc': 0.81416082, 'grad_norm': 0.57177907, 'learning_rate': 1.6e-07, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.653057, 'epoch': 2.9, 'global_step/max_steps': '200/207', 'percentage': '96.62%', 'elapsed_time': '5m 5s', 'remaining_time': '10s'}\n",
      "Train:  97%|████████████████████████████████▊ | 200/207 [05:05<00:10,  1.50s/it]\n",
      "{'eval_loss': 0.58137333, 'eval_acc': 0.83862069, 'eval_runtime': 0.4523, 'eval_samples_per_second': 24.32, 'eval_steps_per_second': 2.211, 'epoch': 2.9, 'global_step/max_steps': '200/207', 'percentage': '96.62%', 'elapsed_time': '5m 5s', 'remaining_time': '10s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 144.83it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/checkpoint-200\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.70630107, 'acc': 0.80280466, 'grad_norm': 0.52060926, 'learning_rate': 1e-08, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.651874, 'epoch': 2.97, 'global_step/max_steps': '205/207', 'percentage': '99.03%', 'elapsed_time': '5m 13s', 'remaining_time': '3s'}\n",
      "Train: 100%|██████████████████████████████████| 207/207 [05:15<00:00,  1.29s/it]\n",
      "{'eval_loss': 0.58209938, 'eval_acc': 0.83931034, 'eval_runtime': 0.4417, 'eval_samples_per_second': 24.901, 'eval_steps_per_second': 2.264, 'epoch': 3.0, 'global_step/max_steps': '207/207', 'percentage': '100.00%', 'elapsed_time': '5m 16s', 'remaining_time': '0s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.96it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/checkpoint-207\n",
      "{'train_runtime': 316.203, 'train_samples_per_second': 10.427, 'train_steps_per_second': 0.655, 'train_loss': 0.76153521, 'epoch': 3.0, 'global_step/max_steps': '207/207', 'percentage': '100.00%', 'elapsed_time': '5m 16s', 'remaining_time': '0s'}\n",
      "Train: 100%|██████████████████████████████████| 207/207 [05:16<00:00,  1.53s/it]\n",
      "[INFO:swift] last_model_checkpoint: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/checkpoint-207\n",
      "[INFO:swift] best_model_checkpoint: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/checkpoint-200\n",
      "[INFO:swift] images_dir: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v10-20250715-162456/images\n",
      "[INFO:swift] End time of running main: 2025-07-15 16:30:18.986233\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env LOG_LEVEL=INFO\n",
    "!swift sft \\\n",
    "--learning_rate '0.00005' \\\n",
    "--lora_rank 8 \\\n",
    "--num_train_epochs 3 \\\n",
    "--dataset './resources/2_7/train_1k.jsonl' \\\n",
    "--batch_size '16' \\\n",
    "--max_length 512 \\\n",
    "--eval_step 20 \\\n",
    "--model_type 'qwen2_5-1_5b-instruct' \\\n",
    "--model_id_or_path './model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62bd2ca",
   "metadata": {},
   "source": [
    "| Training loss image | Evaluation loss image |\n",
    "| --- | --- |\n",
    "|<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01p8rX0d1UAyUOGHeOJ_!!6000000002478-2-tps-671-451.png\" style=\"width: 500px;display: block; margin-left: auto; margin-right: auto\"/> | <img src=\"https://img.alicdn.com/imgextra/i1/O1CN01LjmbJ21P4Uo8ZJyav_!!6000000001787-2-tps-689-451.png\" style=\"width: 500px;display: block; margin-left: auto; margin-right: auto\"/> |\n",
    "\n",
    "\n",
    "| **Observation Metrics (Training Loss, Validation Loss):** | Training loss decreases, validation loss also decreases |\n",
    "| --- | --- |\n",
    "| **Training Status:** | **Underfitting** |\n",
    "| **Reason Analysis:** | Training is almost successful! |\n",
    "| **Adjustment Logic:** | Let the model train more: Increase the number of dataset learning iterations (epoch) to 15. |\n",
    "\n",
    "#### 3.3.5 Fifth Experiment (Requires 20 Minutes)\n",
    "\n",
    "| Parameter | Old Parameter Value | New Parameter Value |\n",
    "| --- | --- | --- |\n",
    "| Number of Training Epochs (num_train_epochs) | 3 | 15 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbae8bc9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-07-15T07:08:25.039405Z",
     "iopub.status.busy": "2025-07-15T07:08:25.039065Z",
     "iopub.status.idle": "2025-07-15T07:34:57.923017Z",
     "shell.execute_reply": "2025-07-15T07:34:57.922451Z",
     "shell.execute_reply.started": "2025-07-15T07:08:25.039384Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: LOG_LEVEL=INFO\n",
      "run sh: `/usr/local/bin/python /usr/local/lib/python3.10/site-packages/swift/cli/sft.py --learning_rate 0.00005 --lora_rank 8 --num_train_epochs 15 --dataset ./resources/2_7/train_1k.jsonl --batch_size 16 --max_length 512 --eval_step 20 --model_type qwen2_5-1_5b-instruct --model_id_or_path ./model`\n",
      "[INFO:swift] Successfully registered `/usr/local/lib/python3.10/site-packages/swift/llm/data/dataset_info.json`\n",
      "[INFO:swift] Start time of running main: 2025-07-15 15:08:32.802459\n",
      "[INFO:swift] Setting template_type: qwen2_5\n",
      "[INFO:swift] Setting args.lazy_tokenize: False\n",
      "[INFO:swift] Setting args.dataloader_num_workers: 1\n",
      "[INFO:swift] output_dir: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832\n",
      "[INFO:swift] args: SftArguments(model_type='qwen2_5-1_5b-instruct', model_id_or_path='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model', model_revision='master', full_determinism=False, sft_type='lora', freeze_parameters=[], freeze_vit=False, freeze_parameters_ratio=0.0, additional_trainable_parameters=[], tuner_backend='peft', template_type='qwen2_5', output_dir='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832', add_output_dir_suffix=True, ddp_backend=None, ddp_find_unused_parameters=None, ddp_broadcast_buffers=None, ddp_timeout=1800, seed=42, resume_from_checkpoint=None, resume_only_model=False, ignore_data_skip=False, dtype='bf16', packing=False, train_backend='transformers', tp=1, pp=1, min_lr=None, sequence_parallel=False, model_kwargs={}, loss_name=None, dataset=['./resources/2_7/train_1k.jsonl'], val_dataset=[], dataset_seed=42, dataset_test_ratio=0.01, use_loss_scale=False, loss_scale_config_path='/usr/local/lib/python3.10/site-packages/swift/llm/agent/default_loss_scale_config.json', system=None, tools_prompt='react_en', max_length=512, truncation_strategy='delete', check_dataset_strategy='none', streaming=False, streaming_val_size=0, streaming_buffer_size=16384, model_name=[None, None], model_author=[None, None], quant_method=None, quantization_bit=0, hqq_axis=0, hqq_dynamic_config_path=None, bnb_4bit_comp_dtype='bf16', bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, rescale_image=-1, target_modules=['q_proj', 'k_proj', 'v_proj'], target_regex=None, modules_to_save=[], lora_rank=8, lora_alpha=32, lora_dropout=0.05, lora_bias_trainable='none', lora_dtype='AUTO', lora_lr_ratio=None, use_rslora=False, use_dora=False, init_lora_weights='true', fourier_n_frequency=2000, fourier_scaling=300.0, rope_scaling=None, boft_block_size=4, boft_block_num=0, boft_n_butterfly_factor=1, boft_dropout=0.0, vera_rank=256, vera_projection_prng_key=0, vera_dropout=0.0, vera_d_initial=0.1, adapter_act='gelu', adapter_length=128, use_galore=False, galore_target_modules=None, galore_rank=128, galore_update_proj_gap=50, galore_scale=1.0, galore_proj_type='std', galore_optim_per_parameter=False, galore_with_embedding=False, galore_quantization=False, galore_proj_quant=False, galore_proj_bits=4, galore_proj_group_size=256, galore_cos_threshold=0.4, galore_gamma_proj=2, galore_queue_size=5, adalora_target_r=8, adalora_init_r=12, adalora_tinit=0, adalora_tfinal=0, adalora_deltaT=1, adalora_beta1=0.85, adalora_beta2=0.85, adalora_orth_reg_weight=0.5, ia3_feedforward_modules=[], llamapro_num_new_blocks=4, llamapro_num_groups=None, neftune_noise_alpha=None, neftune_backend='transformers', lisa_activated_layers=0, lisa_step_interval=20, reft_layer_key=None, reft_layers=None, reft_rank=4, reft_intervention_type='LoreftIntervention', reft_args=None, use_liger=False, gradient_checkpointing=True, deepspeed=None, batch_size=16, eval_batch_size=16, auto_find_batch_size=False, num_train_epochs=15, max_steps=-1, optim='adamw_torch', adam_beta1=0.9, adam_beta2=0.95, adam_epsilon=1e-08, learning_rate=5e-05, weight_decay=0.1, gradient_accumulation_steps=1, max_grad_norm=1, predict_with_generate=False, lr_scheduler_type='cosine', lr_scheduler_kwargs={}, warmup_ratio=0.05, warmup_steps=0, eval_steps=20, save_steps=20, save_only_model=False, save_total_limit=2, logging_steps=5, acc_steps=1, dataloader_num_workers=1, dataloader_pin_memory=True, dataloader_drop_last=False, push_to_hub=False, hub_model_id=None, hub_token=None, hub_private_repo=False, hub_strategy='every_save', test_oom_error=False, disable_tqdm=False, lazy_tokenize=False, preprocess_num_proc=1, use_flash_attn=None, ignore_args_error=False, check_model_is_latest=True, logging_dir='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/runs', report_to=['tensorboard'], acc_strategy='token', save_on_each_node=False, evaluation_strategy='steps', save_strategy='steps', save_safetensors=True, gpu_memory_fraction=None, include_num_input_tokens_seen=False, local_repo_path=None, custom_register_path=None, custom_dataset_info=None, device_map_config=None, device_max_memory=[], max_new_tokens=2048, do_sample=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, fsdp='', fsdp_config=None, sequence_parallel_size=1, model_layer_cls_name=None, metric_warmup_step=0, fsdp_num=1, per_device_train_batch_size=None, per_device_eval_batch_size=None, eval_strategy=None, self_cognition_sample=0, train_dataset_mix_ratio=0.0, train_dataset_mix_ds=['ms-bench'], train_dataset_sample=-1, val_dataset_sample=None, safe_serialization=None, only_save_model=None, neftune_alpha=None, deepspeed_config_path=None, model_cache_dir=None, lora_dropout_p=None, lora_target_modules=[], lora_target_regex=None, lora_modules_to_save=[], boft_target_modules=[], boft_modules_to_save=[], vera_target_modules=[], vera_modules_to_save=[], ia3_target_modules=[], ia3_modules_to_save=[], custom_train_dataset_path=[], custom_val_dataset_path=[], device_map_config_path=None, push_hub_strategy=None)\n",
      "[INFO:swift] Global seed set to 42\n",
      "device_count: 1\n",
      "rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n",
      "[INFO:swift] Loading the model using model_dir: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model\n",
      "[INFO:swift] model_kwargs: {'device_map': 'cuda:0'}\n",
      "[INFO:swift] model.max_model_len: 32768\n",
      "[INFO:swift] model.hf_device_map: {'': device(type='cuda', index=0)}\n",
      "[INFO:swift] model_config: Qwen2Config {\n",
      "  \"_name_or_path\": \"/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO:swift] model.generation_config: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO:swift] target_modules: ['q_proj', 'k_proj', 'v_proj']\n",
      "[INFO:swift] modules_to_save: []\n",
      "[INFO:swift] lora_config: get_wrapped_class.<locals>.PeftWrapper(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/model', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'q_proj', 'v_proj', 'k_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=[], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_dtype=None, lorap_lr_ratio=None, lorap_emb_lr=1e-06)\n",
      "[INFO:swift] [base_model.model.model.embed_tokens.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.o_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.gate_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.up_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.down_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.input_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.post_attention_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] ...\n",
      "[INFO:swift] PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(151936, 1536)\n",
      "        (layers): ModuleList(\n",
      "          (0-27): 28 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2SdpaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "              (rotary_emb): Qwen2RotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "              (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (rotary_emb): Qwen2RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[INFO:swift] PeftModelForCausalLM: 1545.2052M Params (1.4909M Trainable [0.0965%]), 0.0019M Buffers.\n",
      "[INFO:swift] Setting model.config.use_cache: False\n",
      "[INFO:swift] system: You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "[INFO:swift] args.lazy_tokenize: False\n",
      "Generating train split: 1113 examples [00:00, 292772.68 examples/s]\n",
      "Map: 100%|████████████████████████| 1113/1113 [00:00<00:00, 15416.82 examples/s]\n",
      "Filter: 100%|█████████████████████| 1113/1113 [00:00<00:00, 36571.36 examples/s]\n",
      "[INFO:swift] train_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 1102\n",
      "})\n",
      "[INFO:swift] val_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 11\n",
      "})\n",
      "[INFO:swift] [INPUT_IDS] [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 2, 5097, 4956, 785, 2550, 1969, 387, 12966, 448, 279, 1590, 4226, 1283, 32711, 624, 2, 8815, 3491, 4956, 32, 5636, 8086, 2083, 374, 4752, 264, 5636, 13, 576, 949, 807, 614, 5798, 374, 220, 18, 3039, 279, 949, 807, 614, 537, 5798, 13, 2619, 525, 220, 16, 17, 15, 20044, 2115, 13, 2585, 1293, 374, 279, 5636, 30, 151645, 198, 151644, 77091, 198, 785, 949, 807, 614, 5798, 374, 220, 18, 3039, 279, 949, 807, 614, 537, 5798, 13, 576, 949, 807, 614, 537, 5798, 374, 220, 16, 17, 15, 20044, 13, 15277, 11, 279, 949, 807, 614, 5798, 374, 220, 18, 17568, 16, 17, 15, 28, 18, 21, 15, 20044, 13, 576, 2790, 3084, 315, 279, 5636, 374, 279, 2629, 315, 279, 949, 807, 614, 5798, 323, 279, 949, 807, 614, 537, 5798, 11, 429, 374, 11, 220, 18, 21, 15, 10, 16, 17, 15, 28, 19, 23, 15, 20044, 382, 2979, 596, 25, 19, 23, 15, 3417, 20044, 151645]\n",
      "[INFO:swift] [INPUT] <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "#Output#\n",
      "The output must be consistent with the final answer after reasoning.\n",
      "#Math problem#\n",
      "A road construction team is building a road. The part they have built is 3 times the part they have not built. There are 120 meters left. How long is the road?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The part they have built is 3 times the part they have not built. The part they have not built is 120 meters. Therefore, the part they have built is 3×120=360 meters. The total length of the road is the sum of the part they have built and the part they have not built, that is, 360+120=480 meters.\n",
      "\n",
      "{{ans:480}} meters<|im_end|>\n",
      "[INFO:swift] [LABELS_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 785, 949, 807, 614, 5798, 374, 220, 18, 3039, 279, 949, 807, 614, 537, 5798, 13, 576, 949, 807, 614, 537, 5798, 374, 220, 16, 17, 15, 20044, 13, 15277, 11, 279, 949, 807, 614, 5798, 374, 220, 18, 17568, 16, 17, 15, 28, 18, 21, 15, 20044, 13, 576, 2790, 3084, 315, 279, 5636, 374, 279, 2629, 315, 279, 949, 807, 614, 5798, 323, 279, 949, 807, 614, 537, 5798, 11, 429, 374, 11, 220, 18, 21, 15, 10, 16, 17, 15, 28, 19, 23, 15, 20044, 382, 2979, 596, 25, 19, 23, 15, 3417, 20044, 151645]\n",
      "[INFO:swift] [LABELS] [-100 * 88]The part they have built is 3 times the part they have not built. The part they have not built is 120 meters. Therefore, the part they have built is 3×120=360 meters. The total length of the road is the sum of the part they have built and the part they have not built, that is, 360+120=480 meters.\n",
      "\n",
      "{{ans:480}} meters<|im_end|>\n",
      "Map:   0%|                                             | 0/1102 [00:00<?, ?it/s][WARNING:swift] Current length of row(519) is larger than the max_length(512), deleted.\n",
      "Map:  31%|██████████▎                      | 344/1102 [00:00<00:00, 1726.24it/s][WARNING:swift] Current length of row(627) is larger than the max_length(512), deleted.\n",
      "Map:  78%|█████████████████████████▉       | 865/1102 [00:00<00:00, 1719.74it/s][WARNING:swift] Current length of row(529) is larger than the max_length(512), deleted.\n",
      "Map: 100%|████████████████████████████████| 1102/1102 [00:00<00:00, 1724.29it/s]\n",
      "Map: 100%|████████████████████████████████████| 11/11 [00:00<00:00, 1583.19it/s]\n",
      "[INFO:swift] Dataset Token Length: 205.472247±65.802901, min=71.000000, max=493.000000, size=1099\n",
      "[INFO:swift] Dataset Token Length: 230.000000±82.677907, min=152.000000, max=453.000000, size=11\n",
      "[INFO:swift] training_args: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "acc_strategy=token,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': False, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.95,\n",
      "adam_epsilon=1e-08,\n",
      "additional_saved_files=[],\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=42,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=1,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=20,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      ",\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/runs,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "loss_name=None,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "metric_warmup_step=0,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=15,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=20,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_dataset_sample=-1,\n",
      "train_sampler_random=True,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      ")\n",
      "[INFO:swift] The SftArguments will be saved in: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/sft_args.json\n",
      "[INFO:swift] The Seq2SeqTrainingArguments will be saved in: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/training_args.json\n",
      "[INFO:swift] The logging file will be saved in: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/logging.jsonl\n",
      "[2025-07-15 15:08:35,925] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Train:   0%|                                           | 0/1035 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.94491345, 'acc': 0.7805627, 'grad_norm': 0.86230594, 'learning_rate': 9.6e-07, 'memory(GiB)': 13.97, 'train_speed(iter/s)': 0.381783, 'epoch': 0.01, 'global_step/max_steps': '1/1035', 'percentage': '0.10%', 'elapsed_time': '1s', 'remaining_time': '28m 24s'}\n",
      "{'loss': 1.00334072, 'acc': 0.76032031, 'grad_norm': 0.95367336, 'learning_rate': 4.81e-06, 'memory(GiB)': 18.35, 'train_speed(iter/s)': 0.571498, 'epoch': 0.07, 'global_step/max_steps': '5/1035', 'percentage': '0.48%', 'elapsed_time': '7s', 'remaining_time': '26m 42s'}\n",
      "{'loss': 1.0227046, 'acc': 0.75780354, 'grad_norm': 0.88554728, 'learning_rate': 9.62e-06, 'memory(GiB)': 18.35, 'train_speed(iter/s)': 0.625314, 'epoch': 0.14, 'global_step/max_steps': '10/1035', 'percentage': '0.97%', 'elapsed_time': '15s', 'remaining_time': '25m 39s'}\n",
      "{'loss': 1.00744486, 'acc': 0.76809444, 'grad_norm': 0.74635774, 'learning_rate': 1.442e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.614125, 'epoch': 0.22, 'global_step/max_steps': '15/1035', 'percentage': '1.45%', 'elapsed_time': '23s', 'remaining_time': '26m 34s'}\n",
      "{'loss': 0.96410437, 'acc': 0.7790431, 'grad_norm': 0.86207139, 'learning_rate': 1.923e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.625658, 'epoch': 0.29, 'global_step/max_steps': '20/1035', 'percentage': '1.93%', 'elapsed_time': '30s', 'remaining_time': '26m 13s'}\n",
      "Train:   2%|▋                                 | 20/1035 [00:30<24:24,  1.44s/it]\n",
      "{'eval_loss': 0.85050249, 'eval_acc': 0.80206897, 'eval_runtime': 0.4429, 'eval_samples_per_second': 24.838, 'eval_steps_per_second': 2.258, 'epoch': 0.29, 'global_step/max_steps': '20/1035', 'percentage': '1.93%', 'elapsed_time': '31s', 'remaining_time': '26m 35s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 142.09it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-20\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 1.00407104, 'acc': 0.7611608, 'grad_norm': 0.74839199, 'learning_rate': 2.404e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.623026, 'epoch': 0.36, 'global_step/max_steps': '25/1035', 'percentage': '2.42%', 'elapsed_time': '39s', 'remaining_time': '26m 21s'}\n",
      "{'loss': 0.93928471, 'acc': 0.77809868, 'grad_norm': 0.65893853, 'learning_rate': 2.885e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.636357, 'epoch': 0.43, 'global_step/max_steps': '30/1035', 'percentage': '2.90%', 'elapsed_time': '46s', 'remaining_time': '25m 46s'}\n",
      "{'loss': 0.98236876, 'acc': 0.7617672, 'grad_norm': 0.75905114, 'learning_rate': 3.365e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.64311, 'epoch': 0.51, 'global_step/max_steps': '35/1035', 'percentage': '3.38%', 'elapsed_time': '53s', 'remaining_time': '25m 27s'}\n",
      "{'loss': 0.89460392, 'acc': 0.7797492, 'grad_norm': 0.60562557, 'learning_rate': 3.846e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.654752, 'epoch': 0.58, 'global_step/max_steps': '40/1035', 'percentage': '3.86%', 'elapsed_time': '1m 0s', 'remaining_time': '24m 55s'}\n",
      "Train:   4%|█▎                                | 40/1035 [01:00<21:54,  1.32s/it]\n",
      "{'eval_loss': 0.79097557, 'eval_acc': 0.80758621, 'eval_runtime': 0.4446, 'eval_samples_per_second': 24.742, 'eval_steps_per_second': 2.249, 'epoch': 0.58, 'global_step/max_steps': '40/1035', 'percentage': '3.86%', 'elapsed_time': '1m 0s', 'remaining_time': '25m 6s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 134.36it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-40\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.90023022, 'acc': 0.77217813, 'grad_norm': 0.49743572, 'learning_rate': 4.327e-05, 'memory(GiB)': 18.46, 'train_speed(iter/s)': 0.661227, 'epoch': 0.65, 'global_step/max_steps': '45/1035', 'percentage': '4.35%', 'elapsed_time': '1m 7s', 'remaining_time': '24m 35s'}\n",
      "{'loss': 0.87132645, 'acc': 0.78187051, 'grad_norm': 0.49643514, 'learning_rate': 4.808e-05, 'memory(GiB)': 18.52, 'train_speed(iter/s)': 0.657971, 'epoch': 0.72, 'global_step/max_steps': '50/1035', 'percentage': '4.83%', 'elapsed_time': '1m 15s', 'remaining_time': '24m 37s'}\n",
      "{'loss': 0.89736862, 'acc': 0.7703536, 'grad_norm': 0.52285045, 'learning_rate': 5e-05, 'memory(GiB)': 18.52, 'train_speed(iter/s)': 0.666642, 'epoch': 0.8, 'global_step/max_steps': '55/1035', 'percentage': '5.31%', 'elapsed_time': '1m 21s', 'remaining_time': '24m 12s'}\n",
      "{'loss': 0.87391739, 'acc': 0.77238293, 'grad_norm': 0.56092429, 'learning_rate': 4.999e-05, 'memory(GiB)': 18.52, 'train_speed(iter/s)': 0.667054, 'epoch': 0.87, 'global_step/max_steps': '60/1035', 'percentage': '5.80%', 'elapsed_time': '1m 28s', 'remaining_time': '24m 5s'}\n",
      "Train:   6%|█▉                                | 60/1035 [01:28<23:37,  1.45s/it]\n",
      "{'eval_loss': 0.72432202, 'eval_acc': 0.82206897, 'eval_runtime': 0.4454, 'eval_samples_per_second': 24.695, 'eval_steps_per_second': 2.245, 'epoch': 0.87, 'global_step/max_steps': '60/1035', 'percentage': '5.80%', 'elapsed_time': '1m 29s', 'remaining_time': '24m 13s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 142.01it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-60\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.87965412, 'acc': 0.78152418, 'grad_norm': 0.4646323, 'learning_rate': 4.998e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.652374, 'epoch': 0.94, 'global_step/max_steps': '65/1035', 'percentage': '6.28%', 'elapsed_time': '1m 38s', 'remaining_time': '24m 32s'}\n",
      "{'loss': 0.88766747, 'acc': 0.77716184, 'grad_norm': 0.52757925, 'learning_rate': 4.996e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654357, 'epoch': 1.01, 'global_step/max_steps': '70/1035', 'percentage': '6.76%', 'elapsed_time': '1m 46s', 'remaining_time': '24m 21s'}\n",
      "{'loss': 0.76734209, 'acc': 0.80007133, 'grad_norm': 0.47027859, 'learning_rate': 4.993e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656662, 'epoch': 1.09, 'global_step/max_steps': '75/1035', 'percentage': '7.25%', 'elapsed_time': '1m 53s', 'remaining_time': '24m 9s'}\n",
      "{'loss': 0.75557866, 'acc': 0.80775261, 'grad_norm': 0.51294112, 'learning_rate': 4.99e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.661086, 'epoch': 1.16, 'global_step/max_steps': '80/1035', 'percentage': '7.73%', 'elapsed_time': '2m 0s', 'remaining_time': '23m 53s'}\n",
      "Train:   8%|██▋                               | 80/1035 [02:00<22:22,  1.41s/it]\n",
      "{'eval_loss': 0.66574389, 'eval_acc': 0.8262069, 'eval_runtime': 0.4582, 'eval_samples_per_second': 24.005, 'eval_steps_per_second': 2.182, 'epoch': 1.16, 'global_step/max_steps': '80/1035', 'percentage': '7.73%', 'elapsed_time': '2m 0s', 'remaining_time': '23m 58s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 142.10it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-80\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.77594795, 'acc': 0.79565911, 'grad_norm': 0.51051444, 'learning_rate': 4.986e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.658273, 'epoch': 1.23, 'global_step/max_steps': '85/1035', 'percentage': '8.21%', 'elapsed_time': '2m 8s', 'remaining_time': '23m 52s'}\n",
      "{'loss': 0.75057688, 'acc': 0.7984128, 'grad_norm': 0.51856208, 'learning_rate': 4.982e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.65891, 'epoch': 1.3, 'global_step/max_steps': '90/1035', 'percentage': '8.70%', 'elapsed_time': '2m 15s', 'remaining_time': '23m 43s'}\n",
      "{'loss': 0.79699135, 'acc': 0.78818426, 'grad_norm': 0.61479712, 'learning_rate': 4.976e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655123, 'epoch': 1.38, 'global_step/max_steps': '95/1035', 'percentage': '9.18%', 'elapsed_time': '2m 24s', 'remaining_time': '23m 45s'}\n",
      "{'loss': 0.78327203, 'acc': 0.79136305, 'grad_norm': 0.54669398, 'learning_rate': 4.971e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.653145, 'epoch': 1.45, 'global_step/max_steps': '100/1035', 'percentage': '9.66%', 'elapsed_time': '2m 32s', 'remaining_time': '23m 42s'}\n",
      "Train:  10%|███▏                             | 100/1035 [02:32<27:42,  1.78s/it]\n",
      "{'eval_loss': 0.6168977, 'eval_acc': 0.83241379, 'eval_runtime': 0.4512, 'eval_samples_per_second': 24.378, 'eval_steps_per_second': 2.216, 'epoch': 1.45, 'global_step/max_steps': '100/1035', 'percentage': '9.66%', 'elapsed_time': '2m 32s', 'remaining_time': '23m 46s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 143.37it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-100\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.71152973, 'acc': 0.79979868, 'grad_norm': 0.58128321, 'learning_rate': 4.964e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.652436, 'epoch': 1.52, 'global_step/max_steps': '105/1035', 'percentage': '10.14%', 'elapsed_time': '2m 39s', 'remaining_time': '23m 36s'}\n",
      "{'loss': 0.67902894, 'acc': 0.81634607, 'grad_norm': 0.58450872, 'learning_rate': 4.957e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.649857, 'epoch': 1.59, 'global_step/max_steps': '110/1035', 'percentage': '10.63%', 'elapsed_time': '2m 48s', 'remaining_time': '23m 35s'}\n",
      "{'loss': 0.69143329, 'acc': 0.80809698, 'grad_norm': 0.52335644, 'learning_rate': 4.949e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.651436, 'epoch': 1.67, 'global_step/max_steps': '115/1035', 'percentage': '11.11%', 'elapsed_time': '2m 55s', 'remaining_time': '23m 24s'}\n",
      "{'loss': 0.68804312, 'acc': 0.81511478, 'grad_norm': 0.60152215, 'learning_rate': 4.941e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655669, 'epoch': 1.74, 'global_step/max_steps': '120/1035', 'percentage': '11.59%', 'elapsed_time': '3m 2s', 'remaining_time': '23m 8s'}\n",
      "Train:  12%|███▊                             | 120/1035 [03:02<21:07,  1.39s/it]\n",
      "{'eval_loss': 0.59048212, 'eval_acc': 0.8337931, 'eval_runtime': 0.4489, 'eval_samples_per_second': 24.502, 'eval_steps_per_second': 2.227, 'epoch': 1.74, 'global_step/max_steps': '120/1035', 'percentage': '11.59%', 'elapsed_time': '3m 2s', 'remaining_time': '23m 11s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 146.38it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-120\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.71291213, 'acc': 0.80809498, 'grad_norm': 0.55999255, 'learning_rate': 4.932e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655911, 'epoch': 1.81, 'global_step/max_steps': '125/1035', 'percentage': '12.08%', 'elapsed_time': '3m 9s', 'remaining_time': '23m 0s'}\n",
      "{'loss': 0.72835746, 'acc': 0.79720545, 'grad_norm': 0.54086882, 'learning_rate': 4.923e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654094, 'epoch': 1.88, 'global_step/max_steps': '130/1035', 'percentage': '12.56%', 'elapsed_time': '3m 17s', 'remaining_time': '22m 56s'}\n",
      "{'loss': 0.64671206, 'acc': 0.81808834, 'grad_norm': 0.62860543, 'learning_rate': 4.913e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657566, 'epoch': 1.96, 'global_step/max_steps': '135/1035', 'percentage': '13.04%', 'elapsed_time': '3m 24s', 'remaining_time': '22m 42s'}\n",
      "{'loss': 0.70217505, 'acc': 0.80464659, 'grad_norm': 0.53244823, 'learning_rate': 4.902e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.660691, 'epoch': 2.03, 'global_step/max_steps': '140/1035', 'percentage': '13.53%', 'elapsed_time': '3m 30s', 'remaining_time': '22m 28s'}\n",
      "Train:  14%|████▍                            | 140/1035 [03:30<20:45,  1.39s/it]\n",
      "{'eval_loss': 0.5720371, 'eval_acc': 0.84137931, 'eval_runtime': 0.4515, 'eval_samples_per_second': 24.364, 'eval_steps_per_second': 2.215, 'epoch': 2.03, 'global_step/max_steps': '140/1035', 'percentage': '13.53%', 'elapsed_time': '3m 31s', 'remaining_time': '22m 31s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 143.88it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-140\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.58392467, 'acc': 0.83035374, 'grad_norm': 0.59586382, 'learning_rate': 4.89e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.659526, 'epoch': 2.1, 'global_step/max_steps': '145/1035', 'percentage': '14.01%', 'elapsed_time': '3m 38s', 'remaining_time': '22m 23s'}\n",
      "{'loss': 0.66979427, 'acc': 0.81480217, 'grad_norm': 0.5530104, 'learning_rate': 4.878e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657504, 'epoch': 2.17, 'global_step/max_steps': '150/1035', 'percentage': '14.49%', 'elapsed_time': '3m 47s', 'remaining_time': '22m 20s'}\n",
      "{'loss': 0.63923268, 'acc': 0.81968212, 'grad_norm': 0.82931679, 'learning_rate': 4.866e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657342, 'epoch': 2.25, 'global_step/max_steps': '155/1035', 'percentage': '14.98%', 'elapsed_time': '3m 54s', 'remaining_time': '22m 13s'}\n",
      "{'loss': 0.67769175, 'acc': 0.81066427, 'grad_norm': 0.66174036, 'learning_rate': 4.853e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655638, 'epoch': 2.32, 'global_step/max_steps': '160/1035', 'percentage': '15.46%', 'elapsed_time': '4m 3s', 'remaining_time': '22m 9s'}\n",
      "Train:  15%|█████                            | 160/1035 [04:03<22:17,  1.53s/it]\n",
      "{'eval_loss': 0.55852634, 'eval_acc': 0.84551724, 'eval_runtime': 0.452, 'eval_samples_per_second': 24.336, 'eval_steps_per_second': 2.212, 'epoch': 2.32, 'global_step/max_steps': '160/1035', 'percentage': '15.46%', 'elapsed_time': '4m 3s', 'remaining_time': '22m 11s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.52it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-160\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.61956091, 'acc': 0.8217555, 'grad_norm': 0.67857283, 'learning_rate': 4.839e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654255, 'epoch': 2.39, 'global_step/max_steps': '165/1035', 'percentage': '15.94%', 'elapsed_time': '4m 11s', 'remaining_time': '22m 4s'}\n",
      "{'loss': 0.71082864, 'acc': 0.80410347, 'grad_norm': 0.99022317, 'learning_rate': 4.824e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.65577, 'epoch': 2.46, 'global_step/max_steps': '170/1035', 'percentage': '16.43%', 'elapsed_time': '4m 18s', 'remaining_time': '21m 54s'}\n",
      "{'loss': 0.63657498, 'acc': 0.8222249, 'grad_norm': 0.75448585, 'learning_rate': 4.809e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655696, 'epoch': 2.54, 'global_step/max_steps': '175/1035', 'percentage': '16.91%', 'elapsed_time': '4m 25s', 'remaining_time': '21m 46s'}\n",
      "{'loss': 0.61871386, 'acc': 0.82316189, 'grad_norm': 0.73127443, 'learning_rate': 4.794e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656414, 'epoch': 2.61, 'global_step/max_steps': '180/1035', 'percentage': '17.39%', 'elapsed_time': '4m 33s', 'remaining_time': '21m 37s'}\n",
      "Train:  17%|█████▋                           | 180/1035 [04:33<21:04,  1.48s/it]\n",
      "{'eval_loss': 0.55242795, 'eval_acc': 0.84827586, 'eval_runtime': 0.4521, 'eval_samples_per_second': 24.332, 'eval_steps_per_second': 2.212, 'epoch': 2.61, 'global_step/max_steps': '180/1035', 'percentage': '17.39%', 'elapsed_time': '4m 33s', 'remaining_time': '21m 40s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 146.13it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-180\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.67503471, 'acc': 0.81287155, 'grad_norm': 0.80580091, 'learning_rate': 4.778e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.652548, 'epoch': 2.68, 'global_step/max_steps': '185/1035', 'percentage': '17.87%', 'elapsed_time': '4m 42s', 'remaining_time': '21m 38s'}\n",
      "{'loss': 0.64321146, 'acc': 0.81927786, 'grad_norm': 0.71948099, 'learning_rate': 4.761e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.653786, 'epoch': 2.75, 'global_step/max_steps': '190/1035', 'percentage': '18.36%', 'elapsed_time': '4m 49s', 'remaining_time': '21m 28s'}\n",
      "{'loss': 0.67609572, 'acc': 0.81705008, 'grad_norm': 0.64595777, 'learning_rate': 4.743e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.652829, 'epoch': 2.83, 'global_step/max_steps': '195/1035', 'percentage': '18.84%', 'elapsed_time': '4m 57s', 'remaining_time': '21m 22s'}\n",
      "{'loss': 0.63552065, 'acc': 0.82011509, 'grad_norm': 0.72556317, 'learning_rate': 4.726e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.653141, 'epoch': 2.9, 'global_step/max_steps': '200/1035', 'percentage': '19.32%', 'elapsed_time': '5m 5s', 'remaining_time': '21m 14s'}\n",
      "Train:  19%|██████▍                          | 200/1035 [05:05<20:48,  1.50s/it]\n",
      "{'eval_loss': 0.54682064, 'eval_acc': 0.84758621, 'eval_runtime': 0.4511, 'eval_samples_per_second': 24.385, 'eval_steps_per_second': 2.217, 'epoch': 2.9, 'global_step/max_steps': '200/1035', 'percentage': '19.32%', 'elapsed_time': '5m 5s', 'remaining_time': '21m 16s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 144.62it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-200\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.66473141, 'acc': 0.81297951, 'grad_norm': 0.63662922, 'learning_rate': 4.707e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.651961, 'epoch': 2.97, 'global_step/max_steps': '205/1035', 'percentage': '19.81%', 'elapsed_time': '5m 13s', 'remaining_time': '21m 9s'}\n",
      "{'loss': 0.63443708, 'acc': 0.82331238, 'grad_norm': 0.77674061, 'learning_rate': 4.688e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654203, 'epoch': 3.04, 'global_step/max_steps': '210/1035', 'percentage': '20.29%', 'elapsed_time': '5m 20s', 'remaining_time': '20m 57s'}\n",
      "{'loss': 0.65482206, 'acc': 0.81554947, 'grad_norm': 0.92806733, 'learning_rate': 4.668e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655579, 'epoch': 3.12, 'global_step/max_steps': '215/1035', 'percentage': '20.77%', 'elapsed_time': '5m 26s', 'remaining_time': '20m 47s'}\n",
      "{'loss': 0.59296322, 'acc': 0.82995224, 'grad_norm': 0.78967959, 'learning_rate': 4.648e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656994, 'epoch': 3.19, 'global_step/max_steps': '220/1035', 'percentage': '21.26%', 'elapsed_time': '5m 33s', 'remaining_time': '20m 36s'}\n",
      "Train:  21%|███████                          | 220/1035 [05:33<18:39,  1.37s/it]\n",
      "{'eval_loss': 0.54511452, 'eval_acc': 0.84896552, 'eval_runtime': 0.4572, 'eval_samples_per_second': 24.058, 'eval_steps_per_second': 2.187, 'epoch': 3.19, 'global_step/max_steps': '220/1035', 'percentage': '21.26%', 'elapsed_time': '5m 34s', 'remaining_time': '20m 38s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.39it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-220\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.64124613, 'acc': 0.81764088, 'grad_norm': 0.79495919, 'learning_rate': 4.628e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655299, 'epoch': 3.26, 'global_step/max_steps': '225/1035', 'percentage': '21.74%', 'elapsed_time': '5m 42s', 'remaining_time': '20m 32s'}\n",
      "{'loss': 0.61029844, 'acc': 0.82637548, 'grad_norm': 0.88803798, 'learning_rate': 4.606e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654756, 'epoch': 3.33, 'global_step/max_steps': '230/1035', 'percentage': '22.22%', 'elapsed_time': '5m 50s', 'remaining_time': '20m 26s'}\n",
      "{'loss': 0.54739361, 'acc': 0.83894472, 'grad_norm': 0.73521179, 'learning_rate': 4.584e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655313, 'epoch': 3.41, 'global_step/max_steps': '235/1035', 'percentage': '22.71%', 'elapsed_time': '5m 57s', 'remaining_time': '20m 17s'}\n",
      "{'loss': 0.69328775, 'acc': 0.81071568, 'grad_norm': 0.75038695, 'learning_rate': 4.562e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654642, 'epoch': 3.48, 'global_step/max_steps': '240/1035', 'percentage': '23.19%', 'elapsed_time': '6m 5s', 'remaining_time': '20m 11s'}\n",
      "Train:  23%|███████▋                         | 240/1035 [06:05<22:09,  1.67s/it]\n",
      "{'eval_loss': 0.53992003, 'eval_acc': 0.85103448, 'eval_runtime': 0.4542, 'eval_samples_per_second': 24.22, 'eval_steps_per_second': 2.202, 'epoch': 3.48, 'global_step/max_steps': '240/1035', 'percentage': '23.19%', 'elapsed_time': '6m 6s', 'remaining_time': '20m 12s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.86it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-240\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.63879056, 'acc': 0.8195241, 'grad_norm': 0.89627087, 'learning_rate': 4.539e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.653836, 'epoch': 3.55, 'global_step/max_steps': '245/1035', 'percentage': '23.67%', 'elapsed_time': '6m 13s', 'remaining_time': '20m 5s'}\n",
      "{'loss': 0.57856946, 'acc': 0.83747778, 'grad_norm': 0.72520524, 'learning_rate': 4.516e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.653297, 'epoch': 3.62, 'global_step/max_steps': '250/1035', 'percentage': '24.15%', 'elapsed_time': '6m 21s', 'remaining_time': '19m 58s'}\n",
      "{'loss': 0.63165708, 'acc': 0.82011013, 'grad_norm': 0.72257423, 'learning_rate': 4.492e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.653276, 'epoch': 3.7, 'global_step/max_steps': '255/1035', 'percentage': '24.64%', 'elapsed_time': '6m 29s', 'remaining_time': '19m 51s'}\n",
      "{'loss': 0.65303526, 'acc': 0.81511259, 'grad_norm': 0.87296259, 'learning_rate': 4.468e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.653826, 'epoch': 3.77, 'global_step/max_steps': '260/1035', 'percentage': '25.12%', 'elapsed_time': '6m 36s', 'remaining_time': '19m 42s'}\n",
      "Train:  25%|████████▎                        | 260/1035 [06:36<18:53,  1.46s/it]\n",
      "{'eval_loss': 0.53735042, 'eval_acc': 0.84689655, 'eval_runtime': 0.451, 'eval_samples_per_second': 24.391, 'eval_steps_per_second': 2.217, 'epoch': 3.77, 'global_step/max_steps': '260/1035', 'percentage': '25.12%', 'elapsed_time': '6m 37s', 'remaining_time': '19m 43s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.01it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-260\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.62434549, 'acc': 0.82487698, 'grad_norm': 0.89553124, 'learning_rate': 4.443e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.653374, 'epoch': 3.84, 'global_step/max_steps': '265/1035', 'percentage': '25.60%', 'elapsed_time': '6m 44s', 'remaining_time': '19m 35s'}\n",
      "{'loss': 0.62978702, 'acc': 0.82268181, 'grad_norm': 0.89526784, 'learning_rate': 4.417e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.65504, 'epoch': 3.91, 'global_step/max_steps': '270/1035', 'percentage': '26.09%', 'elapsed_time': '6m 51s', 'remaining_time': '19m 25s'}\n",
      "{'loss': 0.58265572, 'acc': 0.83417044, 'grad_norm': 0.81014735, 'learning_rate': 4.392e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654688, 'epoch': 3.99, 'global_step/max_steps': '275/1035', 'percentage': '26.57%', 'elapsed_time': '6m 59s', 'remaining_time': '19m 18s'}\n",
      "{'loss': 0.57660503, 'acc': 0.83616915, 'grad_norm': 0.74732357, 'learning_rate': 4.365e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.65627, 'epoch': 4.06, 'global_step/max_steps': '280/1035', 'percentage': '27.05%', 'elapsed_time': '7m 5s', 'remaining_time': '19m 7s'}\n",
      "Train:  27%|████████▉                        | 280/1035 [07:05<18:09,  1.44s/it]\n",
      "{'eval_loss': 0.53230399, 'eval_acc': 0.84758621, 'eval_runtime': 0.4519, 'eval_samples_per_second': 24.343, 'eval_steps_per_second': 2.213, 'epoch': 4.06, 'global_step/max_steps': '280/1035', 'percentage': '27.05%', 'elapsed_time': '7m 6s', 'remaining_time': '19m 9s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.41it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-280\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.58360472, 'acc': 0.83410473, 'grad_norm': 0.86982232, 'learning_rate': 4.338e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656097, 'epoch': 4.13, 'global_step/max_steps': '285/1035', 'percentage': '27.54%', 'elapsed_time': '7m 13s', 'remaining_time': '19m 0s'}\n",
      "{'loss': 0.62252741, 'acc': 0.82260771, 'grad_norm': 0.91529763, 'learning_rate': 4.311e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657329, 'epoch': 4.2, 'global_step/max_steps': '290/1035', 'percentage': '28.02%', 'elapsed_time': '7m 20s', 'remaining_time': '18m 50s'}\n",
      "{'loss': 0.61864681, 'acc': 0.82275457, 'grad_norm': 0.82190174, 'learning_rate': 4.283e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.6559, 'epoch': 4.28, 'global_step/max_steps': '295/1035', 'percentage': '28.50%', 'elapsed_time': '7m 28s', 'remaining_time': '18m 45s'}\n",
      "{'loss': 0.57742624, 'acc': 0.83110466, 'grad_norm': 0.80736446, 'learning_rate': 4.255e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655879, 'epoch': 4.35, 'global_step/max_steps': '300/1035', 'percentage': '28.99%', 'elapsed_time': '7m 36s', 'remaining_time': '18m 38s'}\n",
      "Train:  29%|█████████▌                       | 300/1035 [07:36<19:35,  1.60s/it]\n",
      "{'eval_loss': 0.53147429, 'eval_acc': 0.84827586, 'eval_runtime': 0.448, 'eval_samples_per_second': 24.553, 'eval_steps_per_second': 2.232, 'epoch': 4.35, 'global_step/max_steps': '300/1035', 'percentage': '28.99%', 'elapsed_time': '7m 36s', 'remaining_time': '18m 39s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 146.74it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-300\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.58252897, 'acc': 0.83467064, 'grad_norm': 1.07867098, 'learning_rate': 4.226e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654905, 'epoch': 4.42, 'global_step/max_steps': '305/1035', 'percentage': '29.47%', 'elapsed_time': '7m 44s', 'remaining_time': '18m 32s'}\n",
      "{'loss': 0.649861, 'acc': 0.81568899, 'grad_norm': 0.85040557, 'learning_rate': 4.197e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654144, 'epoch': 4.49, 'global_step/max_steps': '310/1035', 'percentage': '29.95%', 'elapsed_time': '7m 52s', 'remaining_time': '18m 26s'}\n",
      "{'loss': 0.57358932, 'acc': 0.83675108, 'grad_norm': 0.86273253, 'learning_rate': 4.168e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655365, 'epoch': 4.57, 'global_step/max_steps': '315/1035', 'percentage': '30.43%', 'elapsed_time': '7m 59s', 'remaining_time': '18m 16s'}\n",
      "{'loss': 0.59532127, 'acc': 0.83298512, 'grad_norm': 0.93077648, 'learning_rate': 4.138e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654506, 'epoch': 4.64, 'global_step/max_steps': '320/1035', 'percentage': '30.92%', 'elapsed_time': '8m 7s', 'remaining_time': '18m 10s'}\n",
      "Train:  31%|██████████▏                      | 320/1035 [08:07<19:17,  1.62s/it]\n",
      "{'eval_loss': 0.52766031, 'eval_acc': 0.85034483, 'eval_runtime': 0.4491, 'eval_samples_per_second': 24.494, 'eval_steps_per_second': 2.227, 'epoch': 4.64, 'global_step/max_steps': '320/1035', 'percentage': '30.92%', 'elapsed_time': '8m 8s', 'remaining_time': '18m 11s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.37it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-320\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.58943048, 'acc': 0.83146935, 'grad_norm': 0.84144014, 'learning_rate': 4.107e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654623, 'epoch': 4.71, 'global_step/max_steps': '325/1035', 'percentage': '31.40%', 'elapsed_time': '8m 15s', 'remaining_time': '18m 2s'}\n",
      "{'loss': 0.56192527, 'acc': 0.833078, 'grad_norm': 0.83931661, 'learning_rate': 4.077e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654466, 'epoch': 4.78, 'global_step/max_steps': '330/1035', 'percentage': '31.88%', 'elapsed_time': '8m 23s', 'remaining_time': '17m 55s'}\n",
      "{'loss': 0.57221146, 'acc': 0.83540888, 'grad_norm': 0.96885145, 'learning_rate': 4.045e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654766, 'epoch': 4.86, 'global_step/max_steps': '335/1035', 'percentage': '32.37%', 'elapsed_time': '8m 30s', 'remaining_time': '17m 47s'}\n",
      "{'loss': 0.61261606, 'acc': 0.82209978, 'grad_norm': 1.02321684, 'learning_rate': 4.014e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655147, 'epoch': 4.93, 'global_step/max_steps': '340/1035', 'percentage': '32.85%', 'elapsed_time': '8m 37s', 'remaining_time': '17m 38s'}\n",
      "Train:  33%|██████████▊                      | 340/1035 [08:37<16:49,  1.45s/it]\n",
      "{'eval_loss': 0.52437842, 'eval_acc': 0.85034483, 'eval_runtime': 0.4482, 'eval_samples_per_second': 24.541, 'eval_steps_per_second': 2.231, 'epoch': 4.93, 'global_step/max_steps': '340/1035', 'percentage': '32.85%', 'elapsed_time': '8m 38s', 'remaining_time': '17m 39s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 144.34it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-340\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.64423213, 'acc': 0.82135601, 'grad_norm': 0.96172297, 'learning_rate': 3.982e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655604, 'epoch': 5.0, 'global_step/max_steps': '345/1035', 'percentage': '33.33%', 'elapsed_time': '8m 45s', 'remaining_time': '17m 30s'}\n",
      "{'loss': 0.60603938, 'acc': 0.82862082, 'grad_norm': 0.89034599, 'learning_rate': 3.949e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655983, 'epoch': 5.07, 'global_step/max_steps': '350/1035', 'percentage': '33.82%', 'elapsed_time': '8m 52s', 'remaining_time': '17m 22s'}\n",
      "{'loss': 0.56694984, 'acc': 0.8345335, 'grad_norm': 0.90590042, 'learning_rate': 3.917e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656915, 'epoch': 5.14, 'global_step/max_steps': '355/1035', 'percentage': '34.30%', 'elapsed_time': '8m 59s', 'remaining_time': '17m 13s'}\n",
      "{'loss': 0.61310635, 'acc': 0.82817488, 'grad_norm': 0.90397412, 'learning_rate': 3.884e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657416, 'epoch': 5.22, 'global_step/max_steps': '360/1035', 'percentage': '34.78%', 'elapsed_time': '9m 6s', 'remaining_time': '17m 4s'}\n",
      "Train:  35%|███████████▍                     | 360/1035 [09:06<15:52,  1.41s/it]\n",
      "{'eval_loss': 0.52875036, 'eval_acc': 0.85310345, 'eval_runtime': 0.4533, 'eval_samples_per_second': 24.267, 'eval_steps_per_second': 2.206, 'epoch': 5.22, 'global_step/max_steps': '360/1035', 'percentage': '34.78%', 'elapsed_time': '9m 7s', 'remaining_time': '17m 5s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 144.93it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-360\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.55673656, 'acc': 0.83890095, 'grad_norm': 0.76817125, 'learning_rate': 3.85e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657126, 'epoch': 5.29, 'global_step/max_steps': '365/1035', 'percentage': '35.27%', 'elapsed_time': '9m 14s', 'remaining_time': '16m 57s'}\n",
      "{'loss': 0.58951144, 'acc': 0.83136988, 'grad_norm': 1.28195977, 'learning_rate': 3.816e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.658035, 'epoch': 5.36, 'global_step/max_steps': '370/1035', 'percentage': '35.75%', 'elapsed_time': '9m 21s', 'remaining_time': '16m 48s'}\n",
      "{'loss': 0.62088642, 'acc': 0.82450771, 'grad_norm': 0.96977568, 'learning_rate': 3.782e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.659133, 'epoch': 5.43, 'global_step/max_steps': '375/1035', 'percentage': '36.23%', 'elapsed_time': '9m 27s', 'remaining_time': '16m 39s'}\n",
      "{'loss': 0.56412673, 'acc': 0.83670969, 'grad_norm': 1.05998313, 'learning_rate': 3.748e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.65863, 'epoch': 5.51, 'global_step/max_steps': '380/1035', 'percentage': '36.71%', 'elapsed_time': '9m 35s', 'remaining_time': '16m 32s'}\n",
      "Train:  37%|████████████                     | 380/1035 [09:35<16:51,  1.54s/it]\n",
      "{'eval_loss': 0.52104276, 'eval_acc': 0.85103448, 'eval_runtime': 0.4503, 'eval_samples_per_second': 24.43, 'eval_steps_per_second': 2.221, 'epoch': 5.51, 'global_step/max_steps': '380/1035', 'percentage': '36.71%', 'elapsed_time': '9m 36s', 'remaining_time': '16m 33s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.92it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-380\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.58981318, 'acc': 0.835641, 'grad_norm': 0.84059346, 'learning_rate': 3.713e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657366, 'epoch': 5.58, 'global_step/max_steps': '385/1035', 'percentage': '37.20%', 'elapsed_time': '9m 44s', 'remaining_time': '16m 27s'}\n",
      "{'loss': 0.59873033, 'acc': 0.83248158, 'grad_norm': 0.89702582, 'learning_rate': 3.678e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656442, 'epoch': 5.65, 'global_step/max_steps': '390/1035', 'percentage': '37.68%', 'elapsed_time': '9m 53s', 'remaining_time': '16m 20s'}\n",
      "{'loss': 0.57128692, 'acc': 0.83237858, 'grad_norm': 0.91861266, 'learning_rate': 3.642e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.65658, 'epoch': 5.72, 'global_step/max_steps': '395/1035', 'percentage': '38.16%', 'elapsed_time': '10m 0s', 'remaining_time': '16m 13s'}\n",
      "{'loss': 0.60427561, 'acc': 0.82740488, 'grad_norm': 0.95956969, 'learning_rate': 3.607e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656874, 'epoch': 5.8, 'global_step/max_steps': '400/1035', 'percentage': '38.65%', 'elapsed_time': '10m 7s', 'remaining_time': '16m 5s'}\n",
      "Train:  39%|████████████▊                    | 400/1035 [10:07<15:21,  1.45s/it]\n",
      "{'eval_loss': 0.52221388, 'eval_acc': 0.85034483, 'eval_runtime': 0.4465, 'eval_samples_per_second': 24.635, 'eval_steps_per_second': 2.24, 'epoch': 5.8, 'global_step/max_steps': '400/1035', 'percentage': '38.65%', 'elapsed_time': '10m 8s', 'remaining_time': '16m 5s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.95it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-400\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.52401285, 'acc': 0.84250584, 'grad_norm': 1.06105638, 'learning_rate': 3.571e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656902, 'epoch': 5.87, 'global_step/max_steps': '405/1035', 'percentage': '39.13%', 'elapsed_time': '10m 15s', 'remaining_time': '15m 57s'}\n",
      "{'loss': 0.54796176, 'acc': 0.83902178, 'grad_norm': 1.01397228, 'learning_rate': 3.535e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656661, 'epoch': 5.94, 'global_step/max_steps': '410/1035', 'percentage': '39.61%', 'elapsed_time': '10m 23s', 'remaining_time': '15m 50s'}\n",
      "{'loss': 0.60911713, 'acc': 0.82701292, 'grad_norm': 1.12105513, 'learning_rate': 3.498e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656762, 'epoch': 6.01, 'global_step/max_steps': '415/1035', 'percentage': '40.10%', 'elapsed_time': '10m 30s', 'remaining_time': '15m 42s'}\n",
      "{'loss': 0.59225769, 'acc': 0.83505802, 'grad_norm': 1.11559856, 'learning_rate': 3.461e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657044, 'epoch': 6.09, 'global_step/max_steps': '420/1035', 'percentage': '40.58%', 'elapsed_time': '10m 38s', 'remaining_time': '15m 34s'}\n",
      "Train:  41%|█████████████▍                   | 420/1035 [10:38<15:01,  1.47s/it]\n",
      "{'eval_loss': 0.51804793, 'eval_acc': 0.8537931, 'eval_runtime': 0.4508, 'eval_samples_per_second': 24.399, 'eval_steps_per_second': 2.218, 'epoch': 6.09, 'global_step/max_steps': '420/1035', 'percentage': '40.58%', 'elapsed_time': '10m 38s', 'remaining_time': '15m 35s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 143.34it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-420\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.58274736, 'acc': 0.83438692, 'grad_norm': 0.89878678, 'learning_rate': 3.424e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656253, 'epoch': 6.16, 'global_step/max_steps': '425/1035', 'percentage': '41.06%', 'elapsed_time': '10m 46s', 'remaining_time': '15m 28s'}\n",
      "{'loss': 0.55666056, 'acc': 0.83861637, 'grad_norm': 1.01891088, 'learning_rate': 3.387e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657408, 'epoch': 6.23, 'global_step/max_steps': '430/1035', 'percentage': '41.55%', 'elapsed_time': '10m 53s', 'remaining_time': '15m 18s'}\n",
      "{'loss': 0.56091013, 'acc': 0.84015751, 'grad_norm': 0.91022837, 'learning_rate': 3.35e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657548, 'epoch': 6.3, 'global_step/max_steps': '435/1035', 'percentage': '42.03%', 'elapsed_time': '11m 0s', 'remaining_time': '15m 11s'}\n",
      "{'loss': 0.5436039, 'acc': 0.83928957, 'grad_norm': 1.04637384, 'learning_rate': 3.312e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.659034, 'epoch': 6.38, 'global_step/max_steps': '440/1035', 'percentage': '42.51%', 'elapsed_time': '11m 6s', 'remaining_time': '15m 1s'}\n",
      "Train:  43%|██████████████                   | 440/1035 [11:06<12:37,  1.27s/it]\n",
      "{'eval_loss': 0.52072251, 'eval_acc': 0.85103448, 'eval_runtime': 0.4488, 'eval_samples_per_second': 24.51, 'eval_steps_per_second': 2.228, 'epoch': 6.38, 'global_step/max_steps': '440/1035', 'percentage': '42.51%', 'elapsed_time': '11m 7s', 'remaining_time': '15m 2s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 146.17it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-440\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.58940854, 'acc': 0.82931299, 'grad_norm': 1.03092551, 'learning_rate': 3.274e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.658746, 'epoch': 6.45, 'global_step/max_steps': '445/1035', 'percentage': '43.00%', 'elapsed_time': '11m 14s', 'remaining_time': '14m 54s'}\n",
      "{'loss': 0.52349992, 'acc': 0.8477025, 'grad_norm': 1.05603874, 'learning_rate': 3.236e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.659693, 'epoch': 6.52, 'global_step/max_steps': '450/1035', 'percentage': '43.48%', 'elapsed_time': '11m 21s', 'remaining_time': '14m 45s'}\n",
      "{'loss': 0.55383005, 'acc': 0.83995457, 'grad_norm': 0.95072263, 'learning_rate': 3.198e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.659212, 'epoch': 6.59, 'global_step/max_steps': '455/1035', 'percentage': '43.96%', 'elapsed_time': '11m 29s', 'remaining_time': '14m 38s'}\n",
      "{'loss': 0.52382965, 'acc': 0.84597416, 'grad_norm': 0.93349737, 'learning_rate': 3.159e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.658458, 'epoch': 6.67, 'global_step/max_steps': '460/1035', 'percentage': '44.44%', 'elapsed_time': '11m 37s', 'remaining_time': '14m 32s'}\n",
      "Train:  44%|██████████████▋                  | 460/1035 [11:37<15:35,  1.63s/it]\n",
      "{'eval_loss': 0.51952624, 'eval_acc': 0.85172414, 'eval_runtime': 0.4511, 'eval_samples_per_second': 24.382, 'eval_steps_per_second': 2.217, 'epoch': 6.67, 'global_step/max_steps': '460/1035', 'percentage': '44.44%', 'elapsed_time': '11m 38s', 'remaining_time': '14m 32s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 146.56it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-460\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.62021608, 'acc': 0.82385855, 'grad_norm': 0.94638485, 'learning_rate': 3.121e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657741, 'epoch': 6.74, 'global_step/max_steps': '465/1035', 'percentage': '44.93%', 'elapsed_time': '11m 45s', 'remaining_time': '14m 25s'}\n",
      "{'loss': 0.57623863, 'acc': 0.83051643, 'grad_norm': 1.00628579, 'learning_rate': 3.082e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657948, 'epoch': 6.81, 'global_step/max_steps': '470/1035', 'percentage': '45.41%', 'elapsed_time': '11m 53s', 'remaining_time': '14m 17s'}\n",
      "{'loss': 0.555791, 'acc': 0.8411088, 'grad_norm': 1.12451589, 'learning_rate': 3.043e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656888, 'epoch': 6.88, 'global_step/max_steps': '475/1035', 'percentage': '45.89%', 'elapsed_time': '12m 2s', 'remaining_time': '14m 11s'}\n",
      "{'loss': 0.60328856, 'acc': 0.82721405, 'grad_norm': 1.13704276, 'learning_rate': 3.004e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656844, 'epoch': 6.96, 'global_step/max_steps': '480/1035', 'percentage': '46.38%', 'elapsed_time': '12m 9s', 'remaining_time': '14m 3s'}\n",
      "Train:  46%|███████████████▎                 | 480/1035 [12:09<14:25,  1.56s/it]\n",
      "{'eval_loss': 0.5165751, 'eval_acc': 0.85517241, 'eval_runtime': 0.4512, 'eval_samples_per_second': 24.378, 'eval_steps_per_second': 2.216, 'epoch': 6.96, 'global_step/max_steps': '480/1035', 'percentage': '46.38%', 'elapsed_time': '12m 10s', 'remaining_time': '14m 4s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.08it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-480\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.59746652, 'acc': 0.82839737, 'grad_norm': 1.11556411, 'learning_rate': 2.965e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656666, 'epoch': 7.03, 'global_step/max_steps': '485/1035', 'percentage': '46.86%', 'elapsed_time': '12m 17s', 'remaining_time': '13m 56s'}\n",
      "{'loss': 0.5906323, 'acc': 0.82692585, 'grad_norm': 1.08093083, 'learning_rate': 2.925e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656051, 'epoch': 7.1, 'global_step/max_steps': '490/1035', 'percentage': '47.34%', 'elapsed_time': '12m 25s', 'remaining_time': '13m 49s'}\n",
      "{'loss': 0.53363976, 'acc': 0.84327211, 'grad_norm': 0.90934867, 'learning_rate': 2.886e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655602, 'epoch': 7.17, 'global_step/max_steps': '495/1035', 'percentage': '47.83%', 'elapsed_time': '12m 34s', 'remaining_time': '13m 42s'}\n",
      "{'loss': 0.54091177, 'acc': 0.84519606, 'grad_norm': 0.83169556, 'learning_rate': 2.846e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656165, 'epoch': 7.25, 'global_step/max_steps': '500/1035', 'percentage': '48.31%', 'elapsed_time': '12m 41s', 'remaining_time': '13m 34s'}\n",
      "Train:  48%|███████████████▉                 | 500/1035 [12:41<13:00,  1.46s/it]\n",
      "{'eval_loss': 0.5163393, 'eval_acc': 0.85586207, 'eval_runtime': 0.4545, 'eval_samples_per_second': 24.201, 'eval_steps_per_second': 2.2, 'epoch': 7.25, 'global_step/max_steps': '500/1035', 'percentage': '48.31%', 'elapsed_time': '12m 41s', 'remaining_time': '13m 34s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 144.11it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-500\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.54206023, 'acc': 0.84432678, 'grad_norm': 1.3986764, 'learning_rate': 2.807e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656346, 'epoch': 7.32, 'global_step/max_steps': '505/1035', 'percentage': '48.79%', 'elapsed_time': '12m 48s', 'remaining_time': '13m 26s'}\n",
      "{'loss': 0.56922426, 'acc': 0.83529911, 'grad_norm': 1.07680857, 'learning_rate': 2.767e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656933, 'epoch': 7.39, 'global_step/max_steps': '510/1035', 'percentage': '49.28%', 'elapsed_time': '12m 55s', 'remaining_time': '13m 18s'}\n",
      "{'loss': 0.55342588, 'acc': 0.83860378, 'grad_norm': 1.22017944, 'learning_rate': 2.727e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657021, 'epoch': 7.46, 'global_step/max_steps': '515/1035', 'percentage': '49.76%', 'elapsed_time': '13m 2s', 'remaining_time': '13m 10s'}\n",
      "{'loss': 0.59654899, 'acc': 0.82560101, 'grad_norm': 1.25142002, 'learning_rate': 2.688e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656134, 'epoch': 7.54, 'global_step/max_steps': '520/1035', 'percentage': '50.24%', 'elapsed_time': '13m 11s', 'remaining_time': '13m 3s'}\n",
      "Train:  50%|████████████████▌                | 520/1035 [13:11<14:46,  1.72s/it]\n",
      "{'eval_loss': 0.51362944, 'eval_acc': 0.85448276, 'eval_runtime': 0.4488, 'eval_samples_per_second': 24.511, 'eval_steps_per_second': 2.228, 'epoch': 7.54, 'global_step/max_steps': '520/1035', 'percentage': '50.24%', 'elapsed_time': '13m 12s', 'remaining_time': '13m 4s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.53it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-520\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.55173831, 'acc': 0.84146643, 'grad_norm': 0.87998885, 'learning_rate': 2.648e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655218, 'epoch': 7.61, 'global_step/max_steps': '525/1035', 'percentage': '50.72%', 'elapsed_time': '13m 20s', 'remaining_time': '12m 57s'}\n",
      "{'loss': 0.54047909, 'acc': 0.84087219, 'grad_norm': 1.15446293, 'learning_rate': 2.608e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654959, 'epoch': 7.68, 'global_step/max_steps': '530/1035', 'percentage': '51.21%', 'elapsed_time': '13m 28s', 'remaining_time': '12m 50s'}\n",
      "{'loss': 0.57718859, 'acc': 0.83839254, 'grad_norm': 0.9649564, 'learning_rate': 2.568e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655229, 'epoch': 7.75, 'global_step/max_steps': '535/1035', 'percentage': '51.69%', 'elapsed_time': '13m 35s', 'remaining_time': '12m 42s'}\n",
      "{'loss': 0.58956194, 'acc': 0.83074799, 'grad_norm': 1.04360712, 'learning_rate': 2.528e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655449, 'epoch': 7.83, 'global_step/max_steps': '540/1035', 'percentage': '52.17%', 'elapsed_time': '13m 42s', 'remaining_time': '12m 34s'}\n",
      "Train:  52%|█████████████████▏               | 540/1035 [13:42<11:51,  1.44s/it]\n",
      "{'eval_loss': 0.51434547, 'eval_acc': 0.85310345, 'eval_runtime': 0.4538, 'eval_samples_per_second': 24.242, 'eval_steps_per_second': 2.204, 'epoch': 7.83, 'global_step/max_steps': '540/1035', 'percentage': '52.17%', 'elapsed_time': '13m 43s', 'remaining_time': '12m 34s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 146.15it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-540\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.58883896, 'acc': 0.83227177, 'grad_norm': 1.0569371, 'learning_rate': 2.488e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654932, 'epoch': 7.9, 'global_step/max_steps': '545/1035', 'percentage': '52.66%', 'elapsed_time': '13m 51s', 'remaining_time': '12m 27s'}\n",
      "{'loss': 0.56456766, 'acc': 0.83650894, 'grad_norm': 1.05631697, 'learning_rate': 2.448e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655244, 'epoch': 7.97, 'global_step/max_steps': '550/1035', 'percentage': '53.14%', 'elapsed_time': '13m 58s', 'remaining_time': '12m 19s'}\n",
      "{'loss': 0.52700119, 'acc': 0.84332943, 'grad_norm': 1.12618315, 'learning_rate': 2.408e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655935, 'epoch': 8.04, 'global_step/max_steps': '555/1035', 'percentage': '53.62%', 'elapsed_time': '14m 5s', 'remaining_time': '12m 10s'}\n",
      "{'loss': 0.5328927, 'acc': 0.842348, 'grad_norm': 0.85565245, 'learning_rate': 2.368e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656184, 'epoch': 8.12, 'global_step/max_steps': '560/1035', 'percentage': '54.11%', 'elapsed_time': '14m 12s', 'remaining_time': '12m 3s'}\n",
      "Train:  54%|█████████████████▊               | 560/1035 [14:12<11:28,  1.45s/it]\n",
      "{'eval_loss': 0.51335937, 'eval_acc': 0.85310345, 'eval_runtime': 0.4515, 'eval_samples_per_second': 24.361, 'eval_steps_per_second': 2.215, 'epoch': 8.12, 'global_step/max_steps': '560/1035', 'percentage': '54.11%', 'elapsed_time': '14m 12s', 'remaining_time': '12m 3s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 144.87it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-560\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.58557825, 'acc': 0.83029938, 'grad_norm': 1.02047682, 'learning_rate': 2.328e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655368, 'epoch': 8.19, 'global_step/max_steps': '565/1035', 'percentage': '54.59%', 'elapsed_time': '14m 21s', 'remaining_time': '11m 56s'}\n",
      "{'loss': 0.5699337, 'acc': 0.83757648, 'grad_norm': 1.18348527, 'learning_rate': 2.289e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654892, 'epoch': 8.26, 'global_step/max_steps': '570/1035', 'percentage': '55.07%', 'elapsed_time': '14m 29s', 'remaining_time': '11m 49s'}\n",
      "{'loss': 0.55684667, 'acc': 0.83657093, 'grad_norm': 1.08781302, 'learning_rate': 2.249e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654877, 'epoch': 8.33, 'global_step/max_steps': '575/1035', 'percentage': '55.56%', 'elapsed_time': '14m 37s', 'remaining_time': '11m 41s'}\n",
      "{'loss': 0.54461608, 'acc': 0.84466476, 'grad_norm': 0.9270733, 'learning_rate': 2.209e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655128, 'epoch': 8.41, 'global_step/max_steps': '580/1035', 'percentage': '56.04%', 'elapsed_time': '14m 44s', 'remaining_time': '11m 33s'}\n",
      "Train:  56%|██████████████████▍              | 580/1035 [14:44<11:19,  1.49s/it]\n",
      "{'eval_loss': 0.51340812, 'eval_acc': 0.8537931, 'eval_runtime': 0.451, 'eval_samples_per_second': 24.388, 'eval_steps_per_second': 2.217, 'epoch': 8.41, 'global_step/max_steps': '580/1035', 'percentage': '56.04%', 'elapsed_time': '14m 44s', 'remaining_time': '11m 34s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 144.32it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-580\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.53699045, 'acc': 0.84605455, 'grad_norm': 1.01469064, 'learning_rate': 2.169e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654909, 'epoch': 8.48, 'global_step/max_steps': '585/1035', 'percentage': '56.52%', 'elapsed_time': '14m 52s', 'remaining_time': '11m 26s'}\n",
      "{'loss': 0.56159024, 'acc': 0.83782825, 'grad_norm': 0.91646695, 'learning_rate': 2.13e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654474, 'epoch': 8.55, 'global_step/max_steps': '590/1035', 'percentage': '57.00%', 'elapsed_time': '15m 0s', 'remaining_time': '11m 19s'}\n",
      "{'loss': 0.54034891, 'acc': 0.84272032, 'grad_norm': 1.1730063, 'learning_rate': 2.09e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654539, 'epoch': 8.62, 'global_step/max_steps': '595/1035', 'percentage': '57.49%', 'elapsed_time': '15m 8s', 'remaining_time': '11m 11s'}\n",
      "{'loss': 0.54610848, 'acc': 0.84316092, 'grad_norm': 1.2963928, 'learning_rate': 2.051e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654925, 'epoch': 8.7, 'global_step/max_steps': '600/1035', 'percentage': '57.97%', 'elapsed_time': '15m 15s', 'remaining_time': '11m 3s'}\n",
      "Train:  58%|███████████████████▏             | 600/1035 [15:15<10:36,  1.46s/it]\n",
      "{'eval_loss': 0.51445019, 'eval_acc': 0.85448276, 'eval_runtime': 0.4556, 'eval_samples_per_second': 24.146, 'eval_steps_per_second': 2.195, 'epoch': 8.7, 'global_step/max_steps': '600/1035', 'percentage': '57.97%', 'elapsed_time': '15m 15s', 'remaining_time': '11m 3s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.34it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-600\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.51339006, 'acc': 0.85004778, 'grad_norm': 1.06473875, 'learning_rate': 2.012e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654574, 'epoch': 8.77, 'global_step/max_steps': '605/1035', 'percentage': '58.45%', 'elapsed_time': '15m 23s', 'remaining_time': '10m 56s'}\n",
      "{'loss': 0.59937587, 'acc': 0.82782974, 'grad_norm': 1.19341874, 'learning_rate': 1.973e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654802, 'epoch': 8.84, 'global_step/max_steps': '610/1035', 'percentage': '58.94%', 'elapsed_time': '15m 30s', 'remaining_time': '10m 48s'}\n",
      "{'loss': 0.56946135, 'acc': 0.83648376, 'grad_norm': 1.2007705, 'learning_rate': 1.934e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655308, 'epoch': 8.91, 'global_step/max_steps': '615/1035', 'percentage': '59.42%', 'elapsed_time': '15m 37s', 'remaining_time': '10m 40s'}\n",
      "{'loss': 0.53035288, 'acc': 0.84699364, 'grad_norm': 1.27914977, 'learning_rate': 1.895e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656291, 'epoch': 8.99, 'global_step/max_steps': '620/1035', 'percentage': '59.90%', 'elapsed_time': '15m 43s', 'remaining_time': '10m 31s'}\n",
      "Train:  60%|███████████████████▊             | 620/1035 [15:43<08:53,  1.29s/it]\n",
      "{'eval_loss': 0.51402009, 'eval_acc': 0.85448276, 'eval_runtime': 0.4548, 'eval_samples_per_second': 24.185, 'eval_steps_per_second': 2.199, 'epoch': 8.99, 'global_step/max_steps': '620/1035', 'percentage': '59.90%', 'elapsed_time': '15m 44s', 'remaining_time': '10m 31s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.79it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-620\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.50793157, 'acc': 0.85407352, 'grad_norm': 1.22278464, 'learning_rate': 1.856e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656822, 'epoch': 9.06, 'global_step/max_steps': '625/1035', 'percentage': '60.39%', 'elapsed_time': '15m 50s', 'remaining_time': '10m 23s'}\n",
      "{'loss': 0.51195803, 'acc': 0.8456768, 'grad_norm': 1.20452487, 'learning_rate': 1.818e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657318, 'epoch': 9.13, 'global_step/max_steps': '630/1035', 'percentage': '60.87%', 'elapsed_time': '15m 57s', 'remaining_time': '10m 15s'}\n",
      "{'loss': 0.52914181, 'acc': 0.85376883, 'grad_norm': 1.1592778, 'learning_rate': 1.779e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657415, 'epoch': 9.2, 'global_step/max_steps': '635/1035', 'percentage': '61.35%', 'elapsed_time': '16m 4s', 'remaining_time': '10m 7s'}\n",
      "{'loss': 0.52409716, 'acc': 0.84823551, 'grad_norm': 1.03171098, 'learning_rate': 1.741e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657473, 'epoch': 9.28, 'global_step/max_steps': '640/1035', 'percentage': '61.84%', 'elapsed_time': '16m 12s', 'remaining_time': '10m 0s'}\n",
      "Train:  62%|████████████████████▍            | 640/1035 [16:12<09:37,  1.46s/it]\n",
      "{'eval_loss': 0.51382869, 'eval_acc': 0.8537931, 'eval_runtime': 0.4529, 'eval_samples_per_second': 24.287, 'eval_steps_per_second': 2.208, 'epoch': 9.28, 'global_step/max_steps': '640/1035', 'percentage': '61.84%', 'elapsed_time': '16m 12s', 'remaining_time': '10m 0s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 144.54it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-640\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.55779781, 'acc': 0.84159937, 'grad_norm': 1.25316548, 'learning_rate': 1.703e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657618, 'epoch': 9.35, 'global_step/max_steps': '645/1035', 'percentage': '62.32%', 'elapsed_time': '16m 19s', 'remaining_time': '9m 52s'}\n",
      "{'loss': 0.51317401, 'acc': 0.84727974, 'grad_norm': 1.29727685, 'learning_rate': 1.665e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.658253, 'epoch': 9.42, 'global_step/max_steps': '650/1035', 'percentage': '62.80%', 'elapsed_time': '16m 26s', 'remaining_time': '9m 44s'}\n",
      "{'loss': 0.56248322, 'acc': 0.83784008, 'grad_norm': 1.1685189, 'learning_rate': 1.628e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657928, 'epoch': 9.49, 'global_step/max_steps': '655/1035', 'percentage': '63.29%', 'elapsed_time': '16m 34s', 'remaining_time': '9m 37s'}\n",
      "{'loss': 0.58700433, 'acc': 0.82978468, 'grad_norm': 1.15475559, 'learning_rate': 1.591e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657362, 'epoch': 9.57, 'global_step/max_steps': '660/1035', 'percentage': '63.77%', 'elapsed_time': '16m 43s', 'remaining_time': '9m 29s'}\n",
      "Train:  64%|█████████████████████            | 660/1035 [16:43<10:29,  1.68s/it]\n",
      "{'eval_loss': 0.51262355, 'eval_acc': 0.8537931, 'eval_runtime': 0.4497, 'eval_samples_per_second': 24.462, 'eval_steps_per_second': 2.224, 'epoch': 9.57, 'global_step/max_steps': '660/1035', 'percentage': '63.77%', 'elapsed_time': '16m 43s', 'remaining_time': '9m 30s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 146.24it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-660\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.53972578, 'acc': 0.84544506, 'grad_norm': 1.1779412, 'learning_rate': 1.553e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.657171, 'epoch': 9.64, 'global_step/max_steps': '665/1035', 'percentage': '64.25%', 'elapsed_time': '16m 50s', 'remaining_time': '9m 22s'}\n",
      "{'loss': 0.52896008, 'acc': 0.845648, 'grad_norm': 1.06962931, 'learning_rate': 1.517e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656519, 'epoch': 9.71, 'global_step/max_steps': '670/1035', 'percentage': '64.73%', 'elapsed_time': '16m 59s', 'remaining_time': '9m 15s'}\n",
      "{'loss': 0.56033726, 'acc': 0.83890953, 'grad_norm': 1.26699364, 'learning_rate': 1.48e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655938, 'epoch': 9.78, 'global_step/max_steps': '675/1035', 'percentage': '65.22%', 'elapsed_time': '17m 8s', 'remaining_time': '9m 8s'}\n",
      "{'loss': 0.56943836, 'acc': 0.83087378, 'grad_norm': 1.09688354, 'learning_rate': 1.444e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655783, 'epoch': 9.86, 'global_step/max_steps': '680/1035', 'percentage': '65.70%', 'elapsed_time': '17m 15s', 'remaining_time': '9m 0s'}\n",
      "Train:  66%|█████████████████████▋           | 680/1035 [17:15<09:28,  1.60s/it]\n",
      "{'eval_loss': 0.51286948, 'eval_acc': 0.85517241, 'eval_runtime': 0.4561, 'eval_samples_per_second': 24.117, 'eval_steps_per_second': 2.192, 'epoch': 9.86, 'global_step/max_steps': '680/1035', 'percentage': '65.70%', 'elapsed_time': '17m 16s', 'remaining_time': '9m 1s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 140.26it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-680\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.57904978, 'acc': 0.83140078, 'grad_norm': 1.09272552, 'learning_rate': 1.408e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655536, 'epoch': 9.93, 'global_step/max_steps': '685/1035', 'percentage': '66.18%', 'elapsed_time': '17m 23s', 'remaining_time': '8m 53s'}\n",
      "{'loss': 0.55704603, 'acc': 0.83544559, 'grad_norm': 1.19908619, 'learning_rate': 1.372e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655922, 'epoch': 10.0, 'global_step/max_steps': '690/1035', 'percentage': '66.67%', 'elapsed_time': '17m 30s', 'remaining_time': '8m 45s'}\n",
      "{'loss': 0.59768744, 'acc': 0.82418756, 'grad_norm': 1.43379974, 'learning_rate': 1.336e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656002, 'epoch': 10.07, 'global_step/max_steps': '695/1035', 'percentage': '67.15%', 'elapsed_time': '17m 38s', 'remaining_time': '8m 37s'}\n",
      "{'loss': 0.55584173, 'acc': 0.84096689, 'grad_norm': 1.14325416, 'learning_rate': 1.301e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655647, 'epoch': 10.14, 'global_step/max_steps': '700/1035', 'percentage': '67.63%', 'elapsed_time': '17m 46s', 'remaining_time': '8m 30s'}\n",
      "Train:  68%|██████████████████████▎          | 700/1035 [17:46<09:33,  1.71s/it]\n",
      "{'eval_loss': 0.51367831, 'eval_acc': 0.85448276, 'eval_runtime': 0.4544, 'eval_samples_per_second': 24.209, 'eval_steps_per_second': 2.201, 'epoch': 10.14, 'global_step/max_steps': '700/1035', 'percentage': '67.63%', 'elapsed_time': '17m 47s', 'remaining_time': '8m 30s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 144.75it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-700\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.57033362, 'acc': 0.83425627, 'grad_norm': 1.11843467, 'learning_rate': 1.266e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654963, 'epoch': 10.22, 'global_step/max_steps': '705/1035', 'percentage': '68.12%', 'elapsed_time': '17m 55s', 'remaining_time': '8m 23s'}\n",
      "{'loss': 0.49696255, 'acc': 0.85356655, 'grad_norm': 1.30474174, 'learning_rate': 1.232e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655852, 'epoch': 10.29, 'global_step/max_steps': '710/1035', 'percentage': '68.60%', 'elapsed_time': '18m 1s', 'remaining_time': '8m 15s'}\n",
      "{'loss': 0.55671005, 'acc': 0.83684416, 'grad_norm': 1.11440325, 'learning_rate': 1.197e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656365, 'epoch': 10.36, 'global_step/max_steps': '715/1035', 'percentage': '69.08%', 'elapsed_time': '18m 8s', 'remaining_time': '8m 7s'}\n",
      "{'loss': 0.55991564, 'acc': 0.83861561, 'grad_norm': 1.04920101, 'learning_rate': 1.163e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656619, 'epoch': 10.43, 'global_step/max_steps': '720/1035', 'percentage': '69.57%', 'elapsed_time': '18m 15s', 'remaining_time': '7m 59s'}\n",
      "Train:  70%|██████████████████████▉          | 720/1035 [18:15<07:29,  1.43s/it]\n",
      "{'eval_loss': 0.51259494, 'eval_acc': 0.85034483, 'eval_runtime': 0.4526, 'eval_samples_per_second': 24.303, 'eval_steps_per_second': 2.209, 'epoch': 10.43, 'global_step/max_steps': '720/1035', 'percentage': '69.57%', 'elapsed_time': '18m 16s', 'remaining_time': '7m 59s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.18it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-720\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.56184373, 'acc': 0.83898859, 'grad_norm': 1.05738139, 'learning_rate': 1.13e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656205, 'epoch': 10.51, 'global_step/max_steps': '725/1035', 'percentage': '70.05%', 'elapsed_time': '18m 23s', 'remaining_time': '7m 51s'}\n",
      "{'loss': 0.52988348, 'acc': 0.8421772, 'grad_norm': 1.15066803, 'learning_rate': 1.097e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656322, 'epoch': 10.58, 'global_step/max_steps': '730/1035', 'percentage': '70.53%', 'elapsed_time': '18m 31s', 'remaining_time': '7m 44s'}\n",
      "{'loss': 0.53862982, 'acc': 0.84607601, 'grad_norm': 1.12512958, 'learning_rate': 1.064e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656289, 'epoch': 10.65, 'global_step/max_steps': '735/1035', 'percentage': '71.01%', 'elapsed_time': '18m 38s', 'remaining_time': '7m 36s'}\n",
      "{'loss': 0.4658092, 'acc': 0.86212349, 'grad_norm': 1.36325121, 'learning_rate': 1.031e-05, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.65602, 'epoch': 10.72, 'global_step/max_steps': '740/1035', 'percentage': '71.50%', 'elapsed_time': '18m 47s', 'remaining_time': '7m 29s'}\n",
      "Train:  71%|███████████████████████▌         | 740/1035 [18:47<07:32,  1.54s/it]\n",
      "{'eval_loss': 0.51409173, 'eval_acc': 0.8537931, 'eval_runtime': 0.4508, 'eval_samples_per_second': 24.403, 'eval_steps_per_second': 2.218, 'epoch': 10.72, 'global_step/max_steps': '740/1035', 'percentage': '71.50%', 'elapsed_time': '18m 47s', 'remaining_time': '7m 29s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.50it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-740\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.5424427, 'acc': 0.84543228, 'grad_norm': 1.18129861, 'learning_rate': 9.99e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655378, 'epoch': 10.8, 'global_step/max_steps': '745/1035', 'percentage': '71.98%', 'elapsed_time': '18m 55s', 'remaining_time': '7m 22s'}\n",
      "{'loss': 0.56479859, 'acc': 0.83827276, 'grad_norm': 1.24205291, 'learning_rate': 9.67e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655283, 'epoch': 10.87, 'global_step/max_steps': '750/1035', 'percentage': '72.46%', 'elapsed_time': '19m 3s', 'remaining_time': '7m 14s'}\n",
      "{'loss': 0.49709997, 'acc': 0.85315247, 'grad_norm': 1.11855531, 'learning_rate': 9.36e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655895, 'epoch': 10.94, 'global_step/max_steps': '755/1035', 'percentage': '72.95%', 'elapsed_time': '19m 10s', 'remaining_time': '7m 6s'}\n",
      "{'loss': 0.53599768, 'acc': 0.84348211, 'grad_norm': 1.11792254, 'learning_rate': 9.05e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.65575, 'epoch': 11.01, 'global_step/max_steps': '760/1035', 'percentage': '73.43%', 'elapsed_time': '19m 18s', 'remaining_time': '6m 59s'}\n",
      "Train:  73%|████████████████████████▏        | 760/1035 [19:18<07:15,  1.58s/it]\n",
      "{'eval_loss': 0.51296431, 'eval_acc': 0.85310345, 'eval_runtime': 0.4525, 'eval_samples_per_second': 24.312, 'eval_steps_per_second': 2.21, 'epoch': 11.01, 'global_step/max_steps': '760/1035', 'percentage': '73.43%', 'elapsed_time': '19m 18s', 'remaining_time': '6m 59s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 144.10it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-760\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.53942256, 'acc': 0.84139376, 'grad_norm': 1.09719431, 'learning_rate': 8.74e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655509, 'epoch': 11.09, 'global_step/max_steps': '765/1035', 'percentage': '73.91%', 'elapsed_time': '19m 26s', 'remaining_time': '6m 51s'}\n",
      "{'loss': 0.52558861, 'acc': 0.84579468, 'grad_norm': 1.10050118, 'learning_rate': 8.44e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655296, 'epoch': 11.16, 'global_step/max_steps': '770/1035', 'percentage': '74.40%', 'elapsed_time': '19m 34s', 'remaining_time': '6m 44s'}\n",
      "{'loss': 0.53017969, 'acc': 0.84576454, 'grad_norm': 1.01860654, 'learning_rate': 8.15e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655247, 'epoch': 11.23, 'global_step/max_steps': '775/1035', 'percentage': '74.88%', 'elapsed_time': '19m 41s', 'remaining_time': '6m 36s'}\n",
      "{'loss': 0.51598263, 'acc': 0.85049629, 'grad_norm': 1.14020753, 'learning_rate': 7.85e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655542, 'epoch': 11.3, 'global_step/max_steps': '780/1035', 'percentage': '75.36%', 'elapsed_time': '19m 48s', 'remaining_time': '6m 28s'}\n",
      "Train:  75%|████████████████████████▊        | 780/1035 [19:48<06:25,  1.51s/it]\n",
      "{'eval_loss': 0.51303411, 'eval_acc': 0.85310345, 'eval_runtime': 0.456, 'eval_samples_per_second': 24.124, 'eval_steps_per_second': 2.193, 'epoch': 11.3, 'global_step/max_steps': '780/1035', 'percentage': '75.36%', 'elapsed_time': '19m 49s', 'remaining_time': '6m 28s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 146.77it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-780\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.56911435, 'acc': 0.83783836, 'grad_norm': 1.15498638, 'learning_rate': 7.56e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655207, 'epoch': 11.38, 'global_step/max_steps': '785/1035', 'percentage': '75.85%', 'elapsed_time': '19m 57s', 'remaining_time': '6m 21s'}\n",
      "{'loss': 0.51316695, 'acc': 0.85005684, 'grad_norm': 1.34304619, 'learning_rate': 7.28e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654936, 'epoch': 11.45, 'global_step/max_steps': '790/1035', 'percentage': '76.33%', 'elapsed_time': '20m 5s', 'remaining_time': '6m 13s'}\n",
      "{'loss': 0.55665445, 'acc': 0.83370161, 'grad_norm': 1.16449606, 'learning_rate': 7e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654749, 'epoch': 11.52, 'global_step/max_steps': '795/1035', 'percentage': '76.81%', 'elapsed_time': '20m 13s', 'remaining_time': '6m 6s'}\n",
      "{'loss': 0.53377628, 'acc': 0.84541941, 'grad_norm': 1.2095325, 'learning_rate': 6.73e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654908, 'epoch': 11.59, 'global_step/max_steps': '800/1035', 'percentage': '77.29%', 'elapsed_time': '20m 20s', 'remaining_time': '5m 58s'}\n",
      "Train:  77%|█████████████████████████▌       | 800/1035 [20:20<05:41,  1.45s/it]\n",
      "{'eval_loss': 0.51405704, 'eval_acc': 0.85310345, 'eval_runtime': 0.4509, 'eval_samples_per_second': 24.396, 'eval_steps_per_second': 2.218, 'epoch': 11.59, 'global_step/max_steps': '800/1035', 'percentage': '77.29%', 'elapsed_time': '20m 21s', 'remaining_time': '5m 58s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 147.14it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-800\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.55325751, 'acc': 0.83823652, 'grad_norm': 1.39603412, 'learning_rate': 6.46e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654538, 'epoch': 11.67, 'global_step/max_steps': '805/1035', 'percentage': '77.78%', 'elapsed_time': '20m 28s', 'remaining_time': '5m 51s'}\n",
      "{'loss': 0.5845067, 'acc': 0.83458824, 'grad_norm': 1.31222355, 'learning_rate': 6.19e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654517, 'epoch': 11.74, 'global_step/max_steps': '810/1035', 'percentage': '78.26%', 'elapsed_time': '20m 36s', 'remaining_time': '5m 43s'}\n",
      "{'loss': 0.48393688, 'acc': 0.85353899, 'grad_norm': 0.98668754, 'learning_rate': 5.93e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654806, 'epoch': 11.81, 'global_step/max_steps': '815/1035', 'percentage': '78.74%', 'elapsed_time': '20m 43s', 'remaining_time': '5m 35s'}\n",
      "{'loss': 0.52537365, 'acc': 0.84628134, 'grad_norm': 1.2556119, 'learning_rate': 5.67e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655438, 'epoch': 11.88, 'global_step/max_steps': '820/1035', 'percentage': '79.23%', 'elapsed_time': '20m 50s', 'remaining_time': '5m 27s'}\n",
      "Train:  79%|██████████████████████████▏      | 820/1035 [20:50<04:41,  1.31s/it]\n",
      "{'eval_loss': 0.51470089, 'eval_acc': 0.8537931, 'eval_runtime': 0.4538, 'eval_samples_per_second': 24.24, 'eval_steps_per_second': 2.204, 'epoch': 11.88, 'global_step/max_steps': '820/1035', 'percentage': '79.23%', 'elapsed_time': '20m 50s', 'remaining_time': '5m 27s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 146.17it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-820\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.54362631, 'acc': 0.84339705, 'grad_norm': 1.59139657, 'learning_rate': 5.42e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655253, 'epoch': 11.96, 'global_step/max_steps': '825/1035', 'percentage': '79.71%', 'elapsed_time': '20m 58s', 'remaining_time': '5m 20s'}\n",
      "{'loss': 0.55417795, 'acc': 0.83861179, 'grad_norm': 1.21430922, 'learning_rate': 5.18e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655404, 'epoch': 12.03, 'global_step/max_steps': '830/1035', 'percentage': '80.19%', 'elapsed_time': '21m 5s', 'remaining_time': '5m 12s'}\n",
      "{'loss': 0.54269209, 'acc': 0.84376316, 'grad_norm': 1.06344879, 'learning_rate': 4.94e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.65541, 'epoch': 12.1, 'global_step/max_steps': '835/1035', 'percentage': '80.68%', 'elapsed_time': '21m 13s', 'remaining_time': '5m 4s'}\n",
      "{'loss': 0.53317761, 'acc': 0.83949146, 'grad_norm': 1.29001844, 'learning_rate': 4.7e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655218, 'epoch': 12.17, 'global_step/max_steps': '840/1035', 'percentage': '81.16%', 'elapsed_time': '21m 21s', 'remaining_time': '4m 57s'}\n",
      "Train:  81%|██████████████████████████▊      | 840/1035 [21:21<05:10,  1.59s/it]\n",
      "{'eval_loss': 0.51387125, 'eval_acc': 0.85103448, 'eval_runtime': 0.4602, 'eval_samples_per_second': 23.902, 'eval_steps_per_second': 2.173, 'epoch': 12.17, 'global_step/max_steps': '840/1035', 'percentage': '81.16%', 'elapsed_time': '21m 21s', 'remaining_time': '4m 57s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.22it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-840\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.55020485, 'acc': 0.84319286, 'grad_norm': 1.25467479, 'learning_rate': 4.47e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654424, 'epoch': 12.25, 'global_step/max_steps': '845/1035', 'percentage': '81.64%', 'elapsed_time': '21m 30s', 'remaining_time': '4m 50s'}\n",
      "{'loss': 0.51634369, 'acc': 0.85237026, 'grad_norm': 1.34099627, 'learning_rate': 4.24e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654493, 'epoch': 12.32, 'global_step/max_steps': '850/1035', 'percentage': '82.13%', 'elapsed_time': '21m 37s', 'remaining_time': '4m 42s'}\n",
      "{'loss': 0.53978181, 'acc': 0.84201002, 'grad_norm': 1.14336658, 'learning_rate': 4.02e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654658, 'epoch': 12.39, 'global_step/max_steps': '855/1035', 'percentage': '82.61%', 'elapsed_time': '21m 45s', 'remaining_time': '4m 34s'}\n",
      "{'loss': 0.55079875, 'acc': 0.83861504, 'grad_norm': 1.00793123, 'learning_rate': 3.81e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654324, 'epoch': 12.46, 'global_step/max_steps': '860/1035', 'percentage': '83.09%', 'elapsed_time': '21m 53s', 'remaining_time': '4m 27s'}\n",
      "Train:  83%|███████████████████████████▍     | 860/1035 [21:53<04:58,  1.70s/it]\n",
      "{'eval_loss': 0.51310772, 'eval_acc': 0.85172414, 'eval_runtime': 0.4475, 'eval_samples_per_second': 24.579, 'eval_steps_per_second': 2.234, 'epoch': 12.46, 'global_step/max_steps': '860/1035', 'percentage': '83.09%', 'elapsed_time': '21m 53s', 'remaining_time': '4m 27s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 144.62it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-860\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.52045212, 'acc': 0.84541168, 'grad_norm': 1.33398211, 'learning_rate': 3.6e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.653885, 'epoch': 12.54, 'global_step/max_steps': '865/1035', 'percentage': '83.57%', 'elapsed_time': '22m 1s', 'remaining_time': '4m 19s'}\n",
      "{'loss': 0.53069787, 'acc': 0.84709129, 'grad_norm': 1.20415699, 'learning_rate': 3.4e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654076, 'epoch': 12.61, 'global_step/max_steps': '870/1035', 'percentage': '84.06%', 'elapsed_time': '22m 9s', 'remaining_time': '4m 12s'}\n",
      "{'loss': 0.55530891, 'acc': 0.84181614, 'grad_norm': 1.17799926, 'learning_rate': 3.2e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654848, 'epoch': 12.68, 'global_step/max_steps': '875/1035', 'percentage': '84.54%', 'elapsed_time': '22m 15s', 'remaining_time': '4m 4s'}\n",
      "{'loss': 0.54576225, 'acc': 0.84085026, 'grad_norm': 1.06064045, 'learning_rate': 3.01e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655021, 'epoch': 12.75, 'global_step/max_steps': '880/1035', 'percentage': '85.02%', 'elapsed_time': '22m 22s', 'remaining_time': '3m 56s'}\n",
      "Train:  85%|████████████████████████████     | 880/1035 [22:22<03:57,  1.53s/it]\n",
      "{'eval_loss': 0.51261169, 'eval_acc': 0.85241379, 'eval_runtime': 0.4531, 'eval_samples_per_second': 24.276, 'eval_steps_per_second': 2.207, 'epoch': 12.75, 'global_step/max_steps': '880/1035', 'percentage': '85.02%', 'elapsed_time': '22m 22s', 'remaining_time': '3m 56s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 146.60it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-880\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.52107649, 'acc': 0.85165873, 'grad_norm': 1.26522017, 'learning_rate': 2.82e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655143, 'epoch': 12.83, 'global_step/max_steps': '885/1035', 'percentage': '85.51%', 'elapsed_time': '22m 29s', 'remaining_time': '3m 48s'}\n",
      "{'loss': 0.54177351, 'acc': 0.84004049, 'grad_norm': 1.14034188, 'learning_rate': 2.64e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655588, 'epoch': 12.9, 'global_step/max_steps': '890/1035', 'percentage': '85.99%', 'elapsed_time': '22m 36s', 'remaining_time': '3m 41s'}\n",
      "{'loss': 0.5197176, 'acc': 0.84645519, 'grad_norm': 1.15664291, 'learning_rate': 2.46e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655389, 'epoch': 12.97, 'global_step/max_steps': '895/1035', 'percentage': '86.47%', 'elapsed_time': '22m 44s', 'remaining_time': '3m 33s'}\n",
      "{'loss': 0.5258801, 'acc': 0.84409876, 'grad_norm': 1.37104607, 'learning_rate': 2.29e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655652, 'epoch': 13.04, 'global_step/max_steps': '900/1035', 'percentage': '86.96%', 'elapsed_time': '22m 51s', 'remaining_time': '3m 25s'}\n",
      "Train:  87%|████████████████████████████▋    | 900/1035 [22:51<03:20,  1.48s/it]\n",
      "{'eval_loss': 0.51321113, 'eval_acc': 0.85241379, 'eval_runtime': 0.4558, 'eval_samples_per_second': 24.134, 'eval_steps_per_second': 2.194, 'epoch': 13.04, 'global_step/max_steps': '900/1035', 'percentage': '86.96%', 'elapsed_time': '22m 52s', 'remaining_time': '3m 25s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 143.86it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-900\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.62579184, 'acc': 0.8236022, 'grad_norm': 1.13337755, 'learning_rate': 2.13e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655297, 'epoch': 13.12, 'global_step/max_steps': '905/1035', 'percentage': '87.44%', 'elapsed_time': '23m 0s', 'remaining_time': '3m 18s'}\n",
      "{'loss': 0.52535124, 'acc': 0.845366, 'grad_norm': 1.33701658, 'learning_rate': 1.97e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655607, 'epoch': 13.19, 'global_step/max_steps': '910/1035', 'percentage': '87.92%', 'elapsed_time': '23m 7s', 'remaining_time': '3m 10s'}\n",
      "{'loss': 0.52112007, 'acc': 0.84238329, 'grad_norm': 1.47282851, 'learning_rate': 1.82e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.65544, 'epoch': 13.26, 'global_step/max_steps': '915/1035', 'percentage': '88.41%', 'elapsed_time': '23m 15s', 'remaining_time': '3m 2s'}\n",
      "{'loss': 0.52552133, 'acc': 0.84670935, 'grad_norm': 1.18366683, 'learning_rate': 1.67e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.65491, 'epoch': 13.33, 'global_step/max_steps': '920/1035', 'percentage': '88.89%', 'elapsed_time': '23m 23s', 'remaining_time': '2m 55s'}\n",
      "Train:  89%|█████████████████████████████▎   | 920/1035 [23:23<03:10,  1.66s/it]\n",
      "{'eval_loss': 0.51266164, 'eval_acc': 0.85655172, 'eval_runtime': 0.4502, 'eval_samples_per_second': 24.434, 'eval_steps_per_second': 2.221, 'epoch': 13.33, 'global_step/max_steps': '920/1035', 'percentage': '88.89%', 'elapsed_time': '23m 24s', 'remaining_time': '2m 55s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 146.56it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-920\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.51711159, 'acc': 0.85023556, 'grad_norm': 1.11863482, 'learning_rate': 1.53e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.654819, 'epoch': 13.41, 'global_step/max_steps': '925/1035', 'percentage': '89.37%', 'elapsed_time': '23m 31s', 'remaining_time': '2m 47s'}\n",
      "{'loss': 0.49234681, 'acc': 0.8534606, 'grad_norm': 1.50232649, 'learning_rate': 1.39e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655415, 'epoch': 13.48, 'global_step/max_steps': '930/1035', 'percentage': '89.86%', 'elapsed_time': '23m 37s', 'remaining_time': '2m 40s'}\n",
      "{'loss': 0.51263628, 'acc': 0.85450993, 'grad_norm': 1.22711158, 'learning_rate': 1.27e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655534, 'epoch': 13.55, 'global_step/max_steps': '935/1035', 'percentage': '90.34%', 'elapsed_time': '23m 45s', 'remaining_time': '2m 32s'}\n",
      "{'loss': 0.60938549, 'acc': 0.82757759, 'grad_norm': 1.24416482, 'learning_rate': 1.14e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655368, 'epoch': 13.62, 'global_step/max_steps': '940/1035', 'percentage': '90.82%', 'elapsed_time': '23m 53s', 'remaining_time': '2m 24s'}\n",
      "Train:  91%|█████████████████████████████▉   | 940/1035 [23:53<02:24,  1.52s/it]\n",
      "{'eval_loss': 0.51350605, 'eval_acc': 0.85448276, 'eval_runtime': 0.4543, 'eval_samples_per_second': 24.212, 'eval_steps_per_second': 2.201, 'epoch': 13.62, 'global_step/max_steps': '940/1035', 'percentage': '90.82%', 'elapsed_time': '23m 53s', 'remaining_time': '2m 24s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.06it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-940\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.5205739, 'acc': 0.84940777, 'grad_norm': 1.28027737, 'learning_rate': 1.03e-06, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.65539, 'epoch': 13.7, 'global_step/max_steps': '945/1035', 'percentage': '91.30%', 'elapsed_time': '24m 0s', 'remaining_time': '2m 17s'}\n",
      "{'loss': 0.5190526, 'acc': 0.84639215, 'grad_norm': 1.13237751, 'learning_rate': 9.2e-07, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656004, 'epoch': 13.77, 'global_step/max_steps': '950/1035', 'percentage': '91.79%', 'elapsed_time': '24m 7s', 'remaining_time': '2m 9s'}\n",
      "{'loss': 0.52631216, 'acc': 0.84697809, 'grad_norm': 1.1313858, 'learning_rate': 8.1e-07, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656294, 'epoch': 13.84, 'global_step/max_steps': '955/1035', 'percentage': '92.27%', 'elapsed_time': '24m 14s', 'remaining_time': '2m 1s'}\n",
      "{'loss': 0.53179665, 'acc': 0.8460804, 'grad_norm': 1.18405485, 'learning_rate': 7.1e-07, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.65616, 'epoch': 13.91, 'global_step/max_steps': '960/1035', 'percentage': '92.75%', 'elapsed_time': '24m 22s', 'remaining_time': '1m 54s'}\n",
      "Train:  93%|██████████████████████████████▌  | 960/1035 [24:22<02:05,  1.67s/it]\n",
      "{'eval_loss': 0.51216614, 'eval_acc': 0.85448276, 'eval_runtime': 0.4488, 'eval_samples_per_second': 24.509, 'eval_steps_per_second': 2.228, 'epoch': 13.91, 'global_step/max_steps': '960/1035', 'percentage': '92.75%', 'elapsed_time': '24m 22s', 'remaining_time': '1m 54s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 144.25it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-960\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.46936493, 'acc': 0.86053333, 'grad_norm': 1.44697249, 'learning_rate': 6.2e-07, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656253, 'epoch': 13.99, 'global_step/max_steps': '965/1035', 'percentage': '93.24%', 'elapsed_time': '24m 29s', 'remaining_time': '1m 46s'}\n",
      "{'loss': 0.55587192, 'acc': 0.84167566, 'grad_norm': 1.2429024, 'learning_rate': 5.4e-07, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655848, 'epoch': 14.06, 'global_step/max_steps': '970/1035', 'percentage': '93.72%', 'elapsed_time': '24m 38s', 'remaining_time': '1m 39s'}\n",
      "{'loss': 0.5543189, 'acc': 0.84280443, 'grad_norm': 1.55734515, 'learning_rate': 4.6e-07, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656164, 'epoch': 14.13, 'global_step/max_steps': '975/1035', 'percentage': '94.20%', 'elapsed_time': '24m 44s', 'remaining_time': '1m 31s'}\n",
      "{'loss': 0.518256, 'acc': 0.84625101, 'grad_norm': 1.08921361, 'learning_rate': 3.9e-07, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656082, 'epoch': 14.2, 'global_step/max_steps': '980/1035', 'percentage': '94.69%', 'elapsed_time': '24m 52s', 'remaining_time': '1m 23s'}\n",
      "Train:  95%|███████████████████████████████▏ | 980/1035 [24:52<01:21,  1.48s/it]\n",
      "{'eval_loss': 0.51386458, 'eval_acc': 0.8537931, 'eval_runtime': 0.4564, 'eval_samples_per_second': 24.104, 'eval_steps_per_second': 2.191, 'epoch': 14.2, 'global_step/max_steps': '980/1035', 'percentage': '94.69%', 'elapsed_time': '24m 53s', 'remaining_time': '1m 23s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.00it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-980\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.51457686, 'acc': 0.85054331, 'grad_norm': 1.17539883, 'learning_rate': 3.2e-07, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656141, 'epoch': 14.28, 'global_step/max_steps': '985/1035', 'percentage': '95.17%', 'elapsed_time': '25m 0s', 'remaining_time': '1m 16s'}\n",
      "{'loss': 0.51051121, 'acc': 0.85167933, 'grad_norm': 1.2805171, 'learning_rate': 2.6e-07, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656404, 'epoch': 14.35, 'global_step/max_steps': '990/1035', 'percentage': '95.65%', 'elapsed_time': '25m 7s', 'remaining_time': '1m 8s'}\n",
      "{'loss': 0.5199441, 'acc': 0.84533472, 'grad_norm': 1.07947016, 'learning_rate': 2e-07, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656332, 'epoch': 14.42, 'global_step/max_steps': '995/1035', 'percentage': '96.14%', 'elapsed_time': '25m 15s', 'remaining_time': '1m 0s'}\n",
      "{'loss': 0.55545397, 'acc': 0.83884745, 'grad_norm': 1.19638526, 'learning_rate': 1.6e-07, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.656284, 'epoch': 14.49, 'global_step/max_steps': '1000/1035', 'percentage': '96.62%', 'elapsed_time': '25m 22s', 'remaining_time': '53s'}\n",
      "Train:  97%|██████████████████████████████▉ | 1000/1035 [25:22<00:56,  1.62s/it]\n",
      "{'eval_loss': 0.513026, 'eval_acc': 0.85172414, 'eval_runtime': 0.4563, 'eval_samples_per_second': 24.105, 'eval_steps_per_second': 2.191, 'epoch': 14.49, 'global_step/max_steps': '1000/1035', 'percentage': '96.62%', 'elapsed_time': '25m 23s', 'remaining_time': '53s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 145.81it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-1000\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.57477231, 'acc': 0.83644609, 'grad_norm': 1.03418314, 'learning_rate': 1.1e-07, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655879, 'epoch': 14.57, 'global_step/max_steps': '1005/1035', 'percentage': '97.10%', 'elapsed_time': '25m 31s', 'remaining_time': '45s'}\n",
      "{'loss': 0.53576908, 'acc': 0.84214163, 'grad_norm': 1.30924058, 'learning_rate': 8e-08, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655854, 'epoch': 14.64, 'global_step/max_steps': '1010/1035', 'percentage': '97.58%', 'elapsed_time': '25m 39s', 'remaining_time': '38s'}\n",
      "{'loss': 0.50422502, 'acc': 0.85026159, 'grad_norm': 1.01150954, 'learning_rate': 5e-08, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655699, 'epoch': 14.71, 'global_step/max_steps': '1015/1035', 'percentage': '98.07%', 'elapsed_time': '25m 46s', 'remaining_time': '30s'}\n",
      "{'loss': 0.53827472, 'acc': 0.84216537, 'grad_norm': 1.4158088, 'learning_rate': 3e-08, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655827, 'epoch': 14.78, 'global_step/max_steps': '1020/1035', 'percentage': '98.55%', 'elapsed_time': '25m 54s', 'remaining_time': '22s'}\n",
      "Train:  99%|███████████████████████████████▌| 1020/1035 [25:54<00:22,  1.49s/it]\n",
      "{'eval_loss': 0.51455599, 'eval_acc': 0.85310345, 'eval_runtime': 0.4482, 'eval_samples_per_second': 24.542, 'eval_steps_per_second': 2.231, 'epoch': 14.78, 'global_step/max_steps': '1020/1035', 'percentage': '98.55%', 'elapsed_time': '25m 54s', 'remaining_time': '22s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 144.93it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-1020\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.52888746, 'acc': 0.84600372, 'grad_norm': 1.36834872, 'learning_rate': 1e-08, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655636, 'epoch': 14.86, 'global_step/max_steps': '1025/1035', 'percentage': '99.03%', 'elapsed_time': '26m 2s', 'remaining_time': '15s'}\n",
      "{'loss': 0.54988055, 'acc': 0.84526129, 'grad_norm': 1.04089355, 'learning_rate': 0.0, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655625, 'epoch': 14.93, 'global_step/max_steps': '1030/1035', 'percentage': '99.52%', 'elapsed_time': '26m 10s', 'remaining_time': '7s'}\n",
      "{'loss': 0.54266019, 'acc': 0.84748106, 'grad_norm': 1.40798724, 'learning_rate': 0.0, 'memory(GiB)': 18.73, 'train_speed(iter/s)': 0.655937, 'epoch': 15.0, 'global_step/max_steps': '1035/1035', 'percentage': '100.00%', 'elapsed_time': '26m 16s', 'remaining_time': '0s'}\n",
      "Train: 100%|████████████████████████████████| 1035/1035 [26:16<00:00,  1.36s/it]\n",
      "{'eval_loss': 0.51209462, 'eval_acc': 0.85241379, 'eval_runtime': 0.4423, 'eval_samples_per_second': 24.868, 'eval_steps_per_second': 2.261, 'epoch': 15.0, 'global_step/max_steps': '1035/1035', 'percentage': '100.00%', 'elapsed_time': '26m 17s', 'remaining_time': '0s'}\n",
      "Val: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 146.27it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-1035\n",
      "{'train_runtime': 1577.4491, 'train_samples_per_second': 10.45, 'train_steps_per_second': 0.656, 'train_loss': 0.60217846, 'epoch': 15.0, 'global_step/max_steps': '1035/1035', 'percentage': '100.00%', 'elapsed_time': '26m 17s', 'remaining_time': '0s'}\n",
      "Train: 100%|████████████████████████████████| 1035/1035 [26:17<00:00,  1.52s/it]\n",
      "[INFO:swift] last_model_checkpoint: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-1035\n",
      "[INFO:swift] best_model_checkpoint: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-1035\n",
      "[INFO:swift] images_dir: /mnt/workspace/alibabacloud_acp_learning/ACP/p2_Build LLM Q&A System/output/qwen2_5-1_5b-instruct/v9-20250715-150832/images\n",
      "[INFO:swift] End time of running main: 2025-07-15 15:34:56.159407\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env LOG_LEVEL=INFO\n",
    "!swift sft \\\n",
    "--learning_rate '0.00005' \\\n",
    "--lora_rank 8 \\\n",
    "--num_train_epochs 15 \\\n",
    "--dataset './resources/2_7/train_1k.jsonl' \\\n",
    "--batch_size '16' \\\n",
    "--max_length 512 \\\n",
    "--eval_step 20 \\\n",
    "--model_type 'qwen2_5-1_5b-instruct' \\\n",
    "--model_id_or_path './model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2278eac9",
   "metadata": {},
   "source": [
    "| Training loss image | Evaluation loss image |\n",
    "| --- | --- |\n",
    "|<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01hyQhbn1p04zyTeQkv_!!6000000005297-2-tps-671-451.png\" style=\"width: 500px;display: block; margin-left: auto; margin-right: auto\"/> | <img src=\"https://img.alicdn.com/imgextra/i3/O1CN01oy2oZv1r0ejEmpYdQ_!!6000000005569-2-tps-680-451.png\" style=\"width: 500px;display: block; margin-left: auto; margin-right: auto\"/> |\n",
    "\n",
    "\n",
    "| **Observation Metrics (Training Loss, Evaluation Loss):** | Training loss basically does not decrease, evaluation loss also basically does not decrease and even slightly increases |\n",
    "| --- | --- |\n",
    "| **Training Status:** | **Training Successful!** |  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3c61e",
   "metadata": {},
   "source": [
    "### 3.4 Examination After Fine-Tuning\n",
    "\n",
    "After fine-tuning, two `checkpoint` files are generally saved: `best_model_checkpoint` (the model parameters that performed best on the validation set) and `last_model_checkpoint` (the model parameters at the completion of the fine-tuning task).\n",
    "\n",
    "Here, replace the `ckpt_dir` in the code below with the address of the `best_model_checkpoint`, and you will be able to call the fine-tuned model.\n",
    "\n",
    "First, let's load the model into memory:  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6db20cca",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-07-15T08:33:00.584525Z",
     "iopub.status.busy": "2025-07-15T08:33:00.584196Z",
     "iopub.status.idle": "2025-07-15T08:33:00.833205Z",
     "shell.execute_reply": "2025-07-15T08:33:00.832744Z",
     "shell.execute_reply.started": "2025-07-15T08:33:00.584505Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from swift.tuners import Swift\n",
    "\n",
    "# Please modify ckpt_dir to the correct location before running\n",
    "ckpt_dir = 'output/qwen2_5-1_5b-instruct/v9-20250715-150832/checkpoint-1035' # Modify to your checkpoint location before running\n",
    "# Load the model\n",
    "ft_model = Swift.from_pretrained(model, ckpt_dir, inference_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94883c28",
   "metadata": {},
   "source": [
    "Let's take a look at the performance of the fine-tuned model in the exam.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff22696",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-07-15T08:33:04.539591Z",
     "iopub.status.busy": "2025-07-15T08:33:04.539275Z",
     "iopub.status.idle": "2025-07-15T08:35:30.293744Z",
     "shell.execute_reply": "2025-07-15T08:35:30.293287Z",
     "shell.execute_reply.started": "2025-07-15T08:33:04.539556Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "sum, score = 0, 0.0\n",
    "for line in open(\"./resources/2_7/test.jsonl\"):\n",
    "    # Read math questions from the test set\n",
    "    math_question = json.loads(line)\n",
    "    query = math_question[\"messages\"][1][\"content\"]\n",
    "    # Use the fine-tuned model for inference\n",
    "    response, _ = inference(ft_model, template, query)\n",
    "    # Get the correct answer\n",
    "    ans = math_question[\"messages\"][2][\"content\"]\n",
    "    pos = ans.find(\"ans\")\n",
    "    end_pos = ans[pos:].find('}}')\n",
    "    ans = ans[pos - 2: end_pos + pos + 2]\n",
    "    # Organize output\n",
    "    print((\"========================================================================================\"))\n",
    "    print(query.split(\"#Math Problem#\\n\")[1])\n",
    "    print(\"The answer to the question is: \" + ans)\n",
    "    print(\"-----------Model Response----------------\")\n",
    "    display(Latex(response))\n",
    "    print(\"-----------End of Response----------------\")\n",
    "    # Calculate the model's score\n",
    "    if ans in response:\n",
    "        score += 1\n",
    "        print(\"The model answered correctly\")\n",
    "    elif ans[6 : -2] in response:\n",
    "        score += 0.5\n",
    "        print(\"The model answered correctly but the output format was incorrect\")\n",
    "    else: print(\"The model answered incorrectly\")\n",
    "    sum += 1\n",
    "# Summary\n",
    "display(Markdown(\"The fine-tuned model scored **\" + str(int(100*score/sum)) + \"** points on the exam\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960edb37",
   "metadata": {},
   "source": [
    "### 3.5 Parameter Matrix Fusion\n",
    "\n",
    "After the model training is completed, there are two ways to use the trained model:\n",
    "\n",
    "1. Dynamically load the fine-tuned model at the time of invocation.\n",
    "\n",
    "   The low-rank parameter matrix obtained after fine-tuning occupies only 20MB of storage space, which is very convenient for incremental deployment and distribution. This is a commonly used method in engineering. It should be noted that whichever base model is used for fine-tuning, the corresponding base model must be specified upon loading.\n",
    "\n",
    "   In the previous subsection, we have already tried this method by specifying `ckpt_dir`.\n",
    "\n",
    "2. Merge the base model with the low-rank parameters obtained from fine-tuning to create a complete model with updated parameters, and then invoke the merged model.\n",
    "\n",
    "Here, we introduce the second method: merging the \"fine-tuned parameter matrix\" with the \"base model parameter matrix\" to store the fine-tuned model parameters as a complete parameter matrix.\n",
    "\n",
    "By using the `swift export` method and providing the path of the fine-tuned model (it is recommended to provide the `best_model_checkpoint`), the merged model can be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01928dad",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env LOG_LEVEL=INFO\n",
    "!swift export \\\n",
    "--ckpt_dir 'output/qwen2_5-1_5b-instruct/vx-xxx/checkpoint-xxx<Modify to checkpoint location before running>' \\\n",
    "--merge_lora true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb781cd7",
   "metadata": {},
   "source": [
    "The log displays the path of the model after fusion. The complete parameter matrix after fusion is stored by default in the `checkpoint` directory. (The complete model parameters for the PAI experimental environment are located at: `output/qwen2_5-1_5b-instruct/vX-XXX/checkpoint-XX-merged`).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a31b63",
   "metadata": {},
   "source": [
    "## ✅ Summary\n",
    "\n",
    "In this section, we have learned the following:\n",
    "\n",
    "* Understanding the core value of model fine-tuning: directly improving the model's reasoning ability in mathematics through targeted data injection, overcoming the limitations of prompt engineering and RAG chatbot.\n",
    "\n",
    "* Mastering key training parameters: learning rate controls the magnitude of parameter updates, epoch determines the number of data traversals, batch size affects gradient stability, and the loss function monitors the training status.\n",
    "\n",
    "* Understanding the principle of LoRA efficient fine-tuning: reducing memory consumption based on low-rank matrix decomposition (theoretical explanation), and optimizing training effects by adjusting the lora_rank parameter in practice.\n",
    "\n",
    "* Completing iterative hyperparameter tuning experiments: solving underfitting and overfitting problems through multiple adjustments of learning rate/data volume/training rounds, ultimately significantly improving the model's problem-solving accuracy.\n",
    "\n",
    "Although you can use pre-prepared datasets in this tutorial to experience GPU computing resources for free during fine-tuning, **in actual production, fine-tuning is not simple and requires comprehensive consideration of factors such as computing power costs, data scale, and quality**.\n",
    "Particularly, attention should be paid to the following aspects:\n",
    "1. Whether low-cost solutions like prompt engineering and RAG chatbot are sufficient to handle the problem.\n",
    "2. Whether the amount and quality of data meet the minimum threshold for fine-tuning (at least 1000 high-quality data points).\n",
    "3. Ensuring that the project budget matches the technical expertise, with acceptable cost-effectiveness.\n",
    "\n",
    "### Further Learning\n",
    "#### Fine-tuning for More Machine Learning Tasks\n",
    "\n",
    "* Image classification (e.g., object recognition, medical image diagnosis)\n",
    "    * Fine-tuning purpose: Optimize feature extraction capabilities for specific image datasets based on pre-trained models (e.g., ResNet, ViT).\n",
    "    * Key points: Reduce data requirements and leverage the general visual knowledge of pre-trained models to transfer to niche tasks.\n",
    "\n",
    "* Object detection (e.g., autonomous driving, security monitoring)\n",
    "    * Fine-tuning purpose: Adjust the model (e.g., YOLO, Faster R-CNN) for detecting specific objects or scenes.\n",
    "    * Key points: Optimize the model's sensitivity to target location and category, reducing false positives/missed detections.\n",
    "\n",
    "* Machine translation (e.g., domain-specific translation)\n",
    "    * Fine-tuning purpose: Adapt a general translation model (e.g., mBART, T5) to professional terminology and expression habits.\n",
    "    * Key points: Address semantic bias issues in vertical domain translations for general models.\n",
    "\n",
    "* Recommendation systems (e.g., e-commerce, content platforms)\n",
    "    * Fine-tuning purpose: Optimize recommendation models (e.g., collaborative filtering, deep ranking models) based on user behavior data.\n",
    "    * Key points: Balance personalized recommendations with cold-start problems, improving click-through rates/conversion rates.\n",
    "\n",
    "#### More Efficient Fine-tuning Methods\n",
    "\n",
    "* **Freeze**: This method was one of the earliest PEFT methods. It freezes most of the model's parameters during fine-tuning, training only a small portion of the model’s parameters (e.g., the last few neural network layers) to quickly adapt to specific task needs. Characteristics:\n",
    "    * High parameter efficiency (only a small number of parameters are trained).\n",
    "    * Suitable for scenarios where the task is close to the pre-training objective (e.g., text classification).\n",
    "    * May not perform well for complex tasks.\n",
    "<div style=\"text-align: left;\">\n",
    "<img src=\"https://img.alicdn.com/imgextra/i1/O1CN01X9GOk81sgAEtxflGR_!!6000000005795-2-tps-1340-686.png\" style=\"width: 600px;display: block; margin-left: 60px; margin-right: auto\"/>\n",
    "</div>\n",
    "\n",
    "* **Adapter Tuning**: In the original model architecture, Adapter layers are inserted between certain positions. During fine-tuning, the model’s original parameters are not trained; only these Adapter layers are trained while the original parameters do not participate in training. Characteristics:\n",
    "    * Modular design with strong compatibility.\n",
    "    * Slightly higher parameter count than LoRA but stable performance.\n",
    "    * Requires modifying the model structure, with additional computation required during inference.\n",
    "<div style=\"text-align: left;\">\n",
    "<img src=\"https://img.alicdn.com/imgextra/i2/O1CN016gccCd1CdDpjDxbe9_!!6000000000103-2-tps-1482-1048.png\" style=\"width: 500px;display: block; margin-left: 60px; margin-right: auto\"/>\n",
    "</div>\n",
    "\n",
    "* Prompt Tuning: Indirectly control model behavior by optimizing learnable vectors (Prompt) at the input, freezing model parameters. Characteristics:\n",
    "    * No need to modify the model structure; only adjust the input.\n",
    "    * Friendly to generative tasks (e.g., translation, dialogue).\n",
    "    * Effect depends on prompt design; may be insufficient for complex tasks.\n",
    "\n",
    "#### Fine-tuning Dataset Construction Strategy\n",
    "\n",
    "Generally speaking, for more complex scenarios, fine-tuning requires at least **1000+ high-quality training dataset samples**. When constructing the dataset, confirm the following points:\n",
    "\n",
    "* **Data Quality**: Ensure the dataset is accurate, relevant, and remove ambiguous or incorrect samples.\n",
    "* **Diversity Coverage**: Include full scenarios, multi-contexts, and professional terminology of the task to avoid single distribution.\n",
    "* **Class Balance**: If the task involves multiple class scenarios, ensure balanced samples across classes to prevent model bias towards one class.\n",
    "* **Continuous Iteration**: Fine-tuning is an iterative process; continuously optimize and expand the dataset based on feedback from the model's performance on the validation set.\n",
    "\n",
    "If you lack data when fine-tuning a model, it is recommended to enhance the model's capabilities using knowledge base retrieval (e.g., business documents, FAQs).\n",
    "\n",
    "> In many complex business scenarios, a combined approach of model optimization and knowledge base retrieval can be adopted.\n",
    "\n",
    "You can also use the following strategies to expand the dataset:\n",
    "\n",
    "* **Manual Annotation**: Extend typical scenario data by experts.\n",
    "* **Model Generation**: Simulate business scenario data using LLMs.\n",
    "* **External Collection**: Obtain data through web scraping, public datasets, user feedback, etc.\n",
    "\n",
    "#### Common Evaluation Metrics for Models\n",
    "\n",
    "Evaluation metrics differ significantly for different types of tasks. Below are some typical task evaluation metrics:\n",
    "\n",
    "* **Classification Tasks**:\n",
    "    * Accuracy: The proportion of correct predictions.\n",
    "    * Precision, Recall, and F1 Score: Used to measure the identification effect of positive classes in binary or multi-class classification problems.\n",
    "\n",
    "* **Text Generation Tasks**:\n",
    "    * BLEU (Bilingual Evaluation Understudy): Mainly used in natural language processing tasks such as machine translation, calculating scores by comparing n-gram overlaps between candidate translations and one or more reference translations.\n",
    "    * ROUGE (Recall-Oriented Understudy for Gisting Evaluation): Commonly used for automatic summary evaluation, based on n-gram recall, precision, and F-measure.\n",
    "    * Perplexity: Measures the uncertainty of a probability distribution model predicting a sample; lower is better.\n",
    "\n",
    "* **Image Recognition/Object Detection**:\n",
    "    * Intersection over Union (IoU): The ratio of the intersection area to the union area of two bounding boxes.\n",
    "    * mAP (mean Average Precision): Widely used in object detection tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76c79e-cb06-4a10-b6f0-57cc84ed069d",
   "metadata": {},
   "source": [
    "## 🔥 Post-Class Quiz\n",
    "\n",
    "### 🔍 Single-Choice Question\n",
    "<details>\n",
    "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
    "<b>Which of the following statements about LoRA is incorrect ❓</b>\n",
    "\n",
    "- A. LoRA can effectively reduce the cost of fine-tuning large language models.\n",
    "- B. LoRA modifies the original weights of the fine-tuned model.\n",
    "- C. LoRA's implementation is relatively simple and easy to integrate.\n",
    "- D. The results of LoRA fine-tuning can be easily reverted.\n",
    "\n",
    "**[Click to View Answer]**\n",
    "</summary>\n",
    "\n",
    "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
    "\n",
    "✅ **Reference Answer: B**  \n",
    "📝 **Explanation**:  \n",
    "- LoRA does not directly modify the original weights but indirectly affects model behavior by adding low-rank matrices. This makes rollback operations simple, as you only need to remove the added low-rank matrices.\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 🔍 Multiple-Choice Question\n",
    "<details>\n",
    "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
    "<b>You are using Swift to fine-tune a Qwen model and notice a significant upward trend in loss on the validation set. Which of the following actions can help alleviate or resolve this issue ❓</b>\n",
    "\n",
    "- A. Increase learning rate\n",
    "- B. Decrease learning rate\n",
    "- C. Increase --num_train_epochs\n",
    "- D. Decrease --num_train_epochs\n",
    "\n",
    "**[Click to View Answer]**\n",
    "</summary>\n",
    "\n",
    "<div style=\"margin-top: 10px; padding: 15px;  border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
    "\n",
    "✅ **Reference Answer: BD**  \n",
    "📝 **Explanation**:  \n",
    "- learning_rate: A high learning rate can lead to fast model training but may cause oscillations near the optimal solution, or even non-convergence, resulting in fluctuating loss, which may appear like overfitting. However, this is different from true overfitting.  \n",
    "- num_train_epochs: Overfitting may also be caused by too many training epochs. Reducing the number of training epochs can prevent the model from over-learning the training data.\n",
    "\n",
    "</div>\n",
    "</details>  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
