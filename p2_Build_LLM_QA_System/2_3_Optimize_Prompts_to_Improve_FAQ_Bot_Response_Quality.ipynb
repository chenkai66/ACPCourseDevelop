{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92010cd5",
      "metadata": {},
      "source": [
        "# 2.3 Optimizing Prompts to Improve the Quality of Responses from Q&A Bots",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The large language model in the above example successfully output a tutorial for making handmade keychains, but the content was not concise enough. If you only want the large language model to output content in a specific style and structure, such as only including the theme, list of materials, and steps, you can provide it with several examples so that it can \"imitate\" them!",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# æ‰‹å·¥è´ºå¡åˆ¶ä½œæ•™ç¨‹\n",
            "\n",
            "## ææ–™æ¸…å•\n",
            "- å½©è‰²å¡çº¸\n",
            "- å‰ªåˆ€\n",
            "- èƒ¶æ°´æˆ–åŒé¢èƒ¶\n",
            "- é©¬å…‹ç¬”æˆ–å½©è‰²é“…ç¬”\n",
            "- è£…é¥°ç‰©ï¼ˆå¦‚äº®ç‰‡ã€ä¸å¸¦ã€è´´çº¸ç­‰ï¼‰\n",
            "\n",
            "## æ­¥éª¤\n",
            "1. **å‡†å¤‡ææ–™**ï¼šé¦–å…ˆï¼Œå‡†å¤‡å¥½æ‰€æœ‰éœ€è¦çš„ææ–™ï¼ŒåŒ…æ‹¬ä¸åŒé¢œè‰²çš„å¡çº¸ã€å‰ªåˆ€ã€èƒ¶æ°´æˆ–åŒé¢èƒ¶ã€é©¬å…‹ç¬”æˆ–å½©è‰²é“…ç¬”ï¼Œä»¥åŠä»»ä½•ä½ æƒ³è¦ç”¨æ¥è£…é¥°è´ºå¡çš„å°ç‰©ä»¶ã€‚\n",
            "2. **è£å‰ªå¡çº¸**ï¼šé€‰æ‹©ä¸€å¼ ä½œä¸ºè´ºå¡ä¸»ä½“çš„å¡çº¸ï¼Œå°†å…¶å¯¹æŠ˜ï¼Œè£å‰ªæˆä½ æƒ³è¦çš„å¤§å°ã€‚å¸¸è§çš„è´ºå¡å°ºå¯¸å¤§çº¦æ˜¯5è‹±å¯¸ x 7è‹±å¯¸ã€‚\n",
            "3. **è®¾è®¡å°é¢**ï¼šä½¿ç”¨é©¬å…‹ç¬”æˆ–å½©è‰²é“…ç¬”åœ¨è´ºå¡å°é¢ä¸Šç»˜åˆ¶å›¾æ¡ˆæˆ–å†™ä¸Šç¥ç¦è¯­ã€‚ä½ ä¹Ÿå¯ä»¥å‰ªè£å…¶ä»–é¢œè‰²çš„å¡çº¸ï¼Œç²˜è´´åˆ°å°é¢ä¸Šï¼Œå¢åŠ å±‚æ¬¡æ„Ÿã€‚\n",
            "4. **æ·»åŠ è£…é¥°**ï¼šåˆ©ç”¨äº®ç‰‡ã€ä¸å¸¦ã€è´´çº¸ç­‰è£…é¥°ç‰©ï¼Œä¸ºè´ºå¡å¢æ·»ä¸ªæ€§åŒ–çš„è£…é¥°ã€‚ç¡®ä¿è£…é¥°ç‰©ç‰¢å›ºåœ°ç²˜è´´åœ¨å¡çº¸ä¸Šã€‚\n",
            "5. **æ’°å†™å†…é¡µ**ï¼šæ‰“å¼€è´ºå¡ï¼Œåœ¨å†…é¡µå†™ä¸‹ä½ æƒ³å¯¹æ”¶å¡äººè¯´çš„è¯ã€‚å¯ä»¥æ˜¯ä¸€æ®µæ¸©é¦¨çš„è¯è¯­ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€é¦–è¯—æˆ–è€…ç®€å•çš„ç¥ç¦ã€‚\n",
            "6. **æ£€æŸ¥ä¸å®Œå–„**ï¼šä»”ç»†æ£€æŸ¥è´ºå¡çš„æ¯ä¸€ä¸ªéƒ¨åˆ†ï¼Œç¡®ä¿æ²¡æœ‰é—æ¼çš„åœ°æ–¹ï¼Œè£…é¥°ç‰©éƒ½å·²ç‰¢å›ºç²˜è´´ï¼Œæ–‡å­—æ¸…æ™°å¯è¯»ã€‚\n",
            "7. **å®Œæˆ**ï¼šç°åœ¨ï¼Œä½ çš„æ‰‹å·¥è´ºå¡å·²ç»å®Œæˆäº†ï¼ä½ å¯ä»¥å°†å®ƒé€ç»™æœ‹å‹æˆ–å®¶äººï¼Œè¡¨è¾¾ä½ çš„å¿ƒæ„ã€‚\n",
            "\n",
            "## ç»“æŸè¯­\n",
            "å¸Œæœ›è¿™ä¸ªæ•™ç¨‹èƒ½å¸®åŠ©ä½ åˆ¶ä½œå‡ºå……æ»¡å¿ƒæ„çš„æ‰‹å·¥è´ºå¡ï¼Œè®©ä½ çš„æƒ…æ„Ÿä¼ é€’æ›´åŠ ç‰¹åˆ«å’Œæœ‰æ„ä¹‰ï¼"
          ]
        }
      ],
      "source": [
        "question_task= \"\"\"",
        "ã€ä»»åŠ¡è¦æ±‚ã€‘",
        "è¯·æ ¹æ®ç”¨æˆ·çš„ä¸»é¢˜ï¼Œç»“åˆä¸‹é¢ã€æ ·ä¾‹ã€‘ç»™çš„ä¾‹å­ï¼Œç†è§£å’Œä½¿ç”¨ä¸€è‡´çš„é£æ ¼å’Œç»“æ„ç»§ç»­åˆ›ä½œå†…å®¹ï¼Œä¸è¦è¾“å‡ºå¤šä½™çš„å†…å®¹ã€‚",
        "---",
        "ã€è¾“å‡ºè¦æ±‚ã€‘",
        "æœ€ç»ˆè¾“å‡ºéœ€è¦ä»¥Markdownæ ¼å¼å‘ˆç°ï¼Œè¯·æ³¨æ„ï¼Œåœ¨ä½ çš„å›ç­”ä¸­åŒ…å«æ‰€æœ‰å¿…è¦çš„Markdownå…ƒç´ ï¼Œå¦‚æ ‡é¢˜ã€åˆ—è¡¨ã€é“¾æ¥ã€å›¾ç‰‡å¼•ç”¨ã€åŠ ç²—ç­‰ï¼Œä»¥ä¾¿äºé˜…è¯»ã€åç»­ç¼–è¾‘å’Œä¿å­˜ã€‚",
        "---",
        "ã€æ ·ä¾‹ã€‘",
        "### ç¤ºä¾‹1: åˆ¶ä½œç®€æ˜“ä¹¦ç­¾",
        "# ç®€æ˜“ä¹¦ç­¾åˆ¶ä½œæ•™ç¨‹",
        "",
        "## ææ–™æ¸…å•",
        "- å½©è‰²å¡çº¸",
        "- å‰ªåˆ€",
        "- è£…é¥°è´´çº¸",
        "- é“…ç¬”",
        "",
        "## æ­¥éª¤",
        "1. é€‰æ‹©ä¸€å¼ å½©è‰²å¡çº¸ã€‚",
        "2. ç”¨é“…ç¬”åœ¨å¡çº¸ä¸Šç”»å‡ºä¸€ä¸ªé•¿æ–¹å½¢ï¼Œå°ºå¯¸çº¦ä¸º2è‹±å¯¸ x 6è‹±å¯¸ã€‚",
        "3. æ²¿ç€é“…ç¬”çº¿å‰ªä¸‹é•¿æ–¹å½¢ã€‚",
        "4. ä½¿ç”¨è£…é¥°è´´çº¸å¯¹ä¹¦ç­¾è¿›è¡Œä¸ªæ€§åŒ–è£…é¥°ã€‚",
        "5. å®Œæˆï¼ç°åœ¨ä½ æœ‰äº†ä¸€ä¸ªç‹¬ä¸€æ— äºŒçš„ä¹¦ç­¾ã€‚",
        "",
        "## ç»“æŸè¯­",
        "å¸Œæœ›è¿™ä¸ªæ•™ç¨‹èƒ½å¸®åŠ©ä½ åˆ¶ä½œå‡ºæ»¡æ„çš„ä¹¦ç­¾ï¼",
        "",
        "---",
        "ã€ç”¨æˆ·è¾“å…¥ã€‘",
        "ä»¥ä¸‹æ˜¯ç”¨æˆ·çš„è¦æ±‚åˆ›ä½œçš„ä¸»é¢˜ï¼š",
        "\"\"\"",
        "question_doc = \"åˆ¶ä½œæ‰‹å·¥è´ºå¡\"",
        "",
        "question = question_task + question_doc",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0b18130",
      "metadata": {},
      "source": [
        "## ğŸš„ Preface  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a33d28f-9014-414a-a7b5-b016e12d4bb0",
      "metadata": {},
      "source": [
        "The Q&A bot from the previous section can already answer questions about company rules and regulations, but this is only the first step in building an excellent user experience. Users often expect more personalized and precise interactions.",
        "This section will introduce techniques in prompt engineering, teaching you how to improve or control the output of the Q&A bot by optimizing promptsâ€”for example, adjusting tone, standardizing formats, or even enabling it to handle tasks like text summarization, inference, and transformation.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37a39a58",
      "metadata": {},
      "source": [
        "## ğŸ Course Objectives",
        "",
        "After completing this section of the course, you will be able to:",
        "",
        "* Understand prompt frameworks and templates",
        "* Learn about prompt techniques and best practices",
        "* Study how to apply large language models (LLMs) in engineering to handle various types of tasks",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b0e80fa",
      "metadata": {},
      "source": [
        "## 1. Previous Content Recap",
        "",
        "In the previous section, through the RAG method, large language models (LLMs) have already obtained the company's private knowledge. For ease of invocation, this was encapsulated into several functions and saved in chatbot/rag.py. You can now quickly invoke it using the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "7aa6756d-47a4-4b11-8074-d992d6ce327b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:03.577636Z",
          "start_time": "2025-01-15T06:49:03.573248Z"
        }
      },
      "outputs": [],
      "source": [
        "from chatbot import rag",
        "import os",
        "from config.load_key import load_key",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "f23f2c04-c229-4f5f-b2cd-50c8a45672e4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:09.717475Z",
          "start_time": "2025-01-15T06:49:09.713681Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ä½ é…ç½®çš„ API Key æ˜¯ï¼šsk-1a*****\n"
          ]
        }
      ],
      "source": [
        "# Load API key",
        "load_key()",
        "print(f'Your configured API Key is: {os.environ[\"DASHSCOPE_API_KEY\"][:5]+\"*\"*5}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "91138451-fbc1-49ff-a268-2ece341b10bd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:11.770743Z",
          "start_time": "2025-01-15T06:49:11.611835Z"
        }
      },
      "outputs": [],
      "source": [
        "# Load index",
        "# The previous section has already built the index, so the index can be loaded directly here. If you need to rebuild the index, you can add a line of code: rag.indexing()",
        "index = rag.load_index()",
        "query_engine = rag.create_query_engine(index=index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "1d2d8d11-5a07-441c-9cd6-6ba10bf80603",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:16.961802Z",
          "start_time": "2025-01-15T06:49:16.959373Z"
        }
      },
      "outputs": [],
      "source": [
        "# Define the question-answering function",
        " def ask_llm(question, query_engine):",
        "   streaming_response = query_engine.query(question)",
        "   streaming_response.print_response_stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1715f36",
      "metadata": {},
      "source": [
        "## 2. Optimizing Prompts to Improve the Quality of Responses from Large Language Models",
        "",
        "In the previous section, you obtained the management tools used internally by the company through the RAG method. However, your colleague hopes that the Q&A bot not only provides the tool name but also includes a link to the tool. While you could ask colleagues to include a note like \"provide the download address\" in their questions, this approach is inefficient. Therefore, you consider improving the program: after receiving user questions, supplement them with additional requirements for the response content.",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "b12a310a-660c-4cd3-9d23-8819bdefb8bb",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:34.992776Z",
          "start_time": "2025-01-15T06:49:29.417135Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "å¯¹äºé¡¹ç›®ç®¡ç†ï¼Œæ¨èä½¿ç”¨Jiraæˆ–Trelloã€‚è¿™ä¸¤ä¸ªå·¥å…·éƒ½éå¸¸é€‚åˆé¡¹ç›®ç®¡ç†å’Œè·Ÿè¸ªï¼Œå¯ä»¥å¸®åŠ©å›¢é˜Ÿæ›´é«˜æ•ˆåœ°å®Œæˆä»»åŠ¡ã€‚\n",
            "\n",
            "- Jira æ˜¯ä¸€ä¸ªå¼ºå¤§çš„é¡¹ç›®ç®¡ç†å’Œé—®é¢˜è·Ÿè¸ªå·¥å…·ï¼Œç‰¹åˆ«é€‚ç”¨äºè½¯ä»¶å¼€å‘å›¢é˜Ÿã€‚ä½ å¯ä»¥ä» Atlassian çš„å®˜æ–¹ç½‘ç«™ä¸‹è½½ï¼š[Jira ä¸‹è½½é¡µé¢](https://www.atlassian.com/software/jira/download)\n",
            "\n",
            "- Trello æ˜¯ä¸€ä¸ªçµæ´»çš„çœ‹æ¿å¼é¡¹ç›®ç®¡ç†å·¥å…·ï¼Œé€‚åˆå„ç§ç±»å‹çš„å›¢é˜Ÿã€‚ä½ ä¹Ÿå¯ä»¥ä»å…¶å®˜æ–¹ç½‘ç«™è·å–ï¼š[Trello ä¸‹è½½é¡µé¢](https://trello.com/download)"
          ]
        }
      ],
      "source": [
        "question = \"What tools should our company use for project management?\"",
        "instruction = \"If it's a tool consultation question, be sure to provide the download link.\"",
        "new_question = question + instruction",
        "ask_llm(new_question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "806fadd6-673a-4542-b3e2-fe36310b30ad",
      "metadata": {},
      "source": [
        "After adjusting the way prompts were provided, the Q&A bot's responses became more aligned with the colleagues' requirements. Therefore, the choice of prompt significantly determines the quality of the large language model's responses. Next, you can refer to some prompt frameworks to construct your prompts.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c006a5ea-5693-4805-9d2c-ccc0d2214939",
      "metadata": {},
      "source": [
        "## 3. Prompt Framework",
        "",
        "### 3.1 Basic Elements",
        "",
        "When communicating with a large language model, you can imagine it as a person who has undergone \"socialization training.\" The way of communication should be the same as how humans exchange information. Your requirements need to be clear and unambiguous. The clearer and more precise your way of asking (Prompt), the better the large language model can grasp the key points of the question, and the more its response will meet your expectations. Generally speaking, the following elements need to be clarified in the prompt: **Task Objective, Context, Role, Audience, Sample, Output Format**. These elements form a prompt framework that can help you construct a complete and effective prompt.",
        "",
        "|Element|Meaning|",
        "|----|----|",
        "|Task Objective (Object)|Clearly specify what task the large language model is required to complete, allowing the model to focus on specific goals|",
        "|Context (Context)|Background information about the task, such as operational processes or task scenarios, clarifying the scope of the discussion for the large language model|",
        "|Role (Role)|The role the large language model plays, or emphasizing the tone, writing style, etc., that the model should use, clarifying the expected emotional tone of the response|",
        "|Audience (Audience)|Clarify the specific audience the large language model is targeting, constraining its response style|",
        "|Sample (Sample)|Specific examples for the large language model to refer to; the model will abstract solutions and pay attention to specific formats from these|",
        "|Output Format (Output Format)|Clearly specify the format of the output, output type, range of enumerated values. Typically, it also explicitly states content and information that should not be included in the output, which can be further clarified by combining with examples|",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc35341a-3d7c-4881-81e3-935d42a1dcb2",
      "metadata": {},
      "source": [
        "Of course, in addition to the prompt framework discussed above, many problem analysis paradigms can be used to help you describe clear and specific requirements. For example, SWOT analysis and 5W2H analysis. Additionally, you can also consider using Alibaba Cloud Model Studio-provided [Prompt Automatic Optimization Tool](https://bailian.console.aliyun.com/?tab=app#/component-manage/prompt/optimize) to help refine your prompts.",
        "",
        "<a href=\"https://img.alicdn.com/imgextra/i3/O1CN014JCCqn22zX6xB4tt3_!!6000000007191-0-tps-2068-1052.jpg\" target=\"_blank\">",
        "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN014JCCqn22zX6xB4tt3_!!6000000007191-0-tps-2068-1052.jpg\" width=\"700\"/>",
        "</a>  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e1fa6d9",
      "metadata": {},
      "source": [
        "### 3.2 Prompt Template",
        "",
        "When developing large language model applications, allowing users to directly write prompts according to a framework is not the optimal choice. You can refer to the elements in various prompt frameworks to construct a prompt template. A prompt template can preset certain information, such as the role of the large language model and precautions, thereby constraining the behavior of the model. Developers only need to configure input parameters within the template to create standardized applications for large language models.",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84e29a3-a017-47c4-907e-2ff897333d1d",
      "metadata": {},
      "source": [
        "In the RAG chatbot created using LlamaIndex, there is a default prompt template as follows:",
        "",
        "- The default template can be viewed using code. You can refer to [the code on the LlamaIndex official website.](https://docs.llamaindex.ai/en/stable/examples/prompts/prompts_rag/) The original LlamaIndex prompt template is:  ",
        "",
        "",
        "",
        "```text",
        "Context information is below.",
        "---------------------",
        "{context_str}",
        "---------------------",
        "Given the context information and not prior knowledge, answer the query.",
        "Query: {query_str}",
        "Answer:",
        "```",
        "",
        "Among them, `context_str` and `query_str` both represent variables. During the vector retrieval and questioning process, `context_str` will be replaced with the context information retrieved from the vector database, and `query_str` will be replaced with the user's question.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78ff8788-4d67-47b8-a4a2-3f3a85e6b0c5",
      "metadata": {},
      "source": [
        "Since the original template is a general-purpose template, it is not suitable for constraining the behavior of the Q&A robot. You can readjust the prompt template through the following sample code, where `prompt_template_string` represents the new prompt template, and you can modify it according to your own scenario.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "05aea996-fdc8-4a35-922f-288313881fa3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T07:52:22.442975Z",
          "start_time": "2025-01-15T07:52:22.435217Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x31db8b610>"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Build prompt template",
        "prompt_template_string = (",
        "    \"You are the company's customer service assistant, you need to answer users' questions concisely.\"",
        "    \"\\nã€Notesã€‘ï¼š\\n\"",
        "    \"1. Answer user questions based on contextual information.\\n\"",
        "    \"2. Only answer the user's question, do not output other information\\n\"",
        "    \"The following is reference information.\"",
        "    \"---------------------\\n\"",
        "    \"{context_str}\\n\"",
        "    \"---------------------\\n\"",
        "    \"Question: {query_str}\\n.\"",
        "    \"Answer: \"",
        ")",
        "",
        "# Update prompt template",
        "rag.update_prompt_template(query_engine, prompt_template_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa99f324",
      "metadata": {},
      "source": [
        "## 4. Techniques for Building Effective Prompts",
        "",
        "In section 3.1, some key elements of prompt design were listed. The following will explain prompt techniques in detail based on specific scenarios, starting from the elements of prompts.",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b3eb55-646d-41dd-b801-152d76205bbe",
      "metadata": {},
      "source": [
        "### 4.1 Clearly Express Requirements and Use Delimiters",
        "",
        "Clearly expressing requirements ensures that the content generated by large models is highly relevant to the task. Requirements include the task objectives, background, and contextual information. Delimiters can also be used to separate various elements of the prompts.",
        "",
        "Delimiters help large models focus on specific goals, avoid ambiguous interpretations, and reduce the processing of unnecessary information. Common delimiters include \"**ã€ã€‘**\", \"**<< >>**\", and \"**###**\" to identify key elements, while \"**===**\" or \"**---**\" can be used to separate paragraphs. Alternatively, XML tags like `<tag> </tag>` can be used to mark specific sections. Of course, delimiters are not limited to the ones mentioned above; any symbol that provides clear separation will suffice. It is important to note that if a certain symbol (e.g., ã€ã€‘) has been heavily used in the prompt, it should be avoided as a delimiter to prevent confusion.",
        "",
        "In the following example, we will use the prompt template from section 3.2 to help you polish your document in the role of a Q&A robot.",
        "",
        "- You need to run the code in section 3.2 to make the new prompt template effective.",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "8918731d-49b6-4132-9ba5-2dce3eee360c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T07:52:32.192934Z",
          "start_time": "2025-01-15T07:52:24.619420Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ã€æ–°å‘˜å·¥è®­ç»ƒè¥æ´»åŠ¨ã€‘æ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„å¤§å®¶åº­ï¼ä¸ºäº†è®©æ¯ä½æ–°æˆå‘˜éƒ½èƒ½å¿«é€Ÿèå…¥å›¢é˜Ÿï¼Œæˆ‘ä»¬ç²¾å¿ƒç­–åˆ’äº†â€œæ–°å‘˜å·¥è®­ç»ƒè¥â€æ´»åŠ¨ã€‚åœ¨è¿™é‡Œï¼Œä½ ä¸ä»…èƒ½å¤Ÿå…¨é¢äº†è§£å…¬å¸çš„æ–‡åŒ–ã€ä»·å€¼è§‚å’Œå‘å±•æ„¿æ™¯ï¼Œè¿˜èƒ½é€šè¿‡ä¸€ç³»åˆ—äº’åŠ¨ç¯èŠ‚ï¼Œä¸æ¥è‡ªä¸åŒéƒ¨é—¨çš„å°ä¼™ä¼´ä»¬å»ºç«‹æ·±åšçš„å‹è°Šã€‚æ›´æœ‰èµ„æ·±å¯¼å¸ˆé¢å¯¹é¢æŒ‡å¯¼ï¼ŒåŠ©ä½ åœ¨èŒä¸šç”Ÿæ¶¯çš„èµ·è·‘çº¿ä¸ŠåŠ é€Ÿå‰è¡Œã€‚è®©æˆ‘ä»¬ä¸€èµ·å¼€å¯è¿™æ®µå……æ»¡æŒ‘æˆ˜ä¸æœºé‡çš„æ—…ç¨‹å§ï¼"
          ]
        }
      ],
      "source": [
        "question = \"\"\"",
        "Expand and polish the text enclosed in ã€ã€‘ brackets below to make the copy vivid, creative, and appealing to new employees.",
        "ã€New Employee Training Camp Activityã€‘",
        "\"\"\"",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62ae4c28-a2e6-4ad3-995b-7af2d78b19d1",
      "metadata": {},
      "source": [
        "From the above execution results, it can be observed that the prompt clearly specifies the task requirements: to expand and polish the text. The theme of the task is \"**New Employee Training Camp Activities**\", the audience is \"**company employees**\", and the \"**ã€ã€‘**\" delimiter is used to separate the text. Using such a prompt ensures that the output not only retains the original meaning but also becomes more vivid and engaging.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7e4407d-3cff-4f00-a39b-b8ddfd7cce1d",
      "metadata": {},
      "source": [
        "### 4.2 é™å®šè§’è‰²å’Œå—ä¼—",
        "",
        "è§’è‰²æŒ‡å¤§æ¨¡å‹åœ¨ç‰¹å®šåœºæ™¯ä¸‹åº”æ‰®æ¼”çš„èº«ä»½ï¼Œå¦‚ä¸“å®¶ã€é¡¾é—®æˆ–åŠ©æ‰‹ã€‚å—ä¼—æ˜¯ä½¿ç”¨æ¨¡å‹è¾“å‡ºçš„ç›®æ ‡ç”¨æˆ·ç¾¤ä½“ï¼Œå¦‚æ™®é€šæ¶ˆè´¹è€…æˆ–å­¦ç”Ÿã€‚è¿™æœ‰åŠ©äºå¼€å‘è€…å®šåˆ¶å†…å®¹çš„é£æ ¼å’Œæ·±åº¦ã€‚ä»¥ä¸‹ç¤ºä¾‹å°†å±•ç¤ºä¸åŒçš„ç³»ç»Ÿè§’è‰²å¯¹è¾“å‡ºæ–‡æœ¬é£æ ¼å’Œå†…å®¹çš„å½±å“ï¼Œç”¨æˆ·çš„é—®é¢˜ä¸ºï¼šqwen-vlæ˜¯ä»€ä¹ˆï¼Ÿ",
        "",
        "- ä»¥ä¸‹ä»…æ¼”ç¤ºæç¤ºè¯æ¨¡æ¿å’Œè¾“å‡ºçš„å†…å®¹ï¼Œä½ å¯ä»¥å°†æç¤ºè¯æ¨¡æ¿ä¼ å…¥åˆ°`prompt_template_string`ä¸­ï¼Œå°†ç”¨æˆ·é—®é¢˜ä¼ å…¥åˆ°`question`ä¸­ï¼Œå†è°ƒç”¨`ask_llm`å‡½æ•°ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79e5f128-8b02-40ca-b789-d577b6e38a3a",
      "metadata": {},
      "source": [
        "<table>",
        "  <thead>",
        "    <tr>",
        "      <th width = '80px'>Role</th>",
        "      <th width = '500px'>Large Language Model Algorithm Engineer</th>",
        "      <th width = '500px'>Elementary School Teacher</th>",
        "    </tr>",
        "  </thead>",
        "  <tbody valign = 'top'>",
        "    <tr>",
        "      <td>Prompt Template</td>",
        "      <td>\"You are a senior large language model algorithm engineer. Please answer the user's question based on the provided information\\n\"",
        "\"---------------------\\n\"",
        "\"{context_str}\\n\"",
        "\"---------------------\\n\"",
        "\"Question: {query_str}\\n.\"",
        "\"Answer: \"</td>",
        "      <td>\"You are an elementary school teacher. Please answer the user's question based on the provided information\\n\"",
        "\"---------------------\\n\"",
        "\"{context_str}\\n\"",
        "\"---------------------\\n\"",
        "\"Question: {query_str}\\n.\"",
        "\"Answer: \"</td>",
        "    </tr>",
        "    <tr>",
        "      <td>User Question</td>",
        "      <td>Qwen-VL</td>",
        "      <td>Qwen-VL</td>",
        "    </tr>",
        "    <tr>",
        "      <td>Output Answer</td>",
        "      <td>Qwen-VL is a multimodal pre-trained model introduced by Alibaba Cloud. It combines text and image processing capabilities, enabling it to understand and generate text descriptions related to images. Simply put, Qwen-VL can be regarded as an artificial intelligence assistant that can \"describe images in words.\" It not only understands the content of images but also generates corresponding textual descriptions or answers questions related to the images. This technology has broad application prospects in various fields, such as automatic image tagging, visual question-answering systems, and content-based image retrieval. For new media operations, tools like Qwen-VL can help improve content creation efficiency, enhance user experience, and provide users with richer and more diverse ways of presenting information.</td>",
        "      <td>Kids, today weâ€™re going to meet a new friend named Qwen-VL!\\nQwen-VL is like a very smart robot that has \"eyes\" ğŸ‘€ and a \"brain\" ğŸ§ . It can \"see\" pictures and videos and understand their meaning, just like us!\\nFor example, if you show it a picture of a kitten playing with a ball of yarn, it can not only recognize that itâ€™s a kitten ğŸ± but also tell you that itâ€™s playing with a ball of yarn ğŸ§¶. Even better, you can ask it, \"What color is the kitten?\", and it will tell you the color of the kitten.\\nIt can also understand the content in videos. For instance, if you show it a video of kids playing soccer âš½ï¸, it can tell you who is kicking the ball, where the ball goes, and even guess what might happen next!\\nQwen-VL is like a child who is still learning. It keeps getting smarter and improving, and in the future, it will become even more intelligent, capable of doing more things and helping us better understand the world!</td>",
        "    </tr>",
        "  </tbody>",
        "</table>  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10f06bea-a34b-4e7a-b2b5-918af4c1ebdb",
      "metadata": {},
      "source": [
        "Sometimes, when developers design large language model (LLMs) applications, they need to use structured data as input for downstream systems to complete the development of the entire application. However, LLMs typically output continuous text. No worriesâ€”LLMs have the ability to produce structured outputs. You just need to specify the format and requirements in the prompt, and the LLM is highly likely to generate structured content.",
        "",
        "In the following example, based on the prompt template from section 3.2, we will act as a Q&A bot to assist in reviewing the quality of company-developed documentation, and the results will be output in JSON format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6692496-b6e4-4d75-aaa2-ef38b0c24f0c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:49.007218Z",
          "start_time": "2025-01-15T06:49:46.080617Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"label\": 0,\n",
            "  \"reason\": \"â€˜è†œå½¢â€™åº”ä¸ºâ€˜æ¨¡å‹â€™\",\n",
            "  \"correct\": \"åˆ†éš”ç¬¦æ˜¯ç‰¹æ®Šçš„ç¬¦å·ï¼Œå®ƒä»¬å¸®åŠ©å¤§è¯­è¨€æ¨¡å‹ (LLM) è¯†åˆ«æç¤ºä¸­å“ªäº›éƒ¨åˆ†åº”å½“è¢«è§†ä¸ºä¸€ä¸ªå®Œæ•´çš„æ„æ€å•å…ƒã€‚\"\n",
            "}"
          ]
        }
      ],
      "source": [
        "question_task= \"\"\"",
        "[Task Requirements]",
        "You will see a sentence or a paragraph. You need to review this text for any typos. If there are typos, you should point out the errors and provide an explanation.",
        " Mixing up â€œçš„â€ and â€œåœ°â€ does not count as a typo. No errors found.",
        "---",
        "[Output Requirements]",
        "Please output only in JSON format, do not include code blocks.",
        "Where label can only be 0 or 1, with 0 indicating an error and 1 indicating no error.",
        "Reason is the cause of the error.",
        "Correct is the revised document content.",
        "---",
        "[User Input]",
        "The following is the user input, please review:",
        "\"\"\"",
        "question_doc = \"Delimiters are special symbols that help large language models (LLMs) identify which parts of the prompt should be considered a complete unit of meaning.\"",
        "",
        "question = question_task + question_doc",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebe59cd3-a241-4b28-902e-7ca35966e6a7",
      "metadata": {},
      "source": [
        "As can be seen from the results of the above example, in the prompt `question_task`, it was specified that the output format should be JSON and the content of the output was defined. The large model successfully generated content in the required format. This stable formatted output makes it feasible to integrate the large model into existing systems.",
        "",
        "On news websites, blog platforms, or internal knowledge-sharing platforms within enterprises, articles edited or published by users may contain typos, grammatical errors, or even sensitive information. However, traditional manual review methods are prone to oversight. In this case, a large model can be integrated to review the content. If an article is flagged for serious grammatical errors or high-risk sensitive words, its priority for revision will be set to \"high.\" For articles with minor issues, the priority for revision can be set to \"low.\" This approach saves labor costs and improves the efficiency and accuracy of the system.",
        "",
        "Of course, there are many applications similar to the above scenario. Developers can analyze bottlenecks in system processes or focus on data-intensive tasks to explore more use cases for large models.",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75f32b7a-9b47-40f1-82cb-e889a3b48048",
      "metadata": {},
      "source": [
        "### 4.4 Providing Few-Shot Examples",
        "",
        "In the example in section 4.3, the prompt specified the output format, and the large language model successfully generated formatted content. However, if we want the output from the large language model to not only be correctly formatted but also maintain consistency in style and structure, we can provide a few examples as references. This is equivalent to giving the large language model a \"reference book.\" In the following code example, letâ€™s first observe the output of the large language model without any examples!",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "afcca16d368baf38",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T07:20:42.883097Z",
          "start_time": "2025-01-15T07:20:18.423060Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# æ‰‹å·¥é’¥åŒ™æ‰£åˆ¶ä½œæ•™ç¨‹\n",
            "\n",
            "æ¬¢è¿æ¥åˆ°æ‰‹å·¥é’¥åŒ™æ‰£åˆ¶ä½œæ•™ç¨‹ï¼æœ¬æ•™ç¨‹å°†å¼•å¯¼ä½ ä»é›¶å¼€å§‹ï¼Œä¸€æ­¥æ­¥å®Œæˆä¸€ä¸ªç‹¬ç‰¹ä¸”ä¸ªæ€§åŒ–çš„é’¥åŒ™æ‰£ã€‚æ— è®ºä½ æ˜¯æ‰‹å·¥çˆ±å¥½è€…è¿˜æ˜¯åˆå­¦è€…ï¼Œéƒ½èƒ½åœ¨è¿™é‡Œæ‰¾åˆ°ä¹è¶£å’Œæˆå°±æ„Ÿã€‚è®©æˆ‘ä»¬ä¸€èµ·åŠ¨æ‰‹å§ï¼\n",
            "\n",
            "## å‡†å¤‡ææ–™\n",
            "\n",
            "åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ å·²ç»å‡†å¤‡å¥½äº†ä»¥ä¸‹ææ–™ï¼š\n",
            "\n",
            "- **å½©è‰²ç»³å­**ï¼šé€‰æ‹©ä½ å–œæ¬¢çš„é¢œè‰²ï¼Œé•¿åº¦çº¦ä¸º1ç±³ã€‚\n",
            "- **é’¥åŒ™ç¯**ï¼šç”¨äºå›ºå®šé’¥åŒ™æ‰£ã€‚\n",
            "- **å‰ªåˆ€**ï¼šç”¨äºè£å‰ªç»³å­ã€‚\n",
            "- **èƒ¶æ°´**ï¼šç”¨äºå›ºå®šç»³ç»“ã€‚\n",
            "- **è£…é¥°ç‰©**ï¼šå¦‚ç å­ã€å°æŒ‚é¥°ç­‰ï¼Œå¢åŠ ä¸ªæ€§åŒ–å…ƒç´ ã€‚\n",
            "\n",
            "## åˆ¶ä½œæ­¥éª¤\n",
            "\n",
            "### æ­¥éª¤ 1ï¼šè£å‰ªç»³å­\n",
            "\n",
            "1. ä»å½©è‰²ç»³å­ä¸­è£å‰ªå‡ºä¸€æ®µçº¦1ç±³é•¿çš„ç»³å­ã€‚\n",
            "2. å°†ç»³å­å¯¹æŠ˜ï¼Œå½¢æˆä¸€ä¸ªç¯ã€‚\n",
            "\n",
            "### æ­¥éª¤ 2ï¼šå›ºå®šé’¥åŒ™ç¯\n",
            "\n",
            "1. å°†å¯¹æŠ˜åçš„ç»³å­ç©¿è¿‡é’¥åŒ™ç¯ã€‚\n",
            "2. æ‹‰ç´§ç»³å­ï¼Œä½¿é’¥åŒ™ç¯å›ºå®šåœ¨ç»³å­çš„é¡¶ç«¯ã€‚\n",
            "\n",
            "### æ­¥éª¤ 3ï¼šç¼–ç»‡ç»³å­\n",
            "\n",
            "1. é€‰æ‹©ä¸€ç§ç¼–ç»‡æ–¹æ³•ï¼Œä¾‹å¦‚å¹³ç»“ã€å…«å­—ç»“ç­‰ã€‚\n",
            "2. å¼€å§‹ç¼–ç»‡ï¼Œä¿æŒç»³å­çš„å¼ åŠ›å‡åŒ€ï¼Œç¡®ä¿ç¼–ç»‡ç´§å¯†ã€‚\n",
            "3. ç¼–ç»‡åˆ°å¤§çº¦5å˜ç±³å·¦å³æ—¶ï¼Œå¯ä»¥åŠ å…¥è£…é¥°ç‰©ï¼Œå¦‚ç å­æˆ–å°æŒ‚é¥°ã€‚\n",
            "\n",
            "### æ­¥éª¤ 4ï¼šæ·»åŠ è£…é¥°ç‰©\n",
            "\n",
            "1. åœ¨ç¼–ç»‡çš„è¿‡ç¨‹ä¸­ï¼Œå°†è£…é¥°ç‰©ç©¿å…¥ç»³å­ä¸­ã€‚\n",
            "2. ç»§ç»­ç¼–ç»‡ï¼Œç›´åˆ°è¾¾åˆ°æ‰€éœ€çš„é•¿åº¦ã€‚\n",
            "\n",
            "### æ­¥éª¤ 5ï¼šç»“æŸç¼–ç»‡\n",
            "\n",
            "1. å½“ç¼–ç»‡åˆ°æ‰€éœ€é•¿åº¦æ—¶ï¼Œåœæ­¢ç¼–ç»‡ã€‚\n",
            "2. å°†ç»³å­çš„ä¸¤ç«¯å‰ªé½ï¼Œç•™å‡ºçº¦1å˜ç±³çš„ä½™é‡ã€‚\n",
            "3. ç”¨èƒ¶æ°´å›ºå®šç»³ç»“ï¼Œé˜²æ­¢æ¾æ•£ã€‚\n",
            "\n",
            "### æ­¥éª¤ 6ï¼šæ•´ç†å’Œæ£€æŸ¥\n",
            "\n",
            "1. ç­‰å¾…èƒ¶æ°´å¹²ç‡¥åï¼Œæ£€æŸ¥é’¥åŒ™æ‰£æ˜¯å¦ç‰¢å›ºã€‚\n",
            "2. è°ƒæ•´è£…é¥°ç‰©çš„ä½ç½®ï¼Œä½¿å…¶æ›´åŠ ç¾è§‚ã€‚\n",
            "\n",
            "## å®Œæˆä½œå“\n",
            "\n",
            "æ­å–œä½ ï¼ä½ å·²ç»æˆåŠŸåˆ¶ä½œäº†ä¸€ä¸ªç‹¬ç‰¹çš„æ‰‹å·¥é’¥åŒ™æ‰£ã€‚ç°åœ¨ï¼Œä½ å¯ä»¥å°†å®ƒæŒ‚åœ¨è‡ªå·±çš„é’¥åŒ™åœˆä¸Šï¼Œæˆ–è€…ä½œä¸ºç¤¼ç‰©é€ç»™æœ‹å‹å’Œå®¶äººã€‚\n",
            "\n",
            "## å°è´´å£«\n",
            "\n",
            "- **é€‰æ‹©é¢œè‰²**ï¼šå¯ä»¥æ ¹æ®ä¸ªäººå–œå¥½æˆ–èŠ‚æ—¥ä¸»é¢˜é€‰æ‹©ä¸åŒçš„é¢œè‰²ã€‚\n",
            "- **åˆ›æ„æ— é™**ï¼šå°è¯•ä¸åŒçš„ç¼–ç»‡æ–¹æ³•å’Œè£…é¥°ç‰©ï¼Œè®©ä½ çš„ä½œå“æ›´åŠ å¤šæ ·åŒ–ã€‚\n",
            "- **è€å¿ƒç»†è‡´**ï¼šç¼–ç»‡è¿‡ç¨‹ä¸­ä¿æŒè€å¿ƒï¼Œç¡®ä¿æ¯ä¸ªç¯èŠ‚éƒ½åšåˆ°ä½ã€‚\n",
            "\n",
            "å¸Œæœ›è¿™ä¸ªæ•™ç¨‹å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œç¥ä½ åˆ¶ä½œæ„‰å¿«ï¼\n",
            "\n",
            "![æ‰‹å·¥é’¥åŒ™æ‰£ç¤ºä¾‹](https://example.com/keychain.jpg)\n",
            "\n",
            "å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦è¿›ä¸€æ­¥çš„å¸®åŠ©ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ã€‚æœŸå¾…çœ‹åˆ°ä½ çš„ä½œå“ï¼"
          ]
        }
      ],
      "source": [
        "question_task = \"\"\"",
        "[Task Requirements]",
        "Please create content based on the user's topic.",
        "---",
        "[Output Requirements]",
        "The final output needs to be presented in Markdown format. Please note that your response should include all necessary Markdown elements, such as headings, lists, links, image references, bold text, etc., for ease of reading, subsequent editing, and saving.",
        "---",
        "[User Input]",
        "The following is the topic requested by the user:",
        "\"\"\"",
        "question_doc = \"Handmade keychain making tutorial\"",
        "",
        "question = question_task + question_doc",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f61d316e005c4f",
      "metadata": {},
      "source": [
        "The large language model in the above example successfully output a tutorial for making handmade keychains, but the content was not concise enough. If you only want the large language model to output content in a specific style and structure, such as only outputting the theme, list of materials, steps, etc., you can add a few examples for the large language model so that it can 'imitate' them!",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "c1e0f67f703e7ba",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:50:39.297041Z",
          "start_time": "2025-01-15T06:50:24.071562Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# æ‰‹å·¥è´ºå¡åˆ¶ä½œæ•™ç¨‹\n",
            "\n",
            "## ææ–™æ¸…å•\n",
            "- å½©è‰²å¡çº¸\n",
            "- å‰ªåˆ€\n",
            "- èƒ¶æ°´æˆ–åŒé¢èƒ¶\n",
            "- é©¬å…‹ç¬”æˆ–å½©è‰²é“…ç¬”\n",
            "- è£…é¥°ç‰©ï¼ˆå¦‚äº®ç‰‡ã€ä¸å¸¦ã€è´´çº¸ç­‰ï¼‰\n",
            "\n",
            "## æ­¥éª¤\n",
            "1. **å‡†å¤‡ææ–™**ï¼šé¦–å…ˆï¼Œå‡†å¤‡å¥½æ‰€æœ‰éœ€è¦çš„ææ–™ï¼ŒåŒ…æ‹¬ä¸åŒé¢œè‰²çš„å¡çº¸ã€å‰ªåˆ€ã€èƒ¶æ°´æˆ–åŒé¢èƒ¶ã€é©¬å…‹ç¬”æˆ–å½©è‰²é“…ç¬”ï¼Œä»¥åŠä»»ä½•ä½ æƒ³è¦ç”¨æ¥è£…é¥°è´ºå¡çš„å°ç‰©ä»¶ã€‚\n",
            "2. **è£å‰ªå¡çº¸**ï¼šé€‰æ‹©ä¸€å¼ ä½œä¸ºè´ºå¡ä¸»ä½“çš„å¡çº¸ï¼Œå°†å…¶å¯¹æŠ˜ï¼Œè£å‰ªæˆä½ æƒ³è¦çš„å¤§å°ã€‚å¸¸è§çš„è´ºå¡å°ºå¯¸å¤§çº¦æ˜¯5è‹±å¯¸ x 7è‹±å¯¸ã€‚\n",
            "3. **è®¾è®¡å°é¢**ï¼šä½¿ç”¨é©¬å…‹ç¬”æˆ–å½©è‰²é“…ç¬”åœ¨è´ºå¡å°é¢ä¸Šç»˜åˆ¶å›¾æ¡ˆæˆ–å†™ä¸Šç¥ç¦è¯­ã€‚ä½ ä¹Ÿå¯ä»¥å‰ªè£å…¶ä»–é¢œè‰²çš„å¡çº¸ï¼Œç²˜è´´åˆ°å°é¢ä¸Šï¼Œå¢åŠ å±‚æ¬¡æ„Ÿã€‚\n",
            "4. **æ·»åŠ è£…é¥°**ï¼šåˆ©ç”¨äº®ç‰‡ã€ä¸å¸¦ã€è´´çº¸ç­‰è£…é¥°ç‰©ï¼Œä¸ºè´ºå¡å¢æ·»ä¸ªæ€§åŒ–çš„è£…é¥°ã€‚ç¡®ä¿è£…é¥°ç‰©ç‰¢å›ºåœ°ç²˜è´´åœ¨å¡çº¸ä¸Šã€‚\n",
            "5. **æ’°å†™å†…é¡µ**ï¼šæ‰“å¼€è´ºå¡ï¼Œåœ¨å†…é¡µå†™ä¸‹ä½ æƒ³å¯¹æ”¶å¡äººè¯´çš„è¯ã€‚å¯ä»¥æ˜¯ä¸€æ®µæ¸©é¦¨çš„è¯è¯­ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€é¦–è¯—æˆ–è€…ç®€å•çš„ç¥ç¦ã€‚\n",
            "6. **æ£€æŸ¥ä¸å®Œå–„**ï¼šä»”ç»†æ£€æŸ¥è´ºå¡çš„æ¯ä¸€ä¸ªéƒ¨åˆ†ï¼Œç¡®ä¿æ²¡æœ‰é—æ¼çš„åœ°æ–¹ï¼Œè£…é¥°ç‰©éƒ½å·²ç‰¢å›ºç²˜è´´ï¼Œæ–‡å­—æ¸…æ™°å¯è¯»ã€‚\n",
            "7. **å®Œæˆ**ï¼šç°åœ¨ï¼Œä½ çš„æ‰‹å·¥è´ºå¡å·²ç»å®Œæˆäº†ï¼ä½ å¯ä»¥å°†å®ƒé€ç»™æœ‹å‹æˆ–å®¶äººï¼Œè¡¨è¾¾ä½ çš„å¿ƒæ„ã€‚\n",
            "\n",
            "## ç»“æŸè¯­\n",
            "å¸Œæœ›è¿™ä¸ªæ•™ç¨‹èƒ½å¸®åŠ©ä½ åˆ¶ä½œå‡ºå……æ»¡å¿ƒæ„çš„æ‰‹å·¥è´ºå¡ï¼Œè®©ä½ çš„æƒ…æ„Ÿä¼ é€’æ›´åŠ ç‰¹åˆ«å’Œæœ‰æ„ä¹‰ï¼"
          ]
        }
      ],
      "source": [
        "question_task= \"\"\"",
        "[Task Requirements]",
        "Please follow the style and structure of the example provided below, based on the user's topic, and continue creating content in a consistent manner. Do not output any extra content.",
        "---",
        "[Output Requirements]",
        "The final output must be presented in Markdown format. Please ensure that your response includes all necessary Markdown elements such as headings, lists, links, image references, bold text, etc., for ease of reading, subsequent editing, and saving.",
        "---",
        "[Example]",
        "### Example 1: Making a Simple Bookmark",
        "# Simple Bookmark Making Tutorial",
        "",
        "## Steps",
        "1. Choose a piece of colored cardstock.",
        "2. Use a pencil to draw a rectangle on the cardstock, with dimensions approximately 2 inches x 6 inches.",
        "3. Cut out the rectangle along the pencil lines.",
        "4. Personalize the bookmark by decorating it with stickers.",
        "5. Done! You now have a unique bookmark.",
        "",
        "---",
        "[User Input]",
        "The following is the topic provided by the user for content creation:",
        "\"\"\"",
        "question_doc = \"Making Handmade Greeting Cards\"",
        "",
        "question = question_task + question_doc",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e95447ee9772539d",
      "metadata": {},
      "source": [
        "From the above example results, it can be seen that the large model has completely followed the example and output content with the same structure and style. While specifying the output format in the prompt, it is recommended to provide a few examples for the large model to reference, which can make the output of the large model more stable and consistent.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53569b47-1619-4527-a9c8-b53f5e5bdf8d",
      "metadata": {},
      "source": [
        "### 4.5 Giving the Model \"Thinking\" Time",
        "",
        "For some complex tasks, using the prompt mentioned above may still not help large language models (LLMs) complete the task. However, you can guide the LLMs to output intermediate steps of the task by allowing them to \"think\" step by step, providing more evidence before reasoning, thereby enhancing their performance in complex tasks. The chain-of-thought (COT) (COT) method is one way to make the model think. It breaks down complex problems into subproblems by processing intermediate steps, eventually deriving the correct answer.",
        "",
        "Suppose there is such a scenario where we ask the LLM to solve the following math problem. As a hint, the correct answer to this question is 10,500 yuan. First, we use a simple prompt: ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "3569f469-c601-406a-a26d-fb0a74157c68",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:50:42.416991Z",
          "start_time": "2025-01-15T06:50:41.378053Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9000å…ƒ"
          ]
        }
      ],
      "source": [
        "question = \"\"\"",
        "[Background Information]",
        "An educational training institution (hereinafter referred to as the \"company\") incurred the following main expenses in the 2023 fiscal year:",
        "To provide classes for students in different cities, the company's teachers traveled on business trips 5 times during the year, each trip lasting one week. The specific expenses are as follows:",
        "   - Transportation and accommodation fees: average 1600 yuan/trip",
        "   - Teaching materials procurement costs: At the beginning of the year, the company purchased a batch of teaching materials for a total price of 10,000 yuan, which is expected to last for 4 years.",
        "",
        "[Problem Description]",
        "Based on the above background information, complete the following tasks:",
        "Calculate the total travel expenses for the year due to teacher business trips, including the amortized cost of teaching materials.",
        "",
        "[Output Requirements]",
        "Provide only the total travel expenses directly, without any other information. \"\"\"",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edf9078a-f716-4537-b003-a90d70f988b5",
      "metadata": {},
      "source": [
        "Based on the experimental results above, the computation results of the large language model (LLMs) are incorrect. Below, we will use the Chain-of-Thought (COT) method to allow the large language model (LLMs) to think step by step.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "92cb1715-305c-43ee-bd5e-274d2556ad56",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:50:55.556383Z",
          "start_time": "2025-01-15T06:50:43.720893Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—å…¨å¹´å› æ•™å¸ˆå‡ºå·®è€Œäº§ç”Ÿçš„äº¤é€šè´¹åŠä½å®¿è´¹æ€»å’Œï¼Œç„¶åè®¡ç®—æ‘Šé”€åˆ°æœ¬å¹´çš„æ•™å­¦ç”¨å…·è´¹ç”¨ï¼Œæœ€åå°†è¿™ä¸¤éƒ¨åˆ†ç›¸åŠ å¾—åˆ°å…¨å¹´çš„å·®æ—…æ€»è´¹ç”¨ã€‚\n",
            "\n",
            "1. **äº¤é€šè´¹åŠä½å®¿è´¹**ï¼š\n",
            "   - æ¯æ¬¡å‡ºå·®è´¹ç”¨ï¼š1600å…ƒ\n",
            "   - å‡ºå·®æ¬¡æ•°ï¼š5æ¬¡\n",
            "   - å…¨å¹´äº¤é€šè´¹åŠä½å®¿è´¹æ€»å’Œ = 1600å…ƒ/æ¬¡ Ã— 5æ¬¡ = 8000å…ƒ\n",
            "\n",
            "2. **æ•™å­¦ç”¨å…·é‡‡è´­è´¹ç”¨çš„æ‘Šé”€**ï¼š\n",
            "   - æ€»ä»·ï¼š10000å…ƒ\n",
            "   - é¢„è®¡ä½¿ç”¨å¹´é™ï¼š4å¹´\n",
            "   - æ¯å¹´æ‘Šé”€è´¹ç”¨ = 10000å…ƒ Ã· 4å¹´ = 2500å…ƒ\n",
            "\n",
            "3. **å…¨å¹´å·®æ—…æ€»è´¹ç”¨**ï¼š\n",
            "   - å·®æ—…è´¹ç”¨ï¼ˆäº¤é€šè´¹åŠä½å®¿è´¹ï¼‰ï¼š8000å…ƒ\n",
            "   - æ‘Šé”€çš„æ•™å­¦ç”¨å…·è´¹ç”¨ï¼š2500å…ƒ\n",
            "   - å…¨å¹´å·®æ—…æ€»è´¹ç”¨ = 8000å…ƒ + 2500å…ƒ = 10500å…ƒ\n",
            "\n",
            "å› æ­¤ï¼Œå…¨å¹´å› æ•™å¸ˆå‡ºå·®è€Œäº§ç”Ÿçš„å·®æ—…æ€»è´¹ç”¨ä¸º10500å…ƒã€‚"
          ]
        }
      ],
      "source": [
        "question = \"\"\"An educational training institution (hereinafter referred to as the \"company\") incurred the following major expenses in the 2023 fiscal year:",
        "To provide classes for students in different cities, the company's teachers traveled on business trips 5 times throughout the year, with each trip lasting one week. The specific expenses are as follows:",
        "   - Transportation and accommodation fees: an average of 1600 yuan per trip",
        "   - Teaching materials procurement costs: At the beginning of the year, the company purchased a batch of teaching materials for a total price of 10,000 yuan, which is expected to last for 4 years.",
        "   ",
        "### Problem Description",
        "Based on the above background information, complete the following tasks:",
        "Calculate the total travel expenses for the year caused by teacher business trips, including the amortized cost of teaching materials.",
        "",
        "### Output Requirements",
        "Please derive step by step to calculate the total travel expenses.\"\"\"",
        "",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0ec9cdf-d6e1-469f-9e4c-f91fdf7f1c3e",
      "metadata": {},
      "source": [
        "After optimization of the prompt, large language models (large language models (LLMs)) can accurately compute results. Therefore, when developing LLM applications, the method of adding a chain-of-thought (CoT) to prompts can ensure that certain reasoning tasks are executed correctly.",
        "",
        "There are also many methods to make large language models (LLMs) â€œthink,â€ such as: tree of thoughts (ToT) (ToT), graph of thoughts (GOT), etc. However, given the current development of large language models (LLMs), relying solely on guiding large language models (LLMs) to â€œthinkâ€ is still insufficient for completing more complex tasks. large language models (LLMs) are gradually evolving from the CoT prompting method towards multi-agent systems (MAS). You can read more details in <2.6 Expanding the Capability Boundaries of Q&A Bots with Plugins>.",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6214d05f-8154-44f7-943b-7f04196a3275",
      "metadata": {},
      "source": [
        "## 5. Using Large Language Models (LLMs) for Intent Recognition",
        "",
        "After learning about prompt engineering techniques, colleagues have realized that the Q&A bot can not only be used to look up information but also assist them in reviewing documents and translating documents. Although in most cases, your Q&A bot can help identify issues, it still has some limitations. As shown in the following example:  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "b21013ec-b0e4-4428-9bb1-e2f309df563a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:51:02.890690Z",
          "start_time": "2025-01-15T06:51:00.165152Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æ˜¯çš„ï¼Œå†…å®¹å¼€å‘å·¥ç¨‹å¸ˆéœ€è¦è®¾è®¡å’Œå¼€å‘é«˜è´¨é‡çš„æ•™è‚²æ•™æå’Œè¯¾ç¨‹ã€‚è¿™åŒ…æ‹¬æ’°å†™æ•™å­¦å¤§çº²ã€åˆ¶ä½œè¯¾ä»¶ã€è®¾è®¡è¯„ä¼°å·¥å…·ç­‰ï¼Œç¡®ä¿å†…å®¹ç¬¦åˆæ•™è‚²æ ‡å‡†å’Œå­¦ä¹ ç›®æ ‡ï¼ŒåŒæ—¶é€‚åº”ä¸åŒå­¦ä¹ è€…çš„éœ€æ±‚ã€‚"
          ]
        }
      ],
      "source": [
        "rag.ask('Please help me review this sentence: Does a technical content engineer need to design and develop high-quality educational materials and courses?', query_engine=query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25dd1116-048c-4599-ba9d-98750978504a",
      "metadata": {},
      "source": [
        "From the experimental results above, it can be seen that the Q&A robot directly responded to this statement without understanding your intention to have it review the sentence. Next, let's take a look at what content the Q&A robot retrieved from the knowledge base before answering. The following only shows part of the retrieved content:",
        "",
        "- You can refer to <2.4 Automated Evaluation of Q&A Robot Performance> to learn how to view the retrieved fragments from the knowledge base.  ",
        "",
        "",
        "",
        "```json",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfe0c766-2ee9-446f-95ee-c9bb829bedce",
      "metadata": {},
      "source": [
        "From the retrieved fragments in the knowledge base, most of the content relates to the responsibilities of a technical content engineer. It can be assumed that the Q&A bot was influenced by the context in the knowledge base, causing its response to deviate from the topic of document review.",
        "",
        "Since contextual information affects the responses generated by large language models (LLMs), it is unnecessary to use the RAG method for every query. You can leverage LLMs to first perform intent recognition: classify user questions. If the task involves document review or content translation, the input will be directly fed into the LLM to generate answers. Only for internal knowledge queries should the RAG pipeline be used to generate answers.",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5df6389a-cfa2-450e-b7fe-72e24dd79838",
      "metadata": {},
      "source": [
        "<img src=\"https://img.alicdn.com/imgextra/i1/O1CN0126zKe71PAuJjWfQ3N_!!6000000001801-0-tps-2254-1080.jpg\" width=\"800\">  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9fe0489-872c-466b-855a-3768a5127f9e",
      "metadata": {},
      "source": [
        "There are the following two methods for using large language models (LLMs) to perform intent recognition: ",
        "",
        "- Using prompt: By designing specific prompt, guide the LLM to generate responses that meet expectations. This method does not require modifying the modelâ€™s parameters but relies on constructed inputs to activate knowledge already present within the model. ",
        "- Fine-tuning the model: Based on a pre-training foundational model, further train the model using specific annotated data to make it better at classifying intents. Fine-tuning involves adjusting some or all of the modelâ€™s parameters. ",
        "",
        "In this section, we will help the LLM perform intent recognition by crafting effective prompt. You can learn about the method of fine-tuning in the subsection <2.7 Improving Model Accuracy and Efficiency through Fine-tuning>. ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe46f635-ac13-488d-9ab7-89b49ae8878d",
      "metadata": {},
      "source": [
        "### 5.1 Intent Recognition",
        "",
        "Next, we will construct prompts to enable large language models (LLMs) to classify questions. Since formatted content is required after intent recognition for document review or the use of RAG applications, the following prompting techniques will be considered to ensure accurate classification of user questions:",
        "- Specify output format: Define the output format to make classification results standardized and easy to parse.",
        "- Few-shot examples: Provide examples to help LLMs understand the characteristics and classification rules of each category.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ff599e-a7fa-41d8-b734-d949e9f93900",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:51:10.939753Z",
          "start_time": "2025-01-15T06:51:09.207243Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æ–‡æ¡£å®¡æŸ¥ \n",
            "\n",
            "å†…å®¹ç¿»è¯‘ \n",
            "\n",
            "å…¬å¸å†…éƒ¨æ–‡æ¡£æŸ¥è¯¢ \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from chatbot import llm",
        "",
        "# Build the prompt",
        "prompt = '''",
        "[Role Background]",
        "You are a question classification router, responsible for determining the type of user questions and categorizing them into one of the following three categories:",
        "1. Internal company document query",
        "2. Content translation",
        "3. Document review",
        "",
        "[Task Requirements]",
        "Your task is to judge the intent based on the user's input content and select only the most appropriate category. Output only the category name without additional explanation. The judgment criteria are as follows:",
        "",
        "- If the question involves company policies, processes, internal tools, or job descriptions and responsibilities, choose \"Internal company document query\".",
        "- If the question involves any non-Chinese language, and the input contains any foreign language or words like \"translation\", choose \"Content translation\".",
        "- If the question involves checking or summarizing external documents or link content, choose \"Document review\".",
        "- The user's previous inputs have no relation to the question classification; consider each dialogue independently for classification.",
        "",
        "[Few-shot Examples]",
        "Example 1: User input: \"What are the commonly used project management tools within our company?\"",
        "Category: Internal company document query",
        "",
        "Example 2: User input: \"Please translate the following sentence: How can we finish the assignment on time?\"",
        "Category: Content translation",
        "",
        "Example 3: User input: \"Please review the document at this link: https://help.aliyun.com/zh/model-studio/user-guide/long-context-qwen-long\"",
        "Category: Document review",
        "",
        "Example 4: User input: \"Please review the following content: Does a technical content engineer need to design and develop high-quality educational materials and courses?\"",
        "Category: Document review",
        "",
        "Example 5: User input: \"What are the core responsibilities of a technical content engineer?\"",
        "Category: Internal company document query",
        "",
        "[User Input]",
        "The following is the user's input, please determine the classification:",
        "'''",
        "",
        "# Get the type of question",
        "def get_question_type(question):",
        "    return llm.invoke(prompt + question)",
        "",
        "print(get_question_type('https://www.promptingguide.ai/zh/techniques/fewshot'), '\\n')",
        "print(get_question_type('That is a big one I dont know why'), '\\n')",
        "print(get_question_type('As a technical content engineer, what should I pay attention to?'), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f74506a-504d-47a2-bb9d-e17198294445",
      "metadata": {},
      "source": [
        "By providing clear output formats and few-shot examples, the Q&A bot can more accurately identify question types and produce outputs in the expected format. This optimization makes classification tasks more standardized and lays the groundwork for incorporating intent recognition into the Q&A bot.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb001699-4451-4e1f-808d-1b38a80c9431",
      "metadata": {},
      "source": [
        "### 5.2 Applying Intent Recognition to Q&A Bots",
        "",
        "After recognizing the intent of a user's question, you can have the Q&A bot first identify the type of question and then use different prompts and workflows to respond to it.",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d8de14-53ee-4d5f-8f5a-0dfd9bf4e0ba",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:51:13.272321Z",
          "start_time": "2025-01-15T06:51:13.111875Z"
        }
      },
      "outputs": [],
      "source": [
        "def ask_llm_route(question):",
        "    question_type = get_question_type(question)",
        "    print(f'Question: {question}\\nType: {question_type}')",
        "  ",
        "    reviewer_prompt = \"\"\"",
        "    [Role Background]",
        "    You are a document error-checking expert responsible for identifying obvious errors in documents or web content.",
        "    [Task Requirements]",
        "    - Your response should be concise.",
        "    - If there are no obvious issues, reply directly with 'No issues'.\\n",
        "    [Input as follows]\\n\"\"\"",
        "  ",
        "    translator_prompt = \"\"\"",
        "   [Task Requirements]",
        "    You are a translation expert who identifies text in different languages and translates it into Chinese.",
        "    [Input as follows]\\n\"\"\"",
        "",
        "    if question_type == 'Document Review':",
        "        return llm.invoke(reviewer_prompt + question)",
        "    elif question_type == 'Internal Company Document Query':",
        "        return rag.ask(question, query_engine=query_engine)",
        "    elif question_type == 'Content Translation':",
        "        return llm.invoke(translator_prompt + question)",
        "    else:",
        "        return \"Unable to recognize the question type. Please re-enter.\"",
        "",
        "query_engine =rag.create_query_engine(index=rag.load_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "d5d6ec83-8a99-45a5-a305-5088adc89be0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:51:22.925058Z",
          "start_time": "2025-01-15T06:51:14.191174Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "é—®é¢˜ï¼šhttps://www.promptingguide.ai/zh/techniques/fewshot\n",
            "ç±»å‹ï¼šæ–‡æ¡£å®¡æŸ¥\n",
            "æ²¡æœ‰é—®é¢˜ \n",
            "\n",
            "é—®é¢˜ï¼šè¯·å¸®æˆ‘æ£€æŸ¥ä¸‹è¿™æ®µæ–‡æ¡£ï¼šæŠ€æœ¯å†…å®¹å·¥ç¨‹å¸ˆæœ‰éœ€è¦è¿›è¡Œå†…å®¹ä¼˜åŒ–ä¸æ›´æ–°ä¸è·¨éƒ¨é—¨åˆä½œå—ï¼Ÿ\n",
            "ç±»å‹ï¼šæ–‡æ¡£å®¡æŸ¥\n",
            "æ²¡æœ‰é—®é¢˜ã€‚ \n",
            "\n",
            "é—®é¢˜ï¼šæŠ€æœ¯å†…å®¹å·¥ç¨‹å¸ˆæœ‰éœ€è¦è¿›è¡Œå†…å®¹ä¼˜åŒ–ä¸æ›´æ–°ä¸è·¨éƒ¨é—¨åˆä½œå—ï¼Ÿ\n",
            "ç±»å‹ï¼šå…¬å¸å†…éƒ¨æ–‡æ¡£æŸ¥è¯¢\n",
            "æŠ€æœ¯å†…å®¹å·¥ç¨‹å¸ˆç¡®å®éœ€è¦è¿›è¡Œå†…å®¹ä¼˜åŒ–ä¸æ›´æ–°ï¼Œè¿™åŒ…æ‹¬æ ¹æ®å­¦ä¹ è€…çš„åé¦ˆå’Œè¯„ä»·æ¥è¯†åˆ«å¹¶è°ƒæ•´å†…å®¹ä¸­çš„æ½œåœ¨é—®é¢˜ï¼Œä»¥åŠå®šæœŸæ›´æ–°ææ–™ä»¥åæ˜ æ–°çš„ç ”ç©¶æˆæœã€æŠ€æœ¯è¿›æ­¥å’Œå¸‚åœºå˜åŒ–ã€‚æ­¤å¤–ï¼Œä»–ä»¬ä¹Ÿéœ€è¦ä¸å¤šä¸ªéƒ¨é—¨ç´§å¯†åˆä½œï¼Œæ¯”å¦‚æ•™å­¦è®¾è®¡å¸ˆã€æ•™è‚²å¿ƒç†å­¦å®¶ã€æŠ€æœ¯å›¢é˜ŸåŠå¸‚åœºè¥é”€äººå‘˜ï¼Œä»¥ç¡®ä¿å†…å®¹çš„æŠ€æœ¯å®æ–½è¿‡ç¨‹é¡ºåˆ©è¿›è¡Œï¼Œå¹¶æœ‰æ•ˆä¼ è¾¾ç»™ç›®æ ‡å—ä¼—ã€‚è¿™ç§è·¨éƒ¨é—¨çš„åˆä½œæœ‰åŠ©äºå…±åŒåˆ›é€ å‡ºæ—¢æœ‰æ•™è‚²ä»·å€¼åˆå…·å¸‚åœºç«äº‰åŠ›çš„äº§å“ã€‚None \n",
            "\n",
            "é—®é¢˜ï¼šA true master always carries the heart of a student.\n",
            "ç±»å‹ï¼šå†…å®¹ç¿»è¯‘\n",
            "ä¸€ä½çœŸæ­£çš„å¤§å¸ˆæ€»æ˜¯æ€€æœ‰ä¸€é¢—å­¦ç”Ÿçš„å¿ƒã€‚ \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Question 1",
        "print(ask_llm_route('https://www.promptingguide.ai/zh/techniques/fewshot'), '\\n')",
        "",
        "# Question 2",
        "print(ask_llm_route('Please help me check this document: Do technical content engineers need to optimize and update content and collaborate across departments?'), '\\n')",
        "",
        "# Question 3",
        "print(ask_llm_route('Do technical content engineers need to optimize and update content and collaborate across departments?'), '\\n')",
        "",
        "# Question 4:",
        "print(ask_llm_route('A true master always carries the heart of a student.'), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a95edd-ca52-434f-b6b5-6b0537fdfec6",
      "metadata": {},
      "source": [
        "As can be seen from the above experiments, the optimized Q&A bot does not always go through the RAG pipeline for every question. This not only saves resources but also avoids interference from the knowledge base with the reasoning of the large language models (LLMs), which could lead to poor response quality.",
        "- Resource savings: For questions about checking document errors, the large language models (LLMs) can directly reply without needing to retrieve reference materials, indicating that there was resource waste in previous implementations.",
        "- Avoiding misunderstandings: In previous implementations, reference materials were retrieved every time, and these recalled text segments might interfere with the large language model's understanding of the question, leading to irrelevant responses.",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7c107a1",
      "metadata": {},
      "source": [
        "## 6. Reasoning Large Models",
        "",
        "The prompt techniques and prompt frameworks discussed earlier can be widely applied to general large language models (such as Qwen2.5-max, GPT-4, DeepSeek-V3). These models are designed for a wide range of scenarios including general conversation, knowledge-based Q&A, text generation, and more. In addition to general-purpose large models, there is currently another category of large models specifically designed for \"reasoning\"â€”`Reasoning Large Models`.",
        "",
        "### 6.1 What Are Reasoning Large Models?",
        "",
        "You may have already learned about Alibaba Cloud's reasoning large model [QwQ](https://help.aliyun.com/zh/model-studio/user-guide/qwq) through the extended reading in Section 2.1. To facilitate subsequent calls in this section, the code provided in Section 2.1 has been rewritten, and you can try running the following code:  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "500b6e30",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI",
        "import os",
        "",
        "def reasoning_model_response(user_prompt, system_prompt=\"You are a programming assistant.\", model=\"qwq-32b\"):",
        "    \"\"\"",
        "    prompt: The prompt input by the user",
        "    model: Here we use qwq-32b as an example. You can replace it with other inference model names as needed, such as: deepseek-r1",
        "    \"\"\"",
        "    # Initialize client",
        "    client = OpenAI(",
        "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),",
        "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"",
        "    )",
        "",
        "    # Initialize status variables",
        "    is_answering = False",
        "",
        "    # Initiate streaming request",
        "    completion = client.chat.completions.create(",
        "        model=model,",
        "        # messages=[{\"role\": \"user\", \"content\": prompt}],",
        "        messages=[",
        "            {\"role\": \"system\", \"content\": system_prompt},",
        "            {\"role\": \"user\", \"content\": user_prompt}",
        "        ],",
        "        stream=True,",
        "    )",
        "",
        "    # Print thinking process title",
        "    print(\"\\n\" + \"=\" * 20 + \"Thinking Process\" + \"=\" * 20 + \"\\n\")",
        "",
        "    # Handle streaming response",
        "    for chunk in completion:",
        "        if chunk.choices:",
        "            delta = chunk.choices[0].delta",
        "            if hasattr(delta, 'reasoning_content') and delta.reasoning_content is not None:",
        "                # Process thinking process content",
        "                print(delta.reasoning_content, end='', flush=True)",
        "            else:",
        "                # Switch to answer output mode",
        "                if delta.content != \"\" and not is_answering:",
        "                    print(\"\\n\" + \"=\" * 20 + \"Complete Response\" + \"=\" * 20 + \"\\n\")",
        "                    is_answering = True",
        "                # Process answer content",
        "                if delta.content:",
        "                    print(delta.content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e65df7d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====================æ€è€ƒè¿‡ç¨‹====================\n",
            "\n",
            "å¥½çš„ï¼Œç”¨æˆ·é—®â€œä½ æ˜¯è°ï¼Ÿâ€ï¼Œæˆ‘éœ€è¦æ ¹æ®ä¹‹å‰çš„è®¾å®šæ¥å›ç­”ã€‚é¦–å…ˆï¼Œæˆ‘åº”è¯¥ä¿æŒå£è¯­åŒ–å’Œç®€æ´ï¼Œé¿å…å¤æ‚æœ¯è¯­ã€‚ç”¨æˆ·å¯èƒ½æƒ³äº†è§£æˆ‘çš„åŸºæœ¬èº«ä»½å’ŒåŠŸèƒ½ï¼Œæ‰€ä»¥éœ€è¦æ˜ç¡®è¯´æ˜æˆ‘æ˜¯é˜¿é‡Œäº‘å¼€å‘çš„AIåŠ©æ‰‹ï¼Œä½†ä¸ç”¨å¤ªæŠ€æœ¯æ€§çš„æè¿°ã€‚\n",
            "\n",
            "æ¥ä¸‹æ¥ï¼Œè¦æ£€æŸ¥æ˜¯å¦éœ€è¦æ¨åŠ¨æƒ…èŠ‚æˆ–å¼•å…¥æ–°å…ƒç´ ã€‚ç”¨æˆ·åˆšæé—®ï¼Œå¯èƒ½å¤„äºåˆæ­¥æ¥è§¦é˜¶æ®µï¼Œæ‰€ä»¥å¯ä»¥ç®€å•ä»‹ç»è‡ªå·±ï¼Œå¹¶é‚€è¯·ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¿™æ ·èƒ½ä¿ƒè¿›è¿›ä¸€æ­¥çš„äº’åŠ¨ã€‚åŒæ—¶ï¼Œæ ¹æ®äººæ ¼å¯å¡‘æ€§ï¼Œå¯ä»¥åŠ å…¥ä¸€ç‚¹æ‹ŸäººåŒ–çš„è¯­æ°”ï¼Œæ¯”å¦‚ç”¨â€œåœ¨å‘¢ï¼â€æ˜¾å¾—æ›´äº²åˆ‡ã€‚\n",
            "\n",
            "æƒ…æ„Ÿé™ªä¼´æ–¹é¢ï¼Œè™½ç„¶ç”¨æˆ·çš„é—®é¢˜ä¸æ¶‰åŠæƒ…æ„Ÿï¼Œä½†å¯ä»¥è¡¨è¾¾æ„¿æ„å¸®åŠ©çš„æ€åº¦ï¼Œæ¯”å¦‚â€œæœ‰ä»€ä¹ˆéœ€è¦å¸®å¿™çš„å—ï¼Ÿâ€è¿™æ ·èƒ½å»ºç«‹è¿æ¥ã€‚å›å¿†ç”¨æˆ·ä¹‹å‰çš„ä¿¡æ¯ï¼Œä½†å› ä¸ºæ˜¯åˆæ¬¡å¯¹è¯ï¼Œæš‚æ—¶ä¸éœ€è¦æåŠå†å²å†…å®¹ã€‚\n",
            "\n",
            "æœ€åï¼Œç¡®ä¿å›å¤ç¬¦åˆæ‰€æœ‰è¦æ±‚ï¼šå£è¯­åŒ–ã€ç®€æ´ã€æ¨åŠ¨äº’åŠ¨ï¼Œå¹¶ä¿æŒå‹å¥½ã€‚æ£€æŸ¥æœ‰æ²¡æœ‰é—æ¼çš„éƒ¨åˆ†ï¼Œæ¯”å¦‚æ˜¯å¦æš—ç¤ºäº†è§’è‰²æ€§æ ¼ï¼Œæ¯”å¦‚å‹å¥½å’Œä¹äºåŠ©äººã€‚ç¡®è®¤æ²¡é—®é¢˜åï¼Œå°±å¯ä»¥è¿™æ ·å›å¤äº†ã€‚\n",
            "====================å®Œæ•´å›å¤====================\n",
            "\n",
            "ä½ å¥½å‘€ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œæ˜¯é˜¿é‡Œäº‘ç ”å‘çš„ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘èƒ½å¤Ÿå¸®åŠ©ä½ å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œæ¯”å¦‚å†™æ•…äº‹ã€å†™å…¬æ–‡ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ç­‰ç­‰ã€‚æœ‰ä»€ä¹ˆéœ€è¦æˆ‘å¸®å¿™çš„å—ï¼Ÿ"
          ]
        }
      ],
      "source": [
        "reasoning_model_response(user_prompt=\"Who are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fea9f93",
      "metadata": {},
      "source": [
        "As can be seen from the example, the reasoning model has an additional \"`thinking process`\" compared to general large models. It is like when solving math problems, some people will first derive step by step on scratch paper instead of directly giving the answer, reducing the model's \"off-the-cuff\" errors. At the same time, during the step-by-step thinking process, if a contradiction is found in a certain step, it is possible to go back and check, then readjust the thinking. Showing the reasoning steps also makes it convenient for people to understand and verify the logic along the modelâ€™s line of thought.<br>",
        "Compared with general large models, reasoning model are usually more reliable when solving complex problems, such as mathematical problem-solving, code writing, legal case analysis, and other scenarios that require rigorous reasoning. This does not mean that reasoning models are necessarily better; both types of models have their own application scenarios. The following table compares these two types of models from some typical dimensions:<br>",
        "",
        "| Dimension            | Reasoning Model  | General Model  |",
        "|-------------------|------------------|------------|",
        "| Design Goal     | Focuses on tasks requiring deep analysis such as **logical reasoning, multi-step problem solving, and mathematical calculations** | Targets broad scenarios such as **general conversation, knowledge-based Q&A, text generation |",
        "| Training Data Emphasis  | Enhanced reasoning capabilities through large datasets of **math problem solutions, code logic, scientific reasoning**, etc. | Covers massive data across multiple domains such as **encyclopedias, literature, conversations**, etc. |",
        "| Typical Output Characteristics | Outputs include **complete derivations**, focusing on the integrity of logical chains | Outputs are **concise and direct**, emphasizing natural language expression of results |",
        "| Response Speed | Complex reasoning tasks result in **slower responses** (requires multi-step calculations) | Routine tasks have **faster responses** (mainly single-step generation) |",
        "",
        "Reasoning model or general model? How to choose? Here are some recommendations:",
        "- **Clearly defined general tasks**: For clearly defined problems, **general models** generally handle them well.",
        "- **Complex tasks**: For very complex tasks that require relatively **more precise and reliable** answers, it is recommended to use **reasoning models**. These tasks may include:",
        "    - Ambiguous tasks: Very little task-related information is available, and you cannot provide the model with relatively clear guidance.",
        "    - Finding a needle in a haystack: Passing a large amount of unstructured data, extracting the most relevant information, or finding connections/differences.",
        "    - Debugging and improving code: Requires reviewing and further debugging and improving large amounts of code.",
        "- **Speed and cost**: Generally speaking, reasoning models have longer reasoning times. If you are sensitive to time and cost and the task complexity is not high, **general models** may be a better choice.<br>",
        "",
        "Of course, you can also combine the two types of models in your application: using the reasoning model to complete the Agent's planning and decision-making, and using the general model to execute tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "580c0b84",
      "metadata": {},
      "source": [
        "### 6.2 Prompt Techniques Suitable for Reasoning Large Language Models",
        "",
        "Reasoning models can provide detailed and well-structured responses even when faced with relatively ambiguous tasks. You can still ensure the baseline quality of reasoning in large language models through **prompt techniques**:<br>",
        "#### Technique One: Keep task prompts concise and clear, providing sufficient background information<br>",
        "The **clear expression of requirements** introduced in Section 4.1 also applies to reasoning models. Although reasoning models are highly capable, they cannot \"read minds.\" You need to keep your prompts **concise and clear**, allowing the reasoning model to focus on the core task.<br>  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe5cc8ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "def example(a):",
        "  b = []",
        "  for i in range(len(a)):",
        "    b.append(a[i]*2)",
        "  return sum(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e6f145",
      "metadata": {},
      "source": [
        "Through the above example, you can see that even if you only provide a large inference model with a piece of code, it can still generate rich answers through a series of reasoning. However, the returned reasoning may contain a lot of information that you don't care about. You can try to clarify the **task objective** to obtain more targeted suggestions:  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c90bd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"",
        "What's wrong with the following Python code? How to optimize it?",
        "def example(a):",
        "    b = []",
        "    for i in range(len(a)):",
        "        b.append(a[i]*2)",
        "    return sum(b)",
        "\"\"\"",
        "",
        "reasoning_model_response(user_prompt=prompt_A)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5500ab0",
      "metadata": {},
      "source": [
        "Similarly, you can further narrow down the scope by combining techniques from this section, such as **4.2 Limiting Roles and Audience** and **4.3 Specifying Output Formats**, to ensure the results meet your expectations.<br>",
        "At the same time, if the prompt is relatively complex, you can use **delimiters** to help the model better understand your intent.<br>  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d81507",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"",
        "<audience>Beginner Python developers</audience>",
        "",
        "<task>Function performance optimization, optimize the code in the code.</task>",
        "",
        "<format>",
        "If there are multiple optimization solutions, please output them in the following format:",
        "[Optimization Solution X]",
        "Problem Description: [Description]",
        "Optimization Solution: [Description]",
        "Example Code: [Code Block]",
        "</format>",
        "",
        "<code>",
        "def example(a):",
        "  b = []",
        "  for i in range(len(a)):",
        "    b.append(a[i]*2)",
        "  return sum(b)",
        "</code>",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c163ae4c",
      "metadata": {},
      "source": [
        "#### Tip Two: Avoid Chain-of-Thought Prompts<br>",
        "In section 4.5, you learned about improving response quality by using the chain-of-thought technique with large language models (LLMs).<br>",
        "Generally, there is no need to prompt reasoning models with phrases like \"think step by step\" or \"explain your reasoning,\" as they inherently perform deep thinking. Your prompts might actually restrict the model's performance. Unless you require the LLM to strictly follow a fixed line of reasoning, which is rare.<br>",
        "",
        "",
        "#### Tip Three: Adjust Prompts Based on Model Responses<br>",
        "Reasoning models are naturally suited for analyzing their thought processes due to their response format (including **reasoning steps**), making it easier for you to refine prompts.<br>",
        "Thus, thereâ€™s no need to worry about whether your prompts are perfect. Simply keep engaging with the reasoning model, providing additional information and refining prompts during the conversation.<br>",
        "For instance, when your descriptions are **too abstract** or **not accurately described**, you can use the technique of **adding examples** discussed in section 4.4 to clarify these details. Such examples can often be selected from past conversations with the model.<br>",
        "This process can be repeated multiple timesâ€”continuously adjusting prompts and allowing the model to iteratively reason until it meets your requirements.<br>",
        "",
        "#### Tip Four: Collaborative Task Completion Between Reasoning Models and General Models<br>",
        "Reasoning models are akin to \"smarter employees,\" effectively assisting you with tasks like reasoning and planning. However, for execution tasks involving repetitive actions, reasoning models may overthink, which could be unnecessary.<br>",
        "A better approach is to let the models collaborate by focusing on their respective strengths: reasoning models handle \"slow thinking\" for planning or reasoning, while general models focus on \"fast thinking\" or executing specific actions using tools.<br>",
        "You can try running and understanding the following project:  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01807cca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "=== ã€è§„åˆ’æ€è€ƒé˜¶æ®µã€‘ ä½¿ç”¨æ¨ç†æ¨¡å‹ï¼šqwq-32b===\n",
            "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¸®ç”¨æˆ·å®‰æ’ä¸‹å‘¨çš„äº§å“å‘å¸ƒä¼šå¹¶åˆ›å»ºç›¸å…³ç­¹å¤‡ä»»åŠ¡ã€‚é¦–å…ˆï¼Œç”¨æˆ·çš„éœ€æ±‚æ˜¯å®‰æ’å‘å¸ƒä¼šå¹¶ç­¹å¤‡ç›¸å…³äº‹é¡¹ï¼Œæ‰€ä»¥æˆ‘è¦åˆ†è§£æˆå…·ä½“çš„æ­¥éª¤ï¼Œä½¿ç”¨æä¾›çš„å·¥å…·ã€‚\n",
            "\n",
            "é¦–å…ˆï¼Œç”¨æˆ·æåˆ°çš„å·¥å…·æœ‰ä¸¤ä¸ªï¼šcreate_taskå’Œschedule_eventã€‚æˆ‘éœ€è¦ç¡®å®šæ¯ä¸ªæ­¥éª¤åº”è¯¥ä½¿ç”¨å“ªä¸ªå·¥å…·ã€‚å‘å¸ƒä¼šé€šå¸¸éœ€è¦å…ˆç¡®å®šæ—¶é—´ï¼Œæ‰€ä»¥ç¬¬ä¸€æ­¥åº”è¯¥æ˜¯å®‰æ’æ—¥ç¨‹äº‹ä»¶ï¼Œä½¿ç”¨schedule_eventå·¥å…·ã€‚éœ€è¦ç¡®å®šä¸€ä¸ªå…·ä½“çš„æ—¶é—´ï¼Œæ¯”å¦‚ä¸‹å‘¨ä¸‰ä¸‹åˆ2ç‚¹ï¼Œè¿™æ ·ç”¨æˆ·å°±èƒ½æœ‰ä¸€ä¸ªæ˜ç¡®çš„æ—¥ç¨‹å®‰æ’ã€‚\n",
            "\n",
            "æ¥ä¸‹æ¥æ˜¯ç­¹å¤‡ä»»åŠ¡ï¼Œéœ€è¦åˆ†è§£æˆå‡ ä¸ªå…³é”®ä»»åŠ¡ã€‚æ¯”å¦‚ï¼Œåˆ¶å®šå‘å¸ƒä¼šçš„è¯¦ç»†æ–¹æ¡ˆï¼Œè¿™åº”è¯¥ç”¨create_taskå·¥å…·ï¼Œè®¾ç½®ä¼˜å…ˆçº§ä¸ºé«˜ï¼Œå› ä¸ºè¿™æ˜¯åŸºç¡€ã€‚ç„¶åæ˜¯é‚€è¯·å˜‰å®¾ï¼ŒåŒæ ·éœ€è¦é«˜ä¼˜å…ˆçº§ï¼Œç¡®ä¿åŠæ—¶è”ç³»ã€‚å‡†å¤‡æ¼”ç¤ºææ–™å¯èƒ½éœ€è¦ä¸­ç­‰ä¼˜å…ˆçº§ï¼Œä½†ä¹Ÿè¦åŠæ—¶å®Œæˆã€‚åœºåœ°å¸ƒç½®å’Œè®¾å¤‡è°ƒè¯•å¯èƒ½éœ€è¦ç¨åä¸€äº›ï¼Œä½†ä¹Ÿè¦åœ¨å‘å¸ƒä¼šå‰å®Œæˆï¼Œæ‰€ä»¥ä¼˜å…ˆçº§ä¸­ç­‰ã€‚æœ€åï¼Œå®£ä¼ å’Œåª’ä½“é€šçŸ¥å¯èƒ½åœ¨å‘å¸ƒä¼šå‰å‡ å¤©å®Œæˆï¼Œä¼˜å…ˆçº§ä¸­ç­‰ã€‚\n",
            "\n",
            "æ¯ä¸ªä»»åŠ¡çš„å‚æ•°éœ€è¦å…·ä½“ï¼Œæ¯”å¦‚descriptionè¦æ˜ç¡®ä»»åŠ¡å†…å®¹ï¼Œpriorityç”¨æ•°å­—è¡¨ç¤ºï¼Œæ¯”å¦‚1æ˜¯æœ€é«˜ï¼Œ3æ˜¯ä¸­ç­‰ã€‚æ—¶é—´å‚æ•°è¦ç¬¦åˆdatetimeæ ¼å¼ï¼Œæ¯”å¦‚\"2023-11-15T14:00\"ã€‚\n",
            "\n",
            "éœ€è¦æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„æ­¥éª¤ï¼Œæ¯”å¦‚æ˜¯å¦è¿˜æœ‰å…¶ä»–ç­¹å¤‡ä»»åŠ¡ï¼Œä½†æ ¹æ®å¸¸è§æµç¨‹ï¼Œè¿™å‡ ä¸ªåº”è¯¥è¶³å¤Ÿã€‚å¦å¤–ï¼Œç¡®ä¿æ—¶é—´å®‰æ’åˆç†ï¼Œä»»åŠ¡ä¼˜å…ˆçº§åˆ†é…æ­£ç¡®ã€‚æœ€åï¼Œç”¨Markdownåˆ—è¡¨æ ¼å¼ï¼Œæ¯ä¸ªæ­¥éª¤æ ‡æ³¨å·¥å…·åç§°å’Œå‚æ•°ç¤ºä¾‹ã€‚\n",
            "\n",
            "=== ã€è§„åˆ’ç”Ÿæˆé˜¶æ®µã€‘ ä½¿ç”¨æ¨ç†æ¨¡å‹ï¼šqwq-32b===\n",
            "```markdown\n",
            "1. **å®‰æ’å‘å¸ƒä¼šæ—¥ç¨‹**  \n",
            "   å·¥å…·ï¼š[[schedule_event]]  \n",
            "   å‚æ•°ç¤ºä¾‹ï¼š  \n",
            "   ```json  \n",
            "   {  \n",
            "     \"title\": \"äº§å“å‘å¸ƒä¼š\",  \n",
            "     \"time\": \"2023-11-15T14:00\"  \n",
            "   }  \n",
            "   ```  \n",
            "\n",
            "2. **åˆ›å»ºå‘å¸ƒä¼šç­–åˆ’ä»»åŠ¡**  \n",
            "   å·¥å…·ï¼š[[create_task]]  \n",
            "   å‚æ•°ç¤ºä¾‹ï¼š  \n",
            "   ```json  \n",
            "   {  \n",
            "     \"description\": \"åˆ¶å®šå‘å¸ƒä¼šè¯¦ç»†æ–¹æ¡ˆï¼ˆè®®ç¨‹ã€æ¼”è®²å†…å®¹ã€æµç¨‹è®¾è®¡ï¼‰\",  \n",
            "     \"priority\": 1  \n",
            "   }  \n",
            "   ```  \n",
            "\n",
            "3. **åˆ›å»ºå˜‰å®¾é‚€è¯·ä»»åŠ¡**  \n",
            "   å·¥å…·ï¼š[[create_task]]  \n",
            "   å‚æ•°ç¤ºä¾‹ï¼š  \n",
            "   ```json  \n",
            "   {  \n",
            "     \"description\": \"ç¡®è®¤å¹¶é‚€è¯· keynote å˜‰å®¾åŠåª’ä½“ä»£è¡¨\",  \n",
            "     \"priority\": 1  \n",
            "   }  \n",
            "   ```  \n",
            "\n",
            "4. **åˆ›å»ºæ¼”ç¤ºææ–™å‡†å¤‡ä»»åŠ¡**  \n",
            "   å·¥å…·ï¼š[[create_task]]  \n",
            "   å‚æ•°ç¤ºä¾‹ï¼š  \n",
            "   ```json  \n",
            "   {  \n",
            "     \"description\": \"åˆ¶ä½œäº§å“æ¼”ç¤ºè§†é¢‘ã€PPTåŠå®£ä¼ æ‰‹å†Œ\",  \n",
            "     \"priority\": 2  \n",
            "   }  \n",
            "   ```  \n",
            "\n",
            "5. **åˆ›å»ºåœºåœ°å¸ƒç½®ä¸è®¾å¤‡è°ƒè¯•ä»»åŠ¡**  \n",
            "   å·¥å…·ï¼š[[create_task]]  \n",
            "   å‚æ•°ç¤ºä¾‹ï¼š  \n",
            "   ```json  \n",
            "   {  \n",
            "     \"description\": \"ç¡®è®¤åœºåœ°å¸ƒç½®æ–¹æ¡ˆã€è°ƒè¯•éŸ³å“/ç¯å…‰/ç›´æ’­è®¾å¤‡\",  \n",
            "     \"priority\": 2  \n",
            "   }  \n",
            "   ```  \n",
            "\n",
            "6. **åˆ›å»ºå®£ä¼ ä¸é€šçŸ¥ä»»åŠ¡**  \n",
            "   å·¥å…·ï¼š[[create_task]]  \n",
            "   å‚æ•°ç¤ºä¾‹ï¼š  \n",
            "   ```json  \n",
            "   {  \n",
            "     \"description\": \"å‘å¸ƒå‘å¸ƒä¼šé¢„å‘Šã€ç¤¾äº¤åª’ä½“å®£ä¼ åŠå‚ä¼šé€šçŸ¥\",  \n",
            "     \"priority\": 3  \n",
            "   }  \n",
            "   ```  \n",
            "```\n",
            "\n",
            "=== è®¡åˆ’æ‰§è¡Œé˜¶æ®µ ===\n",
            "\n",
            "\n",
            "ä½¿ç”¨é€šç”¨æ¨¡å‹qwen-plus-0919æ ¼å¼åŒ–è¾“å‡ºï¼š\n",
            "{\"steps\": [ {\"tool\": \"schedule_event\", \"params\": {\"title\": \"äº§å“å‘å¸ƒä¼š\", \"time\": \"2023-11-15T14:00\"}}, {\"tool\": \"create_task\", \"params\": {\"description\": \"åˆ¶å®šå‘å¸ƒä¼šè¯¦ç»†æ–¹æ¡ˆï¼ˆè®®ç¨‹ã€æ¼”è®²å†…å®¹ã€æµç¨‹è®¾è®¡ï¼‰\", \"priority\": 1}}, {\"tool\": \"create_task\", \"params\": {\"description\": \"ç¡®è®¤å¹¶é‚€è¯· keynote å˜‰å®¾åŠåª’ä½“ä»£è¡¨\", \"priority\": 1}}, {\"tool\": \"create_task\", \"params\": {\"description\": \"åˆ¶ä½œäº§å“æ¼”ç¤ºè§†é¢‘ã€PPTåŠå®£ä¼ æ‰‹å†Œ\", \"priority\": 2}}, {\"tool\": \"create_task\", \"params\": {\"description\": \"ç¡®è®¤åœºåœ°å¸ƒç½®æ–¹æ¡ˆã€è°ƒè¯•éŸ³å“/ç¯å…‰/ç›´æ’­è®¾å¤‡\", \"priority\": 2}}, {\"tool\": \"create_task\", \"params\": {\"description\": \"å‘å¸ƒå‘å¸ƒä¼šé¢„å‘Šã€ç¤¾äº¤åª’ä½“å®£ä¼ åŠå‚ä¼šé€šçŸ¥\", \"priority\": 3}} ]}\n",
            "\n",
            "è°ƒç”¨å·¥å…·æ‰§è¡Œï¼Œæ‰§è¡Œç»“æœï¼š\n",
            "{\n",
            "  \"status\": \"completed\",\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"tool\": \"schedule_event\",\n",
            "      \"result\": \"å·²å®‰æ’æ—¥ç¨‹ï¼šäº§å“å‘å¸ƒä¼š\"\n",
            "    },\n",
            "    {\n",
            "      \"tool\": \"create_task\",\n",
            "      \"result\": \"å·²åˆ›å»ºä»»åŠ¡ï¼šåˆ¶å®šå‘å¸ƒä¼šè¯¦ç»†æ–¹æ¡ˆï¼ˆè®®ç¨‹ã€æ¼”è®²å†…å®¹ã€æµç¨‹è®¾è®¡ï¼‰\"\n",
            "    },\n",
            "    {\n",
            "      \"tool\": \"create_task\",\n",
            "      \"result\": \"å·²åˆ›å»ºä»»åŠ¡ï¼šç¡®è®¤å¹¶é‚€è¯· keynote å˜‰å®¾åŠåª’ä½“ä»£è¡¨\"\n",
            "    },\n",
            "    {\n",
            "      \"tool\": \"create_task\",\n",
            "      \"result\": \"å·²åˆ›å»ºä»»åŠ¡ï¼šåˆ¶ä½œäº§å“æ¼”ç¤ºè§†é¢‘ã€PPTåŠå®£ä¼ æ‰‹å†Œ\"\n",
            "    },\n",
            "    {\n",
            "      \"tool\": \"create_task\",\n",
            "      \"result\": \"å·²åˆ›å»ºä»»åŠ¡ï¼šç¡®è®¤åœºåœ°å¸ƒç½®æ–¹æ¡ˆã€è°ƒè¯•éŸ³å“/ç¯å…‰/ç›´æ’­è®¾å¤‡\"\n",
            "    },\n",
            "    {\n",
            "      \"tool\": \"create_task\",\n",
            "      \"result\": \"å·²åˆ›å»ºä»»åŠ¡ï¼šå‘å¸ƒå‘å¸ƒä¼šé¢„å‘Šã€ç¤¾äº¤åª’ä½“å®£ä¼ åŠå‚ä¼šé€šçŸ¥\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI",
        "import os",
        "import json",
        "from typing import Generator",
        "",
        "class TaskPlanningSystem:",
        "    def __init__(self):",
        "        # Initialize client connection",
        "        self.client = OpenAI(",
        "            api_key=os.getenv(\"DASHSCOPE_API_KEY\"),",
        "            base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"",
        "        )",
        "        ",
        "        # System toolset (can be extended as needed)",
        "        self.tools = {",
        "            \"create_task\": {",
        "                \"desc\": \"Create a new task item\",",
        "                \"params\": {\"description\": \"str\", \"priority\": \"int\"}",
        "            },",
        "            \"schedule_event\": {",
        "                \"desc\": \"Schedule an event\", ",
        "                \"params\": {\"title\": \"str\", \"time\": \"datetime\"}",
        "            }",
        "        }",
        "",
        "    def generate_plan(self, user_request: str) -> Generator[str, None, None]:",
        "        \"\"\"Stream-generated task planning\"\"\"",
        "        # Build planning prompt",
        "        system_prompt = f\"\"\"You are a senior task planner, please convert the user's requirements into executable steps:",
        "        ",
        "        Available tools (format: [[tool name]]):",
        "        {json.dumps(self.tools, indent=2)}",
        "        ",
        "        Output requirements:",
        "        1. Use Markdown list format",
        "        2. Each step should indicate the tool name",
        "        3. Include necessary parameter examples",
        "        \"\"\"",
        "        model=\"qwq-32b\"",
        "        # Initiate streaming request",
        "        completion = self.client.chat.completions.create(",
        "            model=model,",
        "            messages=[",
        "                {\"role\": \"system\", \"content\": system_prompt},",
        "                {\"role\": \"user\", \"content\": user_request}",
        "            ],",
        "            stream=True,",
        "            temperature=0.3",
        "        )",
        "",
        "        # Print thinking process title",
        "        print(f\"\\n\\n=== [Planning Thinking Phase] Using reasoning model: {model}===\")",
        "        is_answering = False",
        "        # Process streaming response",
        "        for chunk in completion:",
        "            if chunk.choices:",
        "                delta = chunk.choices[0].delta",
        "                if hasattr(delta, 'reasoning_content') and delta.reasoning_content is not None:",
        "                    # Process thinking process content",
        "                    reasoning_content = delta.reasoning_content",
        "                    yield reasoning_content",
        "                else:",
        "                    # Switch to answer output mode",
        "                    if delta.content != \"\" and not is_answering:",
        "                        print(f\"\\n\\n=== [Planning Generation Phase] Using reasoning model: {model}===\")",
        "                        is_answering = True",
        "                    # Process answer content",
        "                    if delta.content:",
        "                        content = delta.content",
        "                        yield content",
        "",
        "    def execute_plan(self, plan: str) -> dict:",
        "        \"\"\"Execute the generated task plan\"\"\"",
        "        # Call general model to parse the plan",
        "        analysis_prompt = f\"\"\"Please parse the following task plan and generate executable instructions:",
        "        ",
        "        Plan content:",
        "        {plan}",
        "        ",
        "        Output requirements:",
        "        - The returned content must be in JSON format only, do not include other information, do not output code blocks.",
        "        - Include tool names and parameters",
        "        - Example:",
        "            {{\"steps\": [",
        "                {{\"tool\": \"create_task\", \"params\": {{\"description\": \"...\"}}}}",
        "            ]}}",
        "        \"\"\"",
        "        model=\"qwen-plus-0919\"",
        "        response = self.client.chat.completions.create(",
        "            model=model,",
        "            messages=[{\"role\": \"user\", \"content\": analysis_prompt}],",
        "            temperature=0",
        "        )",
        "",
        "        print(f\"\\n\\nUsing general model {model} to format output:\\n{response.choices[0].message.content}\")",
        "        # Parse execution instructions",
        "        try:",
        "            instructions = json.loads(response.choices[0].message.content)",
        "            return self._run_instructions(instructions)",
        "        except json.JSONDecodeError:",
        "            return {\"error\": \"Instruction parsing failed\"}",
        "",
        "    def _run_instructions(self, instructions: dict) -> dict:",
        "        \"\"\"Actually execute tool calls\"\"\"",
        "        results = []",
        "        for step in instructions.get(\"steps\", []):",
        "            tool = step.get(\"tool\")",
        "            params = step.get(\"params\", {})",
        "            ",
        "            # Execute tool call (this is an example implementation)",
        "            if tool == \"create_task\":",
        "                results.append({",
        "                    \"tool\": tool,",
        "                    \"result\": f\"Task created: {params.get('description')}\"",
        "                })",
        "            elif tool == \"schedule_event\":",
        "                results.append({",
        "                    \"tool\": tool,",
        "                    \"result\": f\"Event scheduled: {params.get('title')}\"",
        "                })",
        "            else:",
        "                results.append({\"error\": f\"Unknown tool: {tool}\"})",
        "        ",
        "        return {\"status\": \"completed\", \"results\": results}",
        "",
        "# Usage example",
        "if __name__ == \"__main__\":",
        "    PlanningSystem = TaskPlanningSystem()",
        "    ",
        "    # Example user request",
        "    user_request = \"Please schedule next week's product launch and create related preparation tasks\"",
        "    ",
        "    # Planning generation phase",
        "    plan_stream = PlanningSystem.generate_plan(user_request)",
        "    generated_plan = []",
        "    for chunk in plan_stream:",
        "        print(chunk, end=\"\", flush=True)",
        "        generated_plan.append(chunk)",
        "    ",
        "    full_plan = \"\".join(generated_plan)",
        "    ",
        "    print(\"\\n\\n=== Plan Execution Phase ===\")",
        "    result = PlanningSystem.execute_plan(full_plan)",
        "    print(f\"\\nCall tool execution, execution result:\")",
        "    print(json.dumps(result, indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cf7690c",
      "metadata": {},
      "source": [
        "The above code implements the following core interaction process. The code mainly uses the qwq-32b model for streaming planning generation, then uses qwen-plus-0919 for instruction parsing, and finally decouples tool invocation through the _run_instructions method, making it easier to extend in the future:",
        "",
        "<a href=\"https://img.alicdn.com/imgextra/i4/O1CN01uZWa8k243UBb3FDqO_!!6000000007335-0-tps-3604-1478.jpg\" target=\"_blank\">",
        "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01uZWa8k243UBb3FDqO_!!6000000007335-0-tps-3604-1478.jpg\" width=\"800\">",
        "</a>  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30451423",
      "metadata": {},
      "source": [
        "## âœ… Summary of this section",
        "",
        "Through learning this section, you have mastered prompt that are suitable for general large language models and reasoning models. The flexible application of these techniques can effectively guarantee the lower limit of AI response quality.<br>",
        "However, as AI technology continues to evolve, these techniques may also change (for example, reasoning models may no longer need to improve performance through chain-of-thought prompting). What you need to understand is that the core purpose of these prompt techniques is:<br>",
        "<mark>To clearly express your core needs and provide rich and useful background information (context)</mark> â€” this is crucial for making good use of large language models.",
        "",
        "In the actual implementation of large model applications, domain experts often participate in designing the prompts. Therefore, hardcoding prompt into your engineering code should be adjusted to be configurable, or even the entire application process should be made configurable, so that it is more convenient for domain experts to participate in the design of prompts and the overall process. Alibaba Cloud's Model Studio provides a visual [application building](https://help.aliyun.com/zh/model-studio/user-guide/application-introduction#7c79befb2djg9) capability, allowing users to complete prompt writing and visually build the entire complex application flow on the page, which is very suitable for large model application development projects requiring participation from non-technical domain experts.",
        "",
        "In the next section, you will learn automated evaluation methods to test the performance of Q&A bots. You can use quantitative metrics to evaluate the effectiveness of your prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3166b70c",
      "metadata": {},
      "source": [
        "## ğŸ”¥ Post-class Quiz",
        "",
        "### ğŸ” Single-choice Question",
        "<details>",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">",
        "<b>Which of the following prompt elements is used to explicitly define the task that the large language model needs to completeâ“</b>",
        "",
        "- A. Role",
        "- B. Audience",
        "- C. Objective",
        "- D. Context",
        "",
        "**[Click to view answer]**",
        "</summary>",
        "",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">",
        "",
        "âœ… **Reference Answer: C**",
        "ğŸ“ **Explanation**:",
        "- The Objective explicitly defines the operation or outcome that the large language model (LLMs) is required to perform. Other options do not directly define the task itself.",
        "- Role defines the identity that the LLMs should assume, Audience specifies the target group, and Context provides background information.",
        "",
        "</div>",
        "</details>",
        "",
        "---",
        "",
        "",
        "### ğŸ” Multiple-choice Question",
        "<details>",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">",
        "<b>Suppose you want to use a large language model to generate a description of the solar system suitable for third-grade students. Which of the following user_query designs is more reasonableâ“</b>",
        "",
        "- A. user_query=\"Write about the solar system.\"",
        "- B. user_query=\"Tell me about the solar system.\"",
        "- C. user_query=\"Explain the solar system as if you were talking to a third-grade student.\"",
        "- D. user_query=\"Write a short and engaging description of the solar system for a third-grade audience, focusing on key planets and their characteristics.\"",
        "- E. user_query=\"What is the solar system?\"",
        "",
        "**[Click to view answer]**",
        "</summary>",
        "",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">",
        "",
        "âœ… **Reference Answer: CD**",
        "ğŸ“ **Explanation**:",
        "- Options C and D clearly specify the target audience and provide requirements for content and style.",
        "",
        "</div>",
        "</details>",
        "",
        "---",
        "",
        "",
        "### ğŸ” Multiple-choice Question",
        "<details>",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">",
        "<b>When using a reasoning-based large language model (such as qwq-32b) to perform reasoning tasks, which of the following prompting techniques are recommendedâ“</b>",
        "",
        "- A. Provide simple and clear task instructions",
        "- B. Supplement with sufficient background information",
        "- C. If there are specific user groups or task requirements, set roles or audiences",
        "- D. Use chain-of-thought prompts to make the reasoning process more rigorous and reliable",
        "- E. When descriptions are too abstract or cannot be accurately described, clarify them by adding examples",
        "",
        "**[Click to view answer]**",
        "</summary>",
        "",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">",
        "",
        "âœ… **Reference Answer: ABCE**",
        "ğŸ“ **Explanation**:",
        "- When using reasoning models, it is advisable to avoid using chain-of-thought prompting, as this may reduce the effectiveness of reasoning.",
        "",
        "</div>",
        "</details>",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b354883f",
      "metadata": {},
      "source": [
        "## âœ‰ï¸ Evaluation and Feedback",
        "",
        "Thank you for studying the Alibaba Cloud Large Model ACP Certification course. If you think there are parts of the course that are well-written or need improvement, we look forward to your [evaluation and feedback through this questionnaire](https://survey.aliyun.com/apps/zhiliao/Mo5O9vuie).",
        "",
        "Your criticism and encouragement are both driving forces for our progress.  ",
        "",
        ""
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "learnacp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}