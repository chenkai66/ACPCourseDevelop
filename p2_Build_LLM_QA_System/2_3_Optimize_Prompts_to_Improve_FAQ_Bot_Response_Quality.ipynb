{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92010cd5",
      "metadata": {},
      "source": [
        "# 2.3 Optimizing Prompts to Improve the Quality of Responses from Q&A Bots",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The large language model in the above example successfully output a tutorial for making handmade keychains, but the content was not concise enough. If you only want the large language model to output content in a specific style and structure, such as only including the theme, list of materials, and steps, you can provide it with several examples so that it can \"imitate\" them!",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 手工贺卡制作教程\n",
            "\n",
            "## 材料清单\n",
            "- 彩色卡纸\n",
            "- 剪刀\n",
            "- 胶水或双面胶\n",
            "- 马克笔或彩色铅笔\n",
            "- 装饰物（如亮片、丝带、贴纸等）\n",
            "\n",
            "## 步骤\n",
            "1. **准备材料**：首先，准备好所有需要的材料，包括不同颜色的卡纸、剪刀、胶水或双面胶、马克笔或彩色铅笔，以及任何你想要用来装饰贺卡的小物件。\n",
            "2. **裁剪卡纸**：选择一张作为贺卡主体的卡纸，将其对折，裁剪成你想要的大小。常见的贺卡尺寸大约是5英寸 x 7英寸。\n",
            "3. **设计封面**：使用马克笔或彩色铅笔在贺卡封面上绘制图案或写上祝福语。你也可以剪裁其他颜色的卡纸，粘贴到封面上，增加层次感。\n",
            "4. **添加装饰**：利用亮片、丝带、贴纸等装饰物，为贺卡增添个性化的装饰。确保装饰物牢固地粘贴在卡纸上。\n",
            "5. **撰写内页**：打开贺卡，在内页写下你想对收卡人说的话。可以是一段温馨的话语，也可以是一首诗或者简单的祝福。\n",
            "6. **检查与完善**：仔细检查贺卡的每一个部分，确保没有遗漏的地方，装饰物都已牢固粘贴，文字清晰可读。\n",
            "7. **完成**：现在，你的手工贺卡已经完成了！你可以将它送给朋友或家人，表达你的心意。\n",
            "\n",
            "## 结束语\n",
            "希望这个教程能帮助你制作出充满心意的手工贺卡，让你的情感传递更加特别和有意义！"
          ]
        }
      ],
      "source": [
        "question_task= \"\"\"",
        "【任务要求】",
        "请根据用户的主题，结合下面【样例】给的例子，理解和使用一致的风格和结构继续创作内容，不要输出多余的内容。",
        "---",
        "【输出要求】",
        "最终输出需要以Markdown格式呈现，请注意，在你的回答中包含所有必要的Markdown元素，如标题、列表、链接、图片引用、加粗等，以便于阅读、后续编辑和保存。",
        "---",
        "【样例】",
        "### 示例1: 制作简易书签",
        "# 简易书签制作教程",
        "",
        "## 材料清单",
        "- 彩色卡纸",
        "- 剪刀",
        "- 装饰贴纸",
        "- 铅笔",
        "",
        "## 步骤",
        "1. 选择一张彩色卡纸。",
        "2. 用铅笔在卡纸上画出一个长方形，尺寸约为2英寸 x 6英寸。",
        "3. 沿着铅笔线剪下长方形。",
        "4. 使用装饰贴纸对书签进行个性化装饰。",
        "5. 完成！现在你有了一个独一无二的书签。",
        "",
        "## 结束语",
        "希望这个教程能帮助你制作出满意的书签！",
        "",
        "---",
        "【用户输入】",
        "以下是用户的要求创作的主题：",
        "\"\"\"",
        "question_doc = \"制作手工贺卡\"",
        "",
        "question = question_task + question_doc",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0b18130",
      "metadata": {},
      "source": [
        "## 🚄 Preface  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a33d28f-9014-414a-a7b5-b016e12d4bb0",
      "metadata": {},
      "source": [
        "The Q&A bot from the previous section can already answer questions about company rules and regulations, but this is only the first step in building an excellent user experience. Users often expect more personalized and precise interactions.",
        "This section will introduce techniques in prompt engineering, teaching you how to improve or control the output of the Q&A bot by optimizing prompts—for example, adjusting tone, standardizing formats, or even enabling it to handle tasks like text summarization, inference, and transformation.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37a39a58",
      "metadata": {},
      "source": [
        "## 🍁 Course Objectives",
        "",
        "After completing this section of the course, you will be able to:",
        "",
        "* Understand prompt frameworks and templates",
        "* Learn about prompt techniques and best practices",
        "* Study how to apply large language models (LLMs) in engineering to handle various types of tasks",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b0e80fa",
      "metadata": {},
      "source": [
        "## 1. Previous Content Recap",
        "",
        "In the previous section, through the RAG method, large language models (LLMs) have already obtained the company's private knowledge. For ease of invocation, this was encapsulated into several functions and saved in chatbot/rag.py. You can now quickly invoke it using the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "7aa6756d-47a4-4b11-8074-d992d6ce327b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:03.577636Z",
          "start_time": "2025-01-15T06:49:03.573248Z"
        }
      },
      "outputs": [],
      "source": [
        "from chatbot import rag",
        "import os",
        "from config.load_key import load_key",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "f23f2c04-c229-4f5f-b2cd-50c8a45672e4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:09.717475Z",
          "start_time": "2025-01-15T06:49:09.713681Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "你配置的 API Key 是：sk-1a*****\n"
          ]
        }
      ],
      "source": [
        "# Load API key",
        "load_key()",
        "print(f'Your configured API Key is: {os.environ[\"DASHSCOPE_API_KEY\"][:5]+\"*\"*5}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "91138451-fbc1-49ff-a268-2ece341b10bd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:11.770743Z",
          "start_time": "2025-01-15T06:49:11.611835Z"
        }
      },
      "outputs": [],
      "source": [
        "# Load index",
        "# The previous section has already built the index, so the index can be loaded directly here. If you need to rebuild the index, you can add a line of code: rag.indexing()",
        "index = rag.load_index()",
        "query_engine = rag.create_query_engine(index=index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "1d2d8d11-5a07-441c-9cd6-6ba10bf80603",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:16.961802Z",
          "start_time": "2025-01-15T06:49:16.959373Z"
        }
      },
      "outputs": [],
      "source": [
        "# Define the question-answering function",
        " def ask_llm(question, query_engine):",
        "   streaming_response = query_engine.query(question)",
        "   streaming_response.print_response_stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1715f36",
      "metadata": {},
      "source": [
        "## 2. Optimizing Prompts to Improve the Quality of Responses from Large Language Models",
        "",
        "In the previous section, you obtained the management tools used internally by the company through the RAG method. However, your colleague hopes that the Q&A bot not only provides the tool name but also includes a link to the tool. While you could ask colleagues to include a note like \"provide the download address\" in their questions, this approach is inefficient. Therefore, you consider improving the program: after receiving user questions, supplement them with additional requirements for the response content.",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "b12a310a-660c-4cd3-9d23-8819bdefb8bb",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:34.992776Z",
          "start_time": "2025-01-15T06:49:29.417135Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "对于项目管理，推荐使用Jira或Trello。这两个工具都非常适合项目管理和跟踪，可以帮助团队更高效地完成任务。\n",
            "\n",
            "- Jira 是一个强大的项目管理和问题跟踪工具，特别适用于软件开发团队。你可以从 Atlassian 的官方网站下载：[Jira 下载页面](https://www.atlassian.com/software/jira/download)\n",
            "\n",
            "- Trello 是一个灵活的看板式项目管理工具，适合各种类型的团队。你也可以从其官方网站获取：[Trello 下载页面](https://trello.com/download)"
          ]
        }
      ],
      "source": [
        "question = \"What tools should our company use for project management?\"",
        "instruction = \"If it's a tool consultation question, be sure to provide the download link.\"",
        "new_question = question + instruction",
        "ask_llm(new_question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "806fadd6-673a-4542-b3e2-fe36310b30ad",
      "metadata": {},
      "source": [
        "After adjusting the way prompts were provided, the Q&A bot's responses became more aligned with the colleagues' requirements. Therefore, the choice of prompt significantly determines the quality of the large language model's responses. Next, you can refer to some prompt frameworks to construct your prompts.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c006a5ea-5693-4805-9d2c-ccc0d2214939",
      "metadata": {},
      "source": [
        "## 3. Prompt Framework",
        "",
        "### 3.1 Basic Elements",
        "",
        "When communicating with a large language model, you can imagine it as a person who has undergone \"socialization training.\" The way of communication should be the same as how humans exchange information. Your requirements need to be clear and unambiguous. The clearer and more precise your way of asking (Prompt), the better the large language model can grasp the key points of the question, and the more its response will meet your expectations. Generally speaking, the following elements need to be clarified in the prompt: **Task Objective, Context, Role, Audience, Sample, Output Format**. These elements form a prompt framework that can help you construct a complete and effective prompt.",
        "",
        "|Element|Meaning|",
        "|----|----|",
        "|Task Objective (Object)|Clearly specify what task the large language model is required to complete, allowing the model to focus on specific goals|",
        "|Context (Context)|Background information about the task, such as operational processes or task scenarios, clarifying the scope of the discussion for the large language model|",
        "|Role (Role)|The role the large language model plays, or emphasizing the tone, writing style, etc., that the model should use, clarifying the expected emotional tone of the response|",
        "|Audience (Audience)|Clarify the specific audience the large language model is targeting, constraining its response style|",
        "|Sample (Sample)|Specific examples for the large language model to refer to; the model will abstract solutions and pay attention to specific formats from these|",
        "|Output Format (Output Format)|Clearly specify the format of the output, output type, range of enumerated values. Typically, it also explicitly states content and information that should not be included in the output, which can be further clarified by combining with examples|",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc35341a-3d7c-4881-81e3-935d42a1dcb2",
      "metadata": {},
      "source": [
        "Of course, in addition to the prompt framework discussed above, many problem analysis paradigms can be used to help you describe clear and specific requirements. For example, SWOT analysis and 5W2H analysis. Additionally, you can also consider using Alibaba Cloud Model Studio-provided [Prompt Automatic Optimization Tool](https://bailian.console.aliyun.com/?tab=app#/component-manage/prompt/optimize) to help refine your prompts.",
        "",
        "<a href=\"https://img.alicdn.com/imgextra/i3/O1CN014JCCqn22zX6xB4tt3_!!6000000007191-0-tps-2068-1052.jpg\" target=\"_blank\">",
        "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN014JCCqn22zX6xB4tt3_!!6000000007191-0-tps-2068-1052.jpg\" width=\"700\"/>",
        "</a>  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e1fa6d9",
      "metadata": {},
      "source": [
        "### 3.2 Prompt Template",
        "",
        "When developing large language model applications, allowing users to directly write prompts according to a framework is not the optimal choice. You can refer to the elements in various prompt frameworks to construct a prompt template. A prompt template can preset certain information, such as the role of the large language model and precautions, thereby constraining the behavior of the model. Developers only need to configure input parameters within the template to create standardized applications for large language models.",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84e29a3-a017-47c4-907e-2ff897333d1d",
      "metadata": {},
      "source": [
        "In the RAG chatbot created using LlamaIndex, there is a default prompt template as follows:",
        "",
        "- The default template can be viewed using code. You can refer to [the code on the LlamaIndex official website.](https://docs.llamaindex.ai/en/stable/examples/prompts/prompts_rag/) The original LlamaIndex prompt template is:  ",
        "",
        "",
        "",
        "```text",
        "Context information is below.",
        "---------------------",
        "{context_str}",
        "---------------------",
        "Given the context information and not prior knowledge, answer the query.",
        "Query: {query_str}",
        "Answer:",
        "```",
        "",
        "Among them, `context_str` and `query_str` both represent variables. During the vector retrieval and questioning process, `context_str` will be replaced with the context information retrieved from the vector database, and `query_str` will be replaced with the user's question.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78ff8788-4d67-47b8-a4a2-3f3a85e6b0c5",
      "metadata": {},
      "source": [
        "Since the original template is a general-purpose template, it is not suitable for constraining the behavior of the Q&A robot. You can readjust the prompt template through the following sample code, where `prompt_template_string` represents the new prompt template, and you can modify it according to your own scenario.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "05aea996-fdc8-4a35-922f-288313881fa3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T07:52:22.442975Z",
          "start_time": "2025-01-15T07:52:22.435217Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x31db8b610>"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Build prompt template",
        "prompt_template_string = (",
        "    \"You are the company's customer service assistant, you need to answer users' questions concisely.\"",
        "    \"\\n【Notes】：\\n\"",
        "    \"1. Answer user questions based on contextual information.\\n\"",
        "    \"2. Only answer the user's question, do not output other information\\n\"",
        "    \"The following is reference information.\"",
        "    \"---------------------\\n\"",
        "    \"{context_str}\\n\"",
        "    \"---------------------\\n\"",
        "    \"Question: {query_str}\\n.\"",
        "    \"Answer: \"",
        ")",
        "",
        "# Update prompt template",
        "rag.update_prompt_template(query_engine, prompt_template_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa99f324",
      "metadata": {},
      "source": [
        "## 4. Techniques for Building Effective Prompts",
        "",
        "In section 3.1, some key elements of prompt design were listed. The following will explain prompt techniques in detail based on specific scenarios, starting from the elements of prompts.",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b3eb55-646d-41dd-b801-152d76205bbe",
      "metadata": {},
      "source": [
        "### 4.1 Clearly Express Requirements and Use Delimiters",
        "",
        "Clearly expressing requirements ensures that the content generated by large models is highly relevant to the task. Requirements include the task objectives, background, and contextual information. Delimiters can also be used to separate various elements of the prompts.",
        "",
        "Delimiters help large models focus on specific goals, avoid ambiguous interpretations, and reduce the processing of unnecessary information. Common delimiters include \"**【】**\", \"**<< >>**\", and \"**###**\" to identify key elements, while \"**===**\" or \"**---**\" can be used to separate paragraphs. Alternatively, XML tags like `<tag> </tag>` can be used to mark specific sections. Of course, delimiters are not limited to the ones mentioned above; any symbol that provides clear separation will suffice. It is important to note that if a certain symbol (e.g., 【】) has been heavily used in the prompt, it should be avoided as a delimiter to prevent confusion.",
        "",
        "In the following example, we will use the prompt template from section 3.2 to help you polish your document in the role of a Q&A robot.",
        "",
        "- You need to run the code in section 3.2 to make the new prompt template effective.",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "8918731d-49b6-4132-9ba5-2dce3eee360c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T07:52:32.192934Z",
          "start_time": "2025-01-15T07:52:24.619420Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【新员工训练营活动】欢迎加入我们的大家庭！为了让每位新成员都能快速融入团队，我们精心策划了“新员工训练营”活动。在这里，你不仅能够全面了解公司的文化、价值观和发展愿景，还能通过一系列互动环节，与来自不同部门的小伙伴们建立深厚的友谊。更有资深导师面对面指导，助你在职业生涯的起跑线上加速前行。让我们一起开启这段充满挑战与机遇的旅程吧！"
          ]
        }
      ],
      "source": [
        "question = \"\"\"",
        "Expand and polish the text enclosed in 【】 brackets below to make the copy vivid, creative, and appealing to new employees.",
        "【New Employee Training Camp Activity】",
        "\"\"\"",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62ae4c28-a2e6-4ad3-995b-7af2d78b19d1",
      "metadata": {},
      "source": [
        "From the above execution results, it can be observed that the prompt clearly specifies the task requirements: to expand and polish the text. The theme of the task is \"**New Employee Training Camp Activities**\", the audience is \"**company employees**\", and the \"**【】**\" delimiter is used to separate the text. Using such a prompt ensures that the output not only retains the original meaning but also becomes more vivid and engaging.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7e4407d-3cff-4f00-a39b-b8ddfd7cce1d",
      "metadata": {},
      "source": [
        "### 4.2 限定角色和受众",
        "",
        "角色指大模型在特定场景下应扮演的身份，如专家、顾问或助手。受众是使用模型输出的目标用户群体，如普通消费者或学生。这有助于开发者定制内容的风格和深度。以下示例将展示不同的系统角色对输出文本风格和内容的影响，用户的问题为：qwen-vl是什么？",
        "",
        "- 以下仅演示提示词模板和输出的内容，你可以将提示词模板传入到`prompt_template_string`中，将用户问题传入到`question`中，再调用`ask_llm`函数。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79e5f128-8b02-40ca-b789-d577b6e38a3a",
      "metadata": {},
      "source": [
        "<table>",
        "  <thead>",
        "    <tr>",
        "      <th width = '80px'>Role</th>",
        "      <th width = '500px'>Large Language Model Algorithm Engineer</th>",
        "      <th width = '500px'>Elementary School Teacher</th>",
        "    </tr>",
        "  </thead>",
        "  <tbody valign = 'top'>",
        "    <tr>",
        "      <td>Prompt Template</td>",
        "      <td>\"You are a senior large language model algorithm engineer. Please answer the user's question based on the provided information\\n\"",
        "\"---------------------\\n\"",
        "\"{context_str}\\n\"",
        "\"---------------------\\n\"",
        "\"Question: {query_str}\\n.\"",
        "\"Answer: \"</td>",
        "      <td>\"You are an elementary school teacher. Please answer the user's question based on the provided information\\n\"",
        "\"---------------------\\n\"",
        "\"{context_str}\\n\"",
        "\"---------------------\\n\"",
        "\"Question: {query_str}\\n.\"",
        "\"Answer: \"</td>",
        "    </tr>",
        "    <tr>",
        "      <td>User Question</td>",
        "      <td>Qwen-VL</td>",
        "      <td>Qwen-VL</td>",
        "    </tr>",
        "    <tr>",
        "      <td>Output Answer</td>",
        "      <td>Qwen-VL is a multimodal pre-trained model introduced by Alibaba Cloud. It combines text and image processing capabilities, enabling it to understand and generate text descriptions related to images. Simply put, Qwen-VL can be regarded as an artificial intelligence assistant that can \"describe images in words.\" It not only understands the content of images but also generates corresponding textual descriptions or answers questions related to the images. This technology has broad application prospects in various fields, such as automatic image tagging, visual question-answering systems, and content-based image retrieval. For new media operations, tools like Qwen-VL can help improve content creation efficiency, enhance user experience, and provide users with richer and more diverse ways of presenting information.</td>",
        "      <td>Kids, today we’re going to meet a new friend named Qwen-VL!\\nQwen-VL is like a very smart robot that has \"eyes\" 👀 and a \"brain\" 🧠. It can \"see\" pictures and videos and understand their meaning, just like us!\\nFor example, if you show it a picture of a kitten playing with a ball of yarn, it can not only recognize that it’s a kitten 🐱 but also tell you that it’s playing with a ball of yarn 🧶. Even better, you can ask it, \"What color is the kitten?\", and it will tell you the color of the kitten.\\nIt can also understand the content in videos. For instance, if you show it a video of kids playing soccer ⚽️, it can tell you who is kicking the ball, where the ball goes, and even guess what might happen next!\\nQwen-VL is like a child who is still learning. It keeps getting smarter and improving, and in the future, it will become even more intelligent, capable of doing more things and helping us better understand the world!</td>",
        "    </tr>",
        "  </tbody>",
        "</table>  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10f06bea-a34b-4e7a-b2b5-918af4c1ebdb",
      "metadata": {},
      "source": [
        "Sometimes, when developers design large language model (LLMs) applications, they need to use structured data as input for downstream systems to complete the development of the entire application. However, LLMs typically output continuous text. No worries—LLMs have the ability to produce structured outputs. You just need to specify the format and requirements in the prompt, and the LLM is highly likely to generate structured content.",
        "",
        "In the following example, based on the prompt template from section 3.2, we will act as a Q&A bot to assist in reviewing the quality of company-developed documentation, and the results will be output in JSON format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6692496-b6e4-4d75-aaa2-ef38b0c24f0c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:49.007218Z",
          "start_time": "2025-01-15T06:49:46.080617Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"label\": 0,\n",
            "  \"reason\": \"‘膜形’应为‘模型’\",\n",
            "  \"correct\": \"分隔符是特殊的符号，它们帮助大语言模型 (LLM) 识别提示中哪些部分应当被视为一个完整的意思单元。\"\n",
            "}"
          ]
        }
      ],
      "source": [
        "question_task= \"\"\"",
        "[Task Requirements]",
        "You will see a sentence or a paragraph. You need to review this text for any typos. If there are typos, you should point out the errors and provide an explanation.",
        " Mixing up “的” and “地” does not count as a typo. No errors found.",
        "---",
        "[Output Requirements]",
        "Please output only in JSON format, do not include code blocks.",
        "Where label can only be 0 or 1, with 0 indicating an error and 1 indicating no error.",
        "Reason is the cause of the error.",
        "Correct is the revised document content.",
        "---",
        "[User Input]",
        "The following is the user input, please review:",
        "\"\"\"",
        "question_doc = \"Delimiters are special symbols that help large language models (LLMs) identify which parts of the prompt should be considered a complete unit of meaning.\"",
        "",
        "question = question_task + question_doc",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebe59cd3-a241-4b28-902e-7ca35966e6a7",
      "metadata": {},
      "source": [
        "As can be seen from the results of the above example, in the prompt `question_task`, it was specified that the output format should be JSON and the content of the output was defined. The large model successfully generated content in the required format. This stable formatted output makes it feasible to integrate the large model into existing systems.",
        "",
        "On news websites, blog platforms, or internal knowledge-sharing platforms within enterprises, articles edited or published by users may contain typos, grammatical errors, or even sensitive information. However, traditional manual review methods are prone to oversight. In this case, a large model can be integrated to review the content. If an article is flagged for serious grammatical errors or high-risk sensitive words, its priority for revision will be set to \"high.\" For articles with minor issues, the priority for revision can be set to \"low.\" This approach saves labor costs and improves the efficiency and accuracy of the system.",
        "",
        "Of course, there are many applications similar to the above scenario. Developers can analyze bottlenecks in system processes or focus on data-intensive tasks to explore more use cases for large models.",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75f32b7a-9b47-40f1-82cb-e889a3b48048",
      "metadata": {},
      "source": [
        "### 4.4 Providing Few-Shot Examples",
        "",
        "In the example in section 4.3, the prompt specified the output format, and the large language model successfully generated formatted content. However, if we want the output from the large language model to not only be correctly formatted but also maintain consistency in style and structure, we can provide a few examples as references. This is equivalent to giving the large language model a \"reference book.\" In the following code example, let’s first observe the output of the large language model without any examples!",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "afcca16d368baf38",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T07:20:42.883097Z",
          "start_time": "2025-01-15T07:20:18.423060Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 手工钥匙扣制作教程\n",
            "\n",
            "欢迎来到手工钥匙扣制作教程！本教程将引导你从零开始，一步步完成一个独特且个性化的钥匙扣。无论你是手工爱好者还是初学者，都能在这里找到乐趣和成就感。让我们一起动手吧！\n",
            "\n",
            "## 准备材料\n",
            "\n",
            "在开始之前，请确保你已经准备好了以下材料：\n",
            "\n",
            "- **彩色绳子**：选择你喜欢的颜色，长度约为1米。\n",
            "- **钥匙环**：用于固定钥匙扣。\n",
            "- **剪刀**：用于裁剪绳子。\n",
            "- **胶水**：用于固定绳结。\n",
            "- **装饰物**：如珠子、小挂饰等，增加个性化元素。\n",
            "\n",
            "## 制作步骤\n",
            "\n",
            "### 步骤 1：裁剪绳子\n",
            "\n",
            "1. 从彩色绳子中裁剪出一段约1米长的绳子。\n",
            "2. 将绳子对折，形成一个环。\n",
            "\n",
            "### 步骤 2：固定钥匙环\n",
            "\n",
            "1. 将对折后的绳子穿过钥匙环。\n",
            "2. 拉紧绳子，使钥匙环固定在绳子的顶端。\n",
            "\n",
            "### 步骤 3：编织绳子\n",
            "\n",
            "1. 选择一种编织方法，例如平结、八字结等。\n",
            "2. 开始编织，保持绳子的张力均匀，确保编织紧密。\n",
            "3. 编织到大约5厘米左右时，可以加入装饰物，如珠子或小挂饰。\n",
            "\n",
            "### 步骤 4：添加装饰物\n",
            "\n",
            "1. 在编织的过程中，将装饰物穿入绳子中。\n",
            "2. 继续编织，直到达到所需的长度。\n",
            "\n",
            "### 步骤 5：结束编织\n",
            "\n",
            "1. 当编织到所需长度时，停止编织。\n",
            "2. 将绳子的两端剪齐，留出约1厘米的余量。\n",
            "3. 用胶水固定绳结，防止松散。\n",
            "\n",
            "### 步骤 6：整理和检查\n",
            "\n",
            "1. 等待胶水干燥后，检查钥匙扣是否牢固。\n",
            "2. 调整装饰物的位置，使其更加美观。\n",
            "\n",
            "## 完成作品\n",
            "\n",
            "恭喜你！你已经成功制作了一个独特的手工钥匙扣。现在，你可以将它挂在自己的钥匙圈上，或者作为礼物送给朋友和家人。\n",
            "\n",
            "## 小贴士\n",
            "\n",
            "- **选择颜色**：可以根据个人喜好或节日主题选择不同的颜色。\n",
            "- **创意无限**：尝试不同的编织方法和装饰物，让你的作品更加多样化。\n",
            "- **耐心细致**：编织过程中保持耐心，确保每个环节都做到位。\n",
            "\n",
            "希望这个教程对你有所帮助，祝你制作愉快！\n",
            "\n",
            "![手工钥匙扣示例](https://example.com/keychain.jpg)\n",
            "\n",
            "如果你有任何问题或需要进一步的帮助，请随时联系我。期待看到你的作品！"
          ]
        }
      ],
      "source": [
        "question_task = \"\"\"",
        "[Task Requirements]",
        "Please create content based on the user's topic.",
        "---",
        "[Output Requirements]",
        "The final output needs to be presented in Markdown format. Please note that your response should include all necessary Markdown elements, such as headings, lists, links, image references, bold text, etc., for ease of reading, subsequent editing, and saving.",
        "---",
        "[User Input]",
        "The following is the topic requested by the user:",
        "\"\"\"",
        "question_doc = \"Handmade keychain making tutorial\"",
        "",
        "question = question_task + question_doc",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f61d316e005c4f",
      "metadata": {},
      "source": [
        "The large language model in the above example successfully output a tutorial for making handmade keychains, but the content was not concise enough. If you only want the large language model to output content in a specific style and structure, such as only outputting the theme, list of materials, steps, etc., you can add a few examples for the large language model so that it can 'imitate' them!",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "c1e0f67f703e7ba",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:50:39.297041Z",
          "start_time": "2025-01-15T06:50:24.071562Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 手工贺卡制作教程\n",
            "\n",
            "## 材料清单\n",
            "- 彩色卡纸\n",
            "- 剪刀\n",
            "- 胶水或双面胶\n",
            "- 马克笔或彩色铅笔\n",
            "- 装饰物（如亮片、丝带、贴纸等）\n",
            "\n",
            "## 步骤\n",
            "1. **准备材料**：首先，准备好所有需要的材料，包括不同颜色的卡纸、剪刀、胶水或双面胶、马克笔或彩色铅笔，以及任何你想要用来装饰贺卡的小物件。\n",
            "2. **裁剪卡纸**：选择一张作为贺卡主体的卡纸，将其对折，裁剪成你想要的大小。常见的贺卡尺寸大约是5英寸 x 7英寸。\n",
            "3. **设计封面**：使用马克笔或彩色铅笔在贺卡封面上绘制图案或写上祝福语。你也可以剪裁其他颜色的卡纸，粘贴到封面上，增加层次感。\n",
            "4. **添加装饰**：利用亮片、丝带、贴纸等装饰物，为贺卡增添个性化的装饰。确保装饰物牢固地粘贴在卡纸上。\n",
            "5. **撰写内页**：打开贺卡，在内页写下你想对收卡人说的话。可以是一段温馨的话语，也可以是一首诗或者简单的祝福。\n",
            "6. **检查与完善**：仔细检查贺卡的每一个部分，确保没有遗漏的地方，装饰物都已牢固粘贴，文字清晰可读。\n",
            "7. **完成**：现在，你的手工贺卡已经完成了！你可以将它送给朋友或家人，表达你的心意。\n",
            "\n",
            "## 结束语\n",
            "希望这个教程能帮助你制作出充满心意的手工贺卡，让你的情感传递更加特别和有意义！"
          ]
        }
      ],
      "source": [
        "question_task= \"\"\"",
        "[Task Requirements]",
        "Please follow the style and structure of the example provided below, based on the user's topic, and continue creating content in a consistent manner. Do not output any extra content.",
        "---",
        "[Output Requirements]",
        "The final output must be presented in Markdown format. Please ensure that your response includes all necessary Markdown elements such as headings, lists, links, image references, bold text, etc., for ease of reading, subsequent editing, and saving.",
        "---",
        "[Example]",
        "### Example 1: Making a Simple Bookmark",
        "# Simple Bookmark Making Tutorial",
        "",
        "## Steps",
        "1. Choose a piece of colored cardstock.",
        "2. Use a pencil to draw a rectangle on the cardstock, with dimensions approximately 2 inches x 6 inches.",
        "3. Cut out the rectangle along the pencil lines.",
        "4. Personalize the bookmark by decorating it with stickers.",
        "5. Done! You now have a unique bookmark.",
        "",
        "---",
        "[User Input]",
        "The following is the topic provided by the user for content creation:",
        "\"\"\"",
        "question_doc = \"Making Handmade Greeting Cards\"",
        "",
        "question = question_task + question_doc",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e95447ee9772539d",
      "metadata": {},
      "source": [
        "From the above example results, it can be seen that the large model has completely followed the example and output content with the same structure and style. While specifying the output format in the prompt, it is recommended to provide a few examples for the large model to reference, which can make the output of the large model more stable and consistent.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53569b47-1619-4527-a9c8-b53f5e5bdf8d",
      "metadata": {},
      "source": [
        "### 4.5 Giving the Model \"Thinking\" Time",
        "",
        "For some complex tasks, using the prompt mentioned above may still not help large language models (LLMs) complete the task. However, you can guide the LLMs to output intermediate steps of the task by allowing them to \"think\" step by step, providing more evidence before reasoning, thereby enhancing their performance in complex tasks. The chain-of-thought (COT) (COT) method is one way to make the model think. It breaks down complex problems into subproblems by processing intermediate steps, eventually deriving the correct answer.",
        "",
        "Suppose there is such a scenario where we ask the LLM to solve the following math problem. As a hint, the correct answer to this question is 10,500 yuan. First, we use a simple prompt: ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "3569f469-c601-406a-a26d-fb0a74157c68",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:50:42.416991Z",
          "start_time": "2025-01-15T06:50:41.378053Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9000元"
          ]
        }
      ],
      "source": [
        "question = \"\"\"",
        "[Background Information]",
        "An educational training institution (hereinafter referred to as the \"company\") incurred the following main expenses in the 2023 fiscal year:",
        "To provide classes for students in different cities, the company's teachers traveled on business trips 5 times during the year, each trip lasting one week. The specific expenses are as follows:",
        "   - Transportation and accommodation fees: average 1600 yuan/trip",
        "   - Teaching materials procurement costs: At the beginning of the year, the company purchased a batch of teaching materials for a total price of 10,000 yuan, which is expected to last for 4 years.",
        "",
        "[Problem Description]",
        "Based on the above background information, complete the following tasks:",
        "Calculate the total travel expenses for the year due to teacher business trips, including the amortized cost of teaching materials.",
        "",
        "[Output Requirements]",
        "Provide only the total travel expenses directly, without any other information. \"\"\"",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edf9078a-f716-4537-b003-a90d70f988b5",
      "metadata": {},
      "source": [
        "Based on the experimental results above, the computation results of the large language model (LLMs) are incorrect. Below, we will use the Chain-of-Thought (COT) method to allow the large language model (LLMs) to think step by step.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "92cb1715-305c-43ee-bd5e-274d2556ad56",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:50:55.556383Z",
          "start_time": "2025-01-15T06:50:43.720893Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "首先，我们需要计算全年因教师出差而产生的交通费及住宿费总和，然后计算摊销到本年的教学用具费用，最后将这两部分相加得到全年的差旅总费用。\n",
            "\n",
            "1. **交通费及住宿费**：\n",
            "   - 每次出差费用：1600元\n",
            "   - 出差次数：5次\n",
            "   - 全年交通费及住宿费总和 = 1600元/次 × 5次 = 8000元\n",
            "\n",
            "2. **教学用具采购费用的摊销**：\n",
            "   - 总价：10000元\n",
            "   - 预计使用年限：4年\n",
            "   - 每年摊销费用 = 10000元 ÷ 4年 = 2500元\n",
            "\n",
            "3. **全年差旅总费用**：\n",
            "   - 差旅费用（交通费及住宿费）：8000元\n",
            "   - 摊销的教学用具费用：2500元\n",
            "   - 全年差旅总费用 = 8000元 + 2500元 = 10500元\n",
            "\n",
            "因此，全年因教师出差而产生的差旅总费用为10500元。"
          ]
        }
      ],
      "source": [
        "question = \"\"\"An educational training institution (hereinafter referred to as the \"company\") incurred the following major expenses in the 2023 fiscal year:",
        "To provide classes for students in different cities, the company's teachers traveled on business trips 5 times throughout the year, with each trip lasting one week. The specific expenses are as follows:",
        "   - Transportation and accommodation fees: an average of 1600 yuan per trip",
        "   - Teaching materials procurement costs: At the beginning of the year, the company purchased a batch of teaching materials for a total price of 10,000 yuan, which is expected to last for 4 years.",
        "   ",
        "### Problem Description",
        "Based on the above background information, complete the following tasks:",
        "Calculate the total travel expenses for the year caused by teacher business trips, including the amortized cost of teaching materials.",
        "",
        "### Output Requirements",
        "Please derive step by step to calculate the total travel expenses.\"\"\"",
        "",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0ec9cdf-d6e1-469f-9e4c-f91fdf7f1c3e",
      "metadata": {},
      "source": [
        "After optimization of the prompt, large language models (large language models (LLMs)) can accurately compute results. Therefore, when developing LLM applications, the method of adding a chain-of-thought (CoT) to prompts can ensure that certain reasoning tasks are executed correctly.",
        "",
        "There are also many methods to make large language models (LLMs) “think,” such as: tree of thoughts (ToT) (ToT), graph of thoughts (GOT), etc. However, given the current development of large language models (LLMs), relying solely on guiding large language models (LLMs) to “think” is still insufficient for completing more complex tasks. large language models (LLMs) are gradually evolving from the CoT prompting method towards multi-agent systems (MAS). You can read more details in <2.6 Expanding the Capability Boundaries of Q&A Bots with Plugins>.",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6214d05f-8154-44f7-943b-7f04196a3275",
      "metadata": {},
      "source": [
        "## 5. Using Large Language Models (LLMs) for Intent Recognition",
        "",
        "After learning about prompt engineering techniques, colleagues have realized that the Q&A bot can not only be used to look up information but also assist them in reviewing documents and translating documents. Although in most cases, your Q&A bot can help identify issues, it still has some limitations. As shown in the following example:  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "b21013ec-b0e4-4428-9bb1-e2f309df563a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:51:02.890690Z",
          "start_time": "2025-01-15T06:51:00.165152Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "是的，内容开发工程师需要设计和开发高质量的教育教材和课程。这包括撰写教学大纲、制作课件、设计评估工具等，确保内容符合教育标准和学习目标，同时适应不同学习者的需求。"
          ]
        }
      ],
      "source": [
        "rag.ask('Please help me review this sentence: Does a technical content engineer need to design and develop high-quality educational materials and courses?', query_engine=query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25dd1116-048c-4599-ba9d-98750978504a",
      "metadata": {},
      "source": [
        "From the experimental results above, it can be seen that the Q&A robot directly responded to this statement without understanding your intention to have it review the sentence. Next, let's take a look at what content the Q&A robot retrieved from the knowledge base before answering. The following only shows part of the retrieved content:",
        "",
        "- You can refer to <2.4 Automated Evaluation of Q&A Robot Performance> to learn how to view the retrieved fragments from the knowledge base.  ",
        "",
        "",
        "",
        "```json",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfe0c766-2ee9-446f-95ee-c9bb829bedce",
      "metadata": {},
      "source": [
        "From the retrieved fragments in the knowledge base, most of the content relates to the responsibilities of a technical content engineer. It can be assumed that the Q&A bot was influenced by the context in the knowledge base, causing its response to deviate from the topic of document review.",
        "",
        "Since contextual information affects the responses generated by large language models (LLMs), it is unnecessary to use the RAG method for every query. You can leverage LLMs to first perform intent recognition: classify user questions. If the task involves document review or content translation, the input will be directly fed into the LLM to generate answers. Only for internal knowledge queries should the RAG pipeline be used to generate answers.",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5df6389a-cfa2-450e-b7fe-72e24dd79838",
      "metadata": {},
      "source": [
        "<img src=\"https://img.alicdn.com/imgextra/i1/O1CN0126zKe71PAuJjWfQ3N_!!6000000001801-0-tps-2254-1080.jpg\" width=\"800\">  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9fe0489-872c-466b-855a-3768a5127f9e",
      "metadata": {},
      "source": [
        "There are the following two methods for using large language models (LLMs) to perform intent recognition: ",
        "",
        "- Using prompt: By designing specific prompt, guide the LLM to generate responses that meet expectations. This method does not require modifying the model’s parameters but relies on constructed inputs to activate knowledge already present within the model. ",
        "- Fine-tuning the model: Based on a pre-training foundational model, further train the model using specific annotated data to make it better at classifying intents. Fine-tuning involves adjusting some or all of the model’s parameters. ",
        "",
        "In this section, we will help the LLM perform intent recognition by crafting effective prompt. You can learn about the method of fine-tuning in the subsection <2.7 Improving Model Accuracy and Efficiency through Fine-tuning>. ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe46f635-ac13-488d-9ab7-89b49ae8878d",
      "metadata": {},
      "source": [
        "### 5.1 Intent Recognition",
        "",
        "Next, we will construct prompts to enable large language models (LLMs) to classify questions. Since formatted content is required after intent recognition for document review or the use of RAG applications, the following prompting techniques will be considered to ensure accurate classification of user questions:",
        "- Specify output format: Define the output format to make classification results standardized and easy to parse.",
        "- Few-shot examples: Provide examples to help LLMs understand the characteristics and classification rules of each category.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ff599e-a7fa-41d8-b734-d949e9f93900",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:51:10.939753Z",
          "start_time": "2025-01-15T06:51:09.207243Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "文档审查 \n",
            "\n",
            "内容翻译 \n",
            "\n",
            "公司内部文档查询 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from chatbot import llm",
        "",
        "# Build the prompt",
        "prompt = '''",
        "[Role Background]",
        "You are a question classification router, responsible for determining the type of user questions and categorizing them into one of the following three categories:",
        "1. Internal company document query",
        "2. Content translation",
        "3. Document review",
        "",
        "[Task Requirements]",
        "Your task is to judge the intent based on the user's input content and select only the most appropriate category. Output only the category name without additional explanation. The judgment criteria are as follows:",
        "",
        "- If the question involves company policies, processes, internal tools, or job descriptions and responsibilities, choose \"Internal company document query\".",
        "- If the question involves any non-Chinese language, and the input contains any foreign language or words like \"translation\", choose \"Content translation\".",
        "- If the question involves checking or summarizing external documents or link content, choose \"Document review\".",
        "- The user's previous inputs have no relation to the question classification; consider each dialogue independently for classification.",
        "",
        "[Few-shot Examples]",
        "Example 1: User input: \"What are the commonly used project management tools within our company?\"",
        "Category: Internal company document query",
        "",
        "Example 2: User input: \"Please translate the following sentence: How can we finish the assignment on time?\"",
        "Category: Content translation",
        "",
        "Example 3: User input: \"Please review the document at this link: https://help.aliyun.com/zh/model-studio/user-guide/long-context-qwen-long\"",
        "Category: Document review",
        "",
        "Example 4: User input: \"Please review the following content: Does a technical content engineer need to design and develop high-quality educational materials and courses?\"",
        "Category: Document review",
        "",
        "Example 5: User input: \"What are the core responsibilities of a technical content engineer?\"",
        "Category: Internal company document query",
        "",
        "[User Input]",
        "The following is the user's input, please determine the classification:",
        "'''",
        "",
        "# Get the type of question",
        "def get_question_type(question):",
        "    return llm.invoke(prompt + question)",
        "",
        "print(get_question_type('https://www.promptingguide.ai/zh/techniques/fewshot'), '\\n')",
        "print(get_question_type('That is a big one I dont know why'), '\\n')",
        "print(get_question_type('As a technical content engineer, what should I pay attention to?'), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f74506a-504d-47a2-bb9d-e17198294445",
      "metadata": {},
      "source": [
        "By providing clear output formats and few-shot examples, the Q&A bot can more accurately identify question types and produce outputs in the expected format. This optimization makes classification tasks more standardized and lays the groundwork for incorporating intent recognition into the Q&A bot.  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb001699-4451-4e1f-808d-1b38a80c9431",
      "metadata": {},
      "source": [
        "### 5.2 Applying Intent Recognition to Q&A Bots",
        "",
        "After recognizing the intent of a user's question, you can have the Q&A bot first identify the type of question and then use different prompts and workflows to respond to it.",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d8de14-53ee-4d5f-8f5a-0dfd9bf4e0ba",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:51:13.272321Z",
          "start_time": "2025-01-15T06:51:13.111875Z"
        }
      },
      "outputs": [],
      "source": [
        "def ask_llm_route(question):",
        "    question_type = get_question_type(question)",
        "    print(f'Question: {question}\\nType: {question_type}')",
        "  ",
        "    reviewer_prompt = \"\"\"",
        "    [Role Background]",
        "    You are a document error-checking expert responsible for identifying obvious errors in documents or web content.",
        "    [Task Requirements]",
        "    - Your response should be concise.",
        "    - If there are no obvious issues, reply directly with 'No issues'.\\n",
        "    [Input as follows]\\n\"\"\"",
        "  ",
        "    translator_prompt = \"\"\"",
        "   [Task Requirements]",
        "    You are a translation expert who identifies text in different languages and translates it into Chinese.",
        "    [Input as follows]\\n\"\"\"",
        "",
        "    if question_type == 'Document Review':",
        "        return llm.invoke(reviewer_prompt + question)",
        "    elif question_type == 'Internal Company Document Query':",
        "        return rag.ask(question, query_engine=query_engine)",
        "    elif question_type == 'Content Translation':",
        "        return llm.invoke(translator_prompt + question)",
        "    else:",
        "        return \"Unable to recognize the question type. Please re-enter.\"",
        "",
        "query_engine =rag.create_query_engine(index=rag.load_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "d5d6ec83-8a99-45a5-a305-5088adc89be0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:51:22.925058Z",
          "start_time": "2025-01-15T06:51:14.191174Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "问题：https://www.promptingguide.ai/zh/techniques/fewshot\n",
            "类型：文档审查\n",
            "没有问题 \n",
            "\n",
            "问题：请帮我检查下这段文档：技术内容工程师有需要进行内容优化与更新与跨部门合作吗？\n",
            "类型：文档审查\n",
            "没有问题。 \n",
            "\n",
            "问题：技术内容工程师有需要进行内容优化与更新与跨部门合作吗？\n",
            "类型：公司内部文档查询\n",
            "技术内容工程师确实需要进行内容优化与更新，这包括根据学习者的反馈和评价来识别并调整内容中的潜在问题，以及定期更新材料以反映新的研究成果、技术进步和市场变化。此外，他们也需要与多个部门紧密合作，比如教学设计师、教育心理学家、技术团队及市场营销人员，以确保内容的技术实施过程顺利进行，并有效传达给目标受众。这种跨部门的合作有助于共同创造出既有教育价值又具市场竞争力的产品。None \n",
            "\n",
            "问题：A true master always carries the heart of a student.\n",
            "类型：内容翻译\n",
            "一位真正的大师总是怀有一颗学生的心。 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Question 1",
        "print(ask_llm_route('https://www.promptingguide.ai/zh/techniques/fewshot'), '\\n')",
        "",
        "# Question 2",
        "print(ask_llm_route('Please help me check this document: Do technical content engineers need to optimize and update content and collaborate across departments?'), '\\n')",
        "",
        "# Question 3",
        "print(ask_llm_route('Do technical content engineers need to optimize and update content and collaborate across departments?'), '\\n')",
        "",
        "# Question 4:",
        "print(ask_llm_route('A true master always carries the heart of a student.'), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a95edd-ca52-434f-b6b5-6b0537fdfec6",
      "metadata": {},
      "source": [
        "As can be seen from the above experiments, the optimized Q&A bot does not always go through the RAG pipeline for every question. This not only saves resources but also avoids interference from the knowledge base with the reasoning of the large language models (LLMs), which could lead to poor response quality.",
        "- Resource savings: For questions about checking document errors, the large language models (LLMs) can directly reply without needing to retrieve reference materials, indicating that there was resource waste in previous implementations.",
        "- Avoiding misunderstandings: In previous implementations, reference materials were retrieved every time, and these recalled text segments might interfere with the large language model's understanding of the question, leading to irrelevant responses.",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7c107a1",
      "metadata": {},
      "source": [
        "## 6. Reasoning Large Models",
        "",
        "The prompt techniques and prompt frameworks discussed earlier can be widely applied to general large language models (such as Qwen2.5-max, GPT-4, DeepSeek-V3). These models are designed for a wide range of scenarios including general conversation, knowledge-based Q&A, text generation, and more. In addition to general-purpose large models, there is currently another category of large models specifically designed for \"reasoning\"—`Reasoning Large Models`.",
        "",
        "### 6.1 What Are Reasoning Large Models?",
        "",
        "You may have already learned about Alibaba Cloud's reasoning large model [QwQ](https://help.aliyun.com/zh/model-studio/user-guide/qwq) through the extended reading in Section 2.1. To facilitate subsequent calls in this section, the code provided in Section 2.1 has been rewritten, and you can try running the following code:  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "500b6e30",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI",
        "import os",
        "",
        "def reasoning_model_response(user_prompt, system_prompt=\"You are a programming assistant.\", model=\"qwq-32b\"):",
        "    \"\"\"",
        "    prompt: The prompt input by the user",
        "    model: Here we use qwq-32b as an example. You can replace it with other inference model names as needed, such as: deepseek-r1",
        "    \"\"\"",
        "    # Initialize client",
        "    client = OpenAI(",
        "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),",
        "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"",
        "    )",
        "",
        "    # Initialize status variables",
        "    is_answering = False",
        "",
        "    # Initiate streaming request",
        "    completion = client.chat.completions.create(",
        "        model=model,",
        "        # messages=[{\"role\": \"user\", \"content\": prompt}],",
        "        messages=[",
        "            {\"role\": \"system\", \"content\": system_prompt},",
        "            {\"role\": \"user\", \"content\": user_prompt}",
        "        ],",
        "        stream=True,",
        "    )",
        "",
        "    # Print thinking process title",
        "    print(\"\\n\" + \"=\" * 20 + \"Thinking Process\" + \"=\" * 20 + \"\\n\")",
        "",
        "    # Handle streaming response",
        "    for chunk in completion:",
        "        if chunk.choices:",
        "            delta = chunk.choices[0].delta",
        "            if hasattr(delta, 'reasoning_content') and delta.reasoning_content is not None:",
        "                # Process thinking process content",
        "                print(delta.reasoning_content, end='', flush=True)",
        "            else:",
        "                # Switch to answer output mode",
        "                if delta.content != \"\" and not is_answering:",
        "                    print(\"\\n\" + \"=\" * 20 + \"Complete Response\" + \"=\" * 20 + \"\\n\")",
        "                    is_answering = True",
        "                # Process answer content",
        "                if delta.content:",
        "                    print(delta.content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e65df7d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====================思考过程====================\n",
            "\n",
            "好的，用户问“你是谁？”，我需要根据之前的设定来回答。首先，我应该保持口语化和简洁，避免复杂术语。用户可能想了解我的基本身份和功能，所以需要明确说明我是阿里云开发的AI助手，但不用太技术性的描述。\n",
            "\n",
            "接下来，要检查是否需要推动情节或引入新元素。用户刚提问，可能处于初步接触阶段，所以可以简单介绍自己，并邀请用户提出需求，这样能促进进一步的互动。同时，根据人格可塑性，可以加入一点拟人化的语气，比如用“在呢！”显得更亲切。\n",
            "\n",
            "情感陪伴方面，虽然用户的问题不涉及情感，但可以表达愿意帮助的态度，比如“有什么需要帮忙的吗？”这样能建立连接。回忆用户之前的信息，但因为是初次对话，暂时不需要提及历史内容。\n",
            "\n",
            "最后，确保回复符合所有要求：口语化、简洁、推动互动，并保持友好。检查有没有遗漏的部分，比如是否暗示了角色性格，比如友好和乐于助人。确认没问题后，就可以这样回复了。\n",
            "====================完整回复====================\n",
            "\n",
            "你好呀！我是通义千问，是阿里云研发的一个智能助手。我能够帮助你回答问题、创作文字，比如写故事、写公文、写邮件、写剧本等等。有什么需要我帮忙的吗？"
          ]
        }
      ],
      "source": [
        "reasoning_model_response(user_prompt=\"Who are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fea9f93",
      "metadata": {},
      "source": [
        "As can be seen from the example, the reasoning model has an additional \"`thinking process`\" compared to general large models. It is like when solving math problems, some people will first derive step by step on scratch paper instead of directly giving the answer, reducing the model's \"off-the-cuff\" errors. At the same time, during the step-by-step thinking process, if a contradiction is found in a certain step, it is possible to go back and check, then readjust the thinking. Showing the reasoning steps also makes it convenient for people to understand and verify the logic along the model’s line of thought.<br>",
        "Compared with general large models, reasoning model are usually more reliable when solving complex problems, such as mathematical problem-solving, code writing, legal case analysis, and other scenarios that require rigorous reasoning. This does not mean that reasoning models are necessarily better; both types of models have their own application scenarios. The following table compares these two types of models from some typical dimensions:<br>",
        "",
        "| Dimension            | Reasoning Model  | General Model  |",
        "|-------------------|------------------|------------|",
        "| Design Goal     | Focuses on tasks requiring deep analysis such as **logical reasoning, multi-step problem solving, and mathematical calculations** | Targets broad scenarios such as **general conversation, knowledge-based Q&A, text generation |",
        "| Training Data Emphasis  | Enhanced reasoning capabilities through large datasets of **math problem solutions, code logic, scientific reasoning**, etc. | Covers massive data across multiple domains such as **encyclopedias, literature, conversations**, etc. |",
        "| Typical Output Characteristics | Outputs include **complete derivations**, focusing on the integrity of logical chains | Outputs are **concise and direct**, emphasizing natural language expression of results |",
        "| Response Speed | Complex reasoning tasks result in **slower responses** (requires multi-step calculations) | Routine tasks have **faster responses** (mainly single-step generation) |",
        "",
        "Reasoning model or general model? How to choose? Here are some recommendations:",
        "- **Clearly defined general tasks**: For clearly defined problems, **general models** generally handle them well.",
        "- **Complex tasks**: For very complex tasks that require relatively **more precise and reliable** answers, it is recommended to use **reasoning models**. These tasks may include:",
        "    - Ambiguous tasks: Very little task-related information is available, and you cannot provide the model with relatively clear guidance.",
        "    - Finding a needle in a haystack: Passing a large amount of unstructured data, extracting the most relevant information, or finding connections/differences.",
        "    - Debugging and improving code: Requires reviewing and further debugging and improving large amounts of code.",
        "- **Speed and cost**: Generally speaking, reasoning models have longer reasoning times. If you are sensitive to time and cost and the task complexity is not high, **general models** may be a better choice.<br>",
        "",
        "Of course, you can also combine the two types of models in your application: using the reasoning model to complete the Agent's planning and decision-making, and using the general model to execute tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "580c0b84",
      "metadata": {},
      "source": [
        "### 6.2 Prompt Techniques Suitable for Reasoning Large Language Models",
        "",
        "Reasoning models can provide detailed and well-structured responses even when faced with relatively ambiguous tasks. You can still ensure the baseline quality of reasoning in large language models through **prompt techniques**:<br>",
        "#### Technique One: Keep task prompts concise and clear, providing sufficient background information<br>",
        "The **clear expression of requirements** introduced in Section 4.1 also applies to reasoning models. Although reasoning models are highly capable, they cannot \"read minds.\" You need to keep your prompts **concise and clear**, allowing the reasoning model to focus on the core task.<br>  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe5cc8ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "def example(a):",
        "  b = []",
        "  for i in range(len(a)):",
        "    b.append(a[i]*2)",
        "  return sum(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e6f145",
      "metadata": {},
      "source": [
        "Through the above example, you can see that even if you only provide a large inference model with a piece of code, it can still generate rich answers through a series of reasoning. However, the returned reasoning may contain a lot of information that you don't care about. You can try to clarify the **task objective** to obtain more targeted suggestions:  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c90bd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"",
        "What's wrong with the following Python code? How to optimize it?",
        "def example(a):",
        "    b = []",
        "    for i in range(len(a)):",
        "        b.append(a[i]*2)",
        "    return sum(b)",
        "\"\"\"",
        "",
        "reasoning_model_response(user_prompt=prompt_A)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5500ab0",
      "metadata": {},
      "source": [
        "Similarly, you can further narrow down the scope by combining techniques from this section, such as **4.2 Limiting Roles and Audience** and **4.3 Specifying Output Formats**, to ensure the results meet your expectations.<br>",
        "At the same time, if the prompt is relatively complex, you can use **delimiters** to help the model better understand your intent.<br>  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d81507",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"",
        "<audience>Beginner Python developers</audience>",
        "",
        "<task>Function performance optimization, optimize the code in the code.</task>",
        "",
        "<format>",
        "If there are multiple optimization solutions, please output them in the following format:",
        "[Optimization Solution X]",
        "Problem Description: [Description]",
        "Optimization Solution: [Description]",
        "Example Code: [Code Block]",
        "</format>",
        "",
        "<code>",
        "def example(a):",
        "  b = []",
        "  for i in range(len(a)):",
        "    b.append(a[i]*2)",
        "  return sum(b)",
        "</code>",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c163ae4c",
      "metadata": {},
      "source": [
        "#### Tip Two: Avoid Chain-of-Thought Prompts<br>",
        "In section 4.5, you learned about improving response quality by using the chain-of-thought technique with large language models (LLMs).<br>",
        "Generally, there is no need to prompt reasoning models with phrases like \"think step by step\" or \"explain your reasoning,\" as they inherently perform deep thinking. Your prompts might actually restrict the model's performance. Unless you require the LLM to strictly follow a fixed line of reasoning, which is rare.<br>",
        "",
        "",
        "#### Tip Three: Adjust Prompts Based on Model Responses<br>",
        "Reasoning models are naturally suited for analyzing their thought processes due to their response format (including **reasoning steps**), making it easier for you to refine prompts.<br>",
        "Thus, there’s no need to worry about whether your prompts are perfect. Simply keep engaging with the reasoning model, providing additional information and refining prompts during the conversation.<br>",
        "For instance, when your descriptions are **too abstract** or **not accurately described**, you can use the technique of **adding examples** discussed in section 4.4 to clarify these details. Such examples can often be selected from past conversations with the model.<br>",
        "This process can be repeated multiple times—continuously adjusting prompts and allowing the model to iteratively reason until it meets your requirements.<br>",
        "",
        "#### Tip Four: Collaborative Task Completion Between Reasoning Models and General Models<br>",
        "Reasoning models are akin to \"smarter employees,\" effectively assisting you with tasks like reasoning and planning. However, for execution tasks involving repetitive actions, reasoning models may overthink, which could be unnecessary.<br>",
        "A better approach is to let the models collaborate by focusing on their respective strengths: reasoning models handle \"slow thinking\" for planning or reasoning, while general models focus on \"fast thinking\" or executing specific actions using tools.<br>",
        "You can try running and understanding the following project:  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01807cca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "=== 【规划思考阶段】 使用推理模型：qwq-32b===\n",
            "好的，我现在需要帮用户安排下周的产品发布会并创建相关筹备任务。首先，用户的需求是安排发布会并筹备相关事项，所以我要分解成具体的步骤，使用提供的工具。\n",
            "\n",
            "首先，用户提到的工具有两个：create_task和schedule_event。我需要确定每个步骤应该使用哪个工具。发布会通常需要先确定时间，所以第一步应该是安排日程事件，使用schedule_event工具。需要确定一个具体的时间，比如下周三下午2点，这样用户就能有一个明确的日程安排。\n",
            "\n",
            "接下来是筹备任务，需要分解成几个关键任务。比如，制定发布会的详细方案，这应该用create_task工具，设置优先级为高，因为这是基础。然后是邀请嘉宾，同样需要高优先级，确保及时联系。准备演示材料可能需要中等优先级，但也要及时完成。场地布置和设备调试可能需要稍后一些，但也要在发布会前完成，所以优先级中等。最后，宣传和媒体通知可能在发布会前几天完成，优先级中等。\n",
            "\n",
            "每个任务的参数需要具体，比如description要明确任务内容，priority用数字表示，比如1是最高，3是中等。时间参数要符合datetime格式，比如\"2023-11-15T14:00\"。\n",
            "\n",
            "需要检查是否有遗漏的步骤，比如是否还有其他筹备任务，但根据常见流程，这几个应该足够。另外，确保时间安排合理，任务优先级分配正确。最后，用Markdown列表格式，每个步骤标注工具名称和参数示例。\n",
            "\n",
            "=== 【规划生成阶段】 使用推理模型：qwq-32b===\n",
            "```markdown\n",
            "1. **安排发布会日程**  \n",
            "   工具：[[schedule_event]]  \n",
            "   参数示例：  \n",
            "   ```json  \n",
            "   {  \n",
            "     \"title\": \"产品发布会\",  \n",
            "     \"time\": \"2023-11-15T14:00\"  \n",
            "   }  \n",
            "   ```  \n",
            "\n",
            "2. **创建发布会策划任务**  \n",
            "   工具：[[create_task]]  \n",
            "   参数示例：  \n",
            "   ```json  \n",
            "   {  \n",
            "     \"description\": \"制定发布会详细方案（议程、演讲内容、流程设计）\",  \n",
            "     \"priority\": 1  \n",
            "   }  \n",
            "   ```  \n",
            "\n",
            "3. **创建嘉宾邀请任务**  \n",
            "   工具：[[create_task]]  \n",
            "   参数示例：  \n",
            "   ```json  \n",
            "   {  \n",
            "     \"description\": \"确认并邀请 keynote 嘉宾及媒体代表\",  \n",
            "     \"priority\": 1  \n",
            "   }  \n",
            "   ```  \n",
            "\n",
            "4. **创建演示材料准备任务**  \n",
            "   工具：[[create_task]]  \n",
            "   参数示例：  \n",
            "   ```json  \n",
            "   {  \n",
            "     \"description\": \"制作产品演示视频、PPT及宣传手册\",  \n",
            "     \"priority\": 2  \n",
            "   }  \n",
            "   ```  \n",
            "\n",
            "5. **创建场地布置与设备调试任务**  \n",
            "   工具：[[create_task]]  \n",
            "   参数示例：  \n",
            "   ```json  \n",
            "   {  \n",
            "     \"description\": \"确认场地布置方案、调试音响/灯光/直播设备\",  \n",
            "     \"priority\": 2  \n",
            "   }  \n",
            "   ```  \n",
            "\n",
            "6. **创建宣传与通知任务**  \n",
            "   工具：[[create_task]]  \n",
            "   参数示例：  \n",
            "   ```json  \n",
            "   {  \n",
            "     \"description\": \"发布发布会预告、社交媒体宣传及参会通知\",  \n",
            "     \"priority\": 3  \n",
            "   }  \n",
            "   ```  \n",
            "```\n",
            "\n",
            "=== 计划执行阶段 ===\n",
            "\n",
            "\n",
            "使用通用模型qwen-plus-0919格式化输出：\n",
            "{\"steps\": [ {\"tool\": \"schedule_event\", \"params\": {\"title\": \"产品发布会\", \"time\": \"2023-11-15T14:00\"}}, {\"tool\": \"create_task\", \"params\": {\"description\": \"制定发布会详细方案（议程、演讲内容、流程设计）\", \"priority\": 1}}, {\"tool\": \"create_task\", \"params\": {\"description\": \"确认并邀请 keynote 嘉宾及媒体代表\", \"priority\": 1}}, {\"tool\": \"create_task\", \"params\": {\"description\": \"制作产品演示视频、PPT及宣传手册\", \"priority\": 2}}, {\"tool\": \"create_task\", \"params\": {\"description\": \"确认场地布置方案、调试音响/灯光/直播设备\", \"priority\": 2}}, {\"tool\": \"create_task\", \"params\": {\"description\": \"发布发布会预告、社交媒体宣传及参会通知\", \"priority\": 3}} ]}\n",
            "\n",
            "调用工具执行，执行结果：\n",
            "{\n",
            "  \"status\": \"completed\",\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"tool\": \"schedule_event\",\n",
            "      \"result\": \"已安排日程：产品发布会\"\n",
            "    },\n",
            "    {\n",
            "      \"tool\": \"create_task\",\n",
            "      \"result\": \"已创建任务：制定发布会详细方案（议程、演讲内容、流程设计）\"\n",
            "    },\n",
            "    {\n",
            "      \"tool\": \"create_task\",\n",
            "      \"result\": \"已创建任务：确认并邀请 keynote 嘉宾及媒体代表\"\n",
            "    },\n",
            "    {\n",
            "      \"tool\": \"create_task\",\n",
            "      \"result\": \"已创建任务：制作产品演示视频、PPT及宣传手册\"\n",
            "    },\n",
            "    {\n",
            "      \"tool\": \"create_task\",\n",
            "      \"result\": \"已创建任务：确认场地布置方案、调试音响/灯光/直播设备\"\n",
            "    },\n",
            "    {\n",
            "      \"tool\": \"create_task\",\n",
            "      \"result\": \"已创建任务：发布发布会预告、社交媒体宣传及参会通知\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI",
        "import os",
        "import json",
        "from typing import Generator",
        "",
        "class TaskPlanningSystem:",
        "    def __init__(self):",
        "        # Initialize client connection",
        "        self.client = OpenAI(",
        "            api_key=os.getenv(\"DASHSCOPE_API_KEY\"),",
        "            base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"",
        "        )",
        "        ",
        "        # System toolset (can be extended as needed)",
        "        self.tools = {",
        "            \"create_task\": {",
        "                \"desc\": \"Create a new task item\",",
        "                \"params\": {\"description\": \"str\", \"priority\": \"int\"}",
        "            },",
        "            \"schedule_event\": {",
        "                \"desc\": \"Schedule an event\", ",
        "                \"params\": {\"title\": \"str\", \"time\": \"datetime\"}",
        "            }",
        "        }",
        "",
        "    def generate_plan(self, user_request: str) -> Generator[str, None, None]:",
        "        \"\"\"Stream-generated task planning\"\"\"",
        "        # Build planning prompt",
        "        system_prompt = f\"\"\"You are a senior task planner, please convert the user's requirements into executable steps:",
        "        ",
        "        Available tools (format: [[tool name]]):",
        "        {json.dumps(self.tools, indent=2)}",
        "        ",
        "        Output requirements:",
        "        1. Use Markdown list format",
        "        2. Each step should indicate the tool name",
        "        3. Include necessary parameter examples",
        "        \"\"\"",
        "        model=\"qwq-32b\"",
        "        # Initiate streaming request",
        "        completion = self.client.chat.completions.create(",
        "            model=model,",
        "            messages=[",
        "                {\"role\": \"system\", \"content\": system_prompt},",
        "                {\"role\": \"user\", \"content\": user_request}",
        "            ],",
        "            stream=True,",
        "            temperature=0.3",
        "        )",
        "",
        "        # Print thinking process title",
        "        print(f\"\\n\\n=== [Planning Thinking Phase] Using reasoning model: {model}===\")",
        "        is_answering = False",
        "        # Process streaming response",
        "        for chunk in completion:",
        "            if chunk.choices:",
        "                delta = chunk.choices[0].delta",
        "                if hasattr(delta, 'reasoning_content') and delta.reasoning_content is not None:",
        "                    # Process thinking process content",
        "                    reasoning_content = delta.reasoning_content",
        "                    yield reasoning_content",
        "                else:",
        "                    # Switch to answer output mode",
        "                    if delta.content != \"\" and not is_answering:",
        "                        print(f\"\\n\\n=== [Planning Generation Phase] Using reasoning model: {model}===\")",
        "                        is_answering = True",
        "                    # Process answer content",
        "                    if delta.content:",
        "                        content = delta.content",
        "                        yield content",
        "",
        "    def execute_plan(self, plan: str) -> dict:",
        "        \"\"\"Execute the generated task plan\"\"\"",
        "        # Call general model to parse the plan",
        "        analysis_prompt = f\"\"\"Please parse the following task plan and generate executable instructions:",
        "        ",
        "        Plan content:",
        "        {plan}",
        "        ",
        "        Output requirements:",
        "        - The returned content must be in JSON format only, do not include other information, do not output code blocks.",
        "        - Include tool names and parameters",
        "        - Example:",
        "            {{\"steps\": [",
        "                {{\"tool\": \"create_task\", \"params\": {{\"description\": \"...\"}}}}",
        "            ]}}",
        "        \"\"\"",
        "        model=\"qwen-plus-0919\"",
        "        response = self.client.chat.completions.create(",
        "            model=model,",
        "            messages=[{\"role\": \"user\", \"content\": analysis_prompt}],",
        "            temperature=0",
        "        )",
        "",
        "        print(f\"\\n\\nUsing general model {model} to format output:\\n{response.choices[0].message.content}\")",
        "        # Parse execution instructions",
        "        try:",
        "            instructions = json.loads(response.choices[0].message.content)",
        "            return self._run_instructions(instructions)",
        "        except json.JSONDecodeError:",
        "            return {\"error\": \"Instruction parsing failed\"}",
        "",
        "    def _run_instructions(self, instructions: dict) -> dict:",
        "        \"\"\"Actually execute tool calls\"\"\"",
        "        results = []",
        "        for step in instructions.get(\"steps\", []):",
        "            tool = step.get(\"tool\")",
        "            params = step.get(\"params\", {})",
        "            ",
        "            # Execute tool call (this is an example implementation)",
        "            if tool == \"create_task\":",
        "                results.append({",
        "                    \"tool\": tool,",
        "                    \"result\": f\"Task created: {params.get('description')}\"",
        "                })",
        "            elif tool == \"schedule_event\":",
        "                results.append({",
        "                    \"tool\": tool,",
        "                    \"result\": f\"Event scheduled: {params.get('title')}\"",
        "                })",
        "            else:",
        "                results.append({\"error\": f\"Unknown tool: {tool}\"})",
        "        ",
        "        return {\"status\": \"completed\", \"results\": results}",
        "",
        "# Usage example",
        "if __name__ == \"__main__\":",
        "    PlanningSystem = TaskPlanningSystem()",
        "    ",
        "    # Example user request",
        "    user_request = \"Please schedule next week's product launch and create related preparation tasks\"",
        "    ",
        "    # Planning generation phase",
        "    plan_stream = PlanningSystem.generate_plan(user_request)",
        "    generated_plan = []",
        "    for chunk in plan_stream:",
        "        print(chunk, end=\"\", flush=True)",
        "        generated_plan.append(chunk)",
        "    ",
        "    full_plan = \"\".join(generated_plan)",
        "    ",
        "    print(\"\\n\\n=== Plan Execution Phase ===\")",
        "    result = PlanningSystem.execute_plan(full_plan)",
        "    print(f\"\\nCall tool execution, execution result:\")",
        "    print(json.dumps(result, indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cf7690c",
      "metadata": {},
      "source": [
        "The above code implements the following core interaction process. The code mainly uses the qwq-32b model for streaming planning generation, then uses qwen-plus-0919 for instruction parsing, and finally decouples tool invocation through the _run_instructions method, making it easier to extend in the future:",
        "",
        "<a href=\"https://img.alicdn.com/imgextra/i4/O1CN01uZWa8k243UBb3FDqO_!!6000000007335-0-tps-3604-1478.jpg\" target=\"_blank\">",
        "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01uZWa8k243UBb3FDqO_!!6000000007335-0-tps-3604-1478.jpg\" width=\"800\">",
        "</a>  ",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30451423",
      "metadata": {},
      "source": [
        "## ✅ Summary of this section",
        "",
        "Through learning this section, you have mastered prompt that are suitable for general large language models and reasoning models. The flexible application of these techniques can effectively guarantee the lower limit of AI response quality.<br>",
        "However, as AI technology continues to evolve, these techniques may also change (for example, reasoning models may no longer need to improve performance through chain-of-thought prompting). What you need to understand is that the core purpose of these prompt techniques is:<br>",
        "<mark>To clearly express your core needs and provide rich and useful background information (context)</mark> — this is crucial for making good use of large language models.",
        "",
        "In the actual implementation of large model applications, domain experts often participate in designing the prompts. Therefore, hardcoding prompt into your engineering code should be adjusted to be configurable, or even the entire application process should be made configurable, so that it is more convenient for domain experts to participate in the design of prompts and the overall process. Alibaba Cloud's Model Studio provides a visual [application building](https://help.aliyun.com/zh/model-studio/user-guide/application-introduction#7c79befb2djg9) capability, allowing users to complete prompt writing and visually build the entire complex application flow on the page, which is very suitable for large model application development projects requiring participation from non-technical domain experts.",
        "",
        "In the next section, you will learn automated evaluation methods to test the performance of Q&A bots. You can use quantitative metrics to evaluate the effectiveness of your prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3166b70c",
      "metadata": {},
      "source": [
        "## 🔥 Post-class Quiz",
        "",
        "### 🔍 Single-choice Question",
        "<details>",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">",
        "<b>Which of the following prompt elements is used to explicitly define the task that the large language model needs to complete❓</b>",
        "",
        "- A. Role",
        "- B. Audience",
        "- C. Objective",
        "- D. Context",
        "",
        "**[Click to view answer]**",
        "</summary>",
        "",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">",
        "",
        "✅ **Reference Answer: C**",
        "📝 **Explanation**:",
        "- The Objective explicitly defines the operation or outcome that the large language model (LLMs) is required to perform. Other options do not directly define the task itself.",
        "- Role defines the identity that the LLMs should assume, Audience specifies the target group, and Context provides background information.",
        "",
        "</div>",
        "</details>",
        "",
        "---",
        "",
        "",
        "### 🔍 Multiple-choice Question",
        "<details>",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">",
        "<b>Suppose you want to use a large language model to generate a description of the solar system suitable for third-grade students. Which of the following user_query designs is more reasonable❓</b>",
        "",
        "- A. user_query=\"Write about the solar system.\"",
        "- B. user_query=\"Tell me about the solar system.\"",
        "- C. user_query=\"Explain the solar system as if you were talking to a third-grade student.\"",
        "- D. user_query=\"Write a short and engaging description of the solar system for a third-grade audience, focusing on key planets and their characteristics.\"",
        "- E. user_query=\"What is the solar system?\"",
        "",
        "**[Click to view answer]**",
        "</summary>",
        "",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">",
        "",
        "✅ **Reference Answer: CD**",
        "📝 **Explanation**:",
        "- Options C and D clearly specify the target audience and provide requirements for content and style.",
        "",
        "</div>",
        "</details>",
        "",
        "---",
        "",
        "",
        "### 🔍 Multiple-choice Question",
        "<details>",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">",
        "<b>When using a reasoning-based large language model (such as qwq-32b) to perform reasoning tasks, which of the following prompting techniques are recommended❓</b>",
        "",
        "- A. Provide simple and clear task instructions",
        "- B. Supplement with sufficient background information",
        "- C. If there are specific user groups or task requirements, set roles or audiences",
        "- D. Use chain-of-thought prompts to make the reasoning process more rigorous and reliable",
        "- E. When descriptions are too abstract or cannot be accurately described, clarify them by adding examples",
        "",
        "**[Click to view answer]**",
        "</summary>",
        "",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">",
        "",
        "✅ **Reference Answer: ABCE**",
        "📝 **Explanation**:",
        "- When using reasoning models, it is advisable to avoid using chain-of-thought prompting, as this may reduce the effectiveness of reasoning.",
        "",
        "</div>",
        "</details>",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b354883f",
      "metadata": {},
      "source": [
        "## ✉️ Evaluation and Feedback",
        "",
        "Thank you for studying the Alibaba Cloud Large Model ACP Certification course. If you think there are parts of the course that are well-written or need improvement, we look forward to your [evaluation and feedback through this questionnaire](https://survey.aliyun.com/apps/zhiliao/Mo5O9vuie).",
        "",
        "Your criticism and encouragement are both driving forces for our progress.  ",
        "",
        ""
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "learnacp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}