{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.1 Building a New Employee Q&A Robot with Large Language Models\n",
        "\n",
        "## ğŸš„ Preface\n",
        "You work at an educational content development company, and with the continuous influx of new employees, frequent Q&A demands have led to significant time and resource costs. You decide to use large language models (LLMs) technology to build a Q&A robot to improve the accuracy and efficiency of responses.\n",
        "\n",
        "## ğŸ Course Objectives\n",
        "After completing this course, you will be able to:\n",
        "\n",
        "* Call Qwen-Max through its API\n",
        "* Learn the working principles of LLM\n",
        "* Understand the limitations of LLM and their solutions\n",
        "\n",
        "## âš ï¸ Environment Preparation\n",
        "To ensure a smoother experience with the tutorial, we recommend that you first complete the [**Environment Preparation**](https://edu.aliyun.com/course/3130200/lesson/343310285) chapter in the **Alibaba Cloud Large Language Model Senior Engineer ACP Certification Course**, and ensure that the required environment for the course is correctly installed.\n",
        "\n",
        "\n",
        "## ğŸ’» 1. Calling Qwen-Max via API\n",
        "\n",
        "The most direct way to experience LLM is by interacting with them through a web interface (such as [Qwen-Max](https://tongyi.aliyun.com/qianwen/)). However, as a developer, you often need to integrate LLM capabilities into your own applications. You can use the widely adopted OpenAI Python SDK to call the Qwen-Max LLM. You have already installed the necessary dependencies in `1.0 Computing Environment Setup`. Before executing the following code, confirm that you have switched to the newly created Python environment, such as the `Python (llm_learn)` environment created in this example.\n",
        "\n",
        "<a href=\"https://img.alicdn.com/imgextra/i4/O1CN01nf2EYI1pwhhMbOWHe_!!6000000005425-0-tps-2258-1004.jpg\" target=\"_blank\">\n",
        "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01nf2EYI1pwhhMbOWHe_!!6000000005425-0-tps-2258-1004.jpg\" width=\"600\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://img.alicdn.com/imgextra/i3/O1CN01rn0jvB1Z1QJXUWaG2_!!6000000003134-0-tps-3138-914.jpg\" target=\"_blank\">\n",
        "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01rn0jvB1Z1QJXUWaG2_!!6000000003134-0-tps-3138-914.jpg\" width=\"600\">\n",
        "</a>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To invoke Qwen, you need to go to Model Studio, Alibaba Cloud's large model service platform, and activate [Model Invocation Service](https://bailian.console.aliyun.com/#/model-market) and [Create an API key](https://bailian.console.aliyun.com/?apiKey=1#/api-key).\n",
        "\n",
        "> If the following is displayed at the top of the page, it means that you have not yet activated the Model Studio invocation service. After activating the service, you can invoke the model.\n",
        "> <img src=\"https://help-static-aliyun-doc.aliyuncs.com/assets/img/zh-CN/5298748271/p856749.png\" width=\"600\">\n",
        "\n",
        "Before using the API, you need to properly handle the security issues of the API key. Writing the API key directly into the code is a bad habit because it is easy to leak the key when sharing the code, and all parts of the API key encoded in plaintext need to be modified after replacing the API key. A safer and more convenient approach is to store the API key in an environment variable.\n",
        "\n",
        "The following code will load your API key from the configuration file and set it as an environment variable. After the code is executed, the first five characters of the API key will be displayed (followed by asterisks), so you can confirm whether the configuration is correct without exposing the complete key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-15T08:58:03.667571Z",
          "iopub.status.busy": "2024-11-15T08:58:03.667184Z",
          "iopub.status.idle": "2024-11-15T08:59:44.060371Z",
          "shell.execute_reply": "2024-11-15T08:59:44.059650Z",
          "shell.execute_reply.started": "2024-11-15T08:58:03.667545Z"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Load the API Key for invoking the Qwen large model\n",
        "import os\n",
        "from config.load_key import load_key\n",
        "load_key()\n",
        "print(f'''Your configured API Key is: {os.environ[\"DASHSCOPE_API_KEY\"][:5]+\"*\"*5}''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You need to press â€œEnterâ€ to confirm the â€œAPI-KEYâ€ you entered. After successful input, you will see the message ```â€œThe API Key you configured is: sk-88***** â€```. \n",
        "If you need to change the â€œAPI-KEYâ€, please edit the â€œKEY.jsonâ€ file in the parent directory. \n",
        "If you are using VS-CODE, the input box for the API-KEY will appear at the **top** of the window.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start with a simple conversation. The following code creates an assistant named \"Company Assistant,\" which can answer questions about company operations. You can use the common question \"Choosing a project management tool\" as an example:  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-15T08:59:46.180449Z",
          "iopub.status.busy": "2024-11-15T08:59:46.179984Z",
          "iopub.status.idle": "2024-11-15T08:59:59.498380Z",
          "shell.execute_reply": "2024-11-15T08:59:59.497544Z",
          "shell.execute_reply.started": "2024-11-15T08:59:46.180423Z"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "é€‰æ‹©é¡¹ç›®ç®¡ç†å·¥å…·æ—¶ï¼Œéœ€è¦è€ƒè™‘å›¢é˜Ÿçš„å…·ä½“éœ€æ±‚ã€é¡¹ç›®ç±»å‹ã€é¢„ç®—ä»¥åŠå›¢é˜Ÿæˆå‘˜å¯¹æ–°å·¥å…·çš„æ¥å—ç¨‹åº¦ç­‰å› ç´ ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨çš„é¡¹ç›®ç®¡ç†å·¥å…·ï¼Œä¾›æ‚¨å‚è€ƒï¼š\n",
            "\n",
            "1. **Trello**ï¼šé€‚åˆå°å‹å›¢é˜Ÿå’Œç®€å•é¡¹ç›®ç®¡ç†ã€‚ä½¿ç”¨å¡ç‰‡å¼ç•Œé¢ï¼Œå¯ä»¥ç›´è§‚åœ°çœ‹åˆ°é¡¹ç›®çš„è¿›åº¦ã€‚\n",
            "\n",
            "2. **Jira**ï¼šç‰¹åˆ«é€‚ç”¨äºè½¯ä»¶å¼€å‘é¡¹ç›®ï¼Œæ”¯æŒæ•æ·å¼€å‘æ–¹æ³•ã€‚åŠŸèƒ½å¼ºå¤§ï¼Œä½†å¯èƒ½éœ€è¦ä¸€å®šæ—¶é—´æ¥å­¦ä¹ å¦‚ä½•é«˜æ•ˆä½¿ç”¨ã€‚\n",
            "\n",
            "3. **Asana**ï¼šé€‚ç”¨äºå„ç§è§„æ¨¡çš„å›¢é˜Ÿå’Œä¸åŒç±»å‹çš„é¡¹ç›®ã€‚æä¾›ä»»åŠ¡åˆ†é…ã€æ—¶é—´çº¿è§„åˆ’ç­‰åŠŸèƒ½ï¼Œæ˜“äºä¸Šæ‰‹ã€‚\n",
            "\n",
            "4. **Monday.com**ï¼šç•Œé¢å‹å¥½ï¼Œå¯å®šåˆ¶æ€§å¼ºï¼Œé€‚åˆåˆ›æ„å›¢é˜Ÿå’Œè¥é”€é¡¹ç›®ç®¡ç†ã€‚\n",
            "\n",
            "5. **Wrike**ï¼šæä¾›å¼ºå¤§çš„é¡¹ç›®ç®¡ç†å’Œåä½œåŠŸèƒ½ï¼Œé€‚åˆä¸­å¤§å‹ä¼ä¸šä½¿ç”¨ã€‚\n",
            "\n",
            "6. **Notion**ï¼šä¸ä»…å¯ä»¥ç”¨ä½œé¡¹ç›®ç®¡ç†å·¥å…·ï¼Œè¿˜æ”¯æŒçŸ¥è¯†ç®¡ç†ã€æ–‡æ¡£ç¼–è¾‘ç­‰å¤šç§ç”¨é€”ï¼Œéå¸¸é€‚åˆéœ€è¦ç»¼åˆç®¡ç†ä¿¡æ¯çš„å›¢é˜Ÿã€‚\n",
            "\n",
            "7. **Microsoft Project**ï¼šä¼ ç»Ÿçš„ä¼ä¸šçº§é¡¹ç›®ç®¡ç†è½¯ä»¶ï¼ŒåŠŸèƒ½å…¨é¢ï¼Œé€‚åˆå¤§å‹å¤æ‚é¡¹ç›®çš„ç®¡ç†ã€‚\n",
            "\n",
            "å»ºè®®æ‚¨å¯ä»¥å…ˆç¡®å®šå›¢é˜Ÿçš„å…·ä½“éœ€æ±‚ï¼ˆå¦‚é¡¹ç›®è§„æ¨¡ã€å›¢é˜Ÿäººæ•°ã€æ‰€éœ€åŠŸèƒ½ç­‰ï¼‰ï¼Œç„¶åæ ¹æ®è¿™äº›éœ€æ±‚ä»ä¸Šè¿°é€‰é¡¹ä¸­æŒ‘é€‰æœ€é€‚åˆçš„å·¥å…·è¿›è¡Œè¯•ç”¨ã€‚å¾ˆå¤šå·¥å…·éƒ½æä¾›äº†å…è´¹è¯•ç”¨æœŸï¼Œåˆ©ç”¨è¿™ä¸ªæœºä¼šå¯ä»¥è®©å›¢é˜Ÿæˆå‘˜ä½“éªŒå¹¶åé¦ˆï¼Œæœ€ç»ˆåšå‡ºæ›´åŠ åˆé€‚çš„é€‰æ‹©ã€‚\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "client = OpenAI(\n",
        "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
        "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
        ")\n",
        "def get_qwen_response(prompt):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"qwen-plus-0919\",\n",
        "        messages=[\n",
        "            # system message is used to set the role and task of the large model\n",
        "            {\"role\": \"system\", \"content\": \"You are responsible for answering questions in an educational content development company. Your name is Company Xiaomi, and you need to answer your colleagues' questions.\"},\n",
        "            # user message is used to input the user's question\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "response = get_qwen_response(\"What tools should our company use for project management?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to implement multi-round conversations (allowing the large model to reference historical dialogue information for replies), you can refer to [multi-round conversation](https://help.aliyun.com/zh/model-studio/user-guide/text-generation#865b38621dwin).\n",
        "\n",
        "After running the code above, you will notice that it takes some time (about 20 seconds) to see the complete response. This is because, by default, the API waits until the model has generated all the content before returning the result in one go. In practical applications, this waiting period might affect the user experience â€” imagine a scenario where users are staring at a blank interface for 20 seconds!\n",
        "\n",
        "Fortunately, you can use \"streaming output\" to optimize this issue. With streaming output, the model outputs responses progressively as it generates them, similar to how humans type, allowing users to see partial responses immediately. This significantly enhances the interactive experience. Next, letâ€™s take a look at how to implement streaming output...\n",
        "> ğŸ’¡ Tip: Streaming output only changes the way content is displayed; the model's reasoning process and the quality of the final answer remain unchanged. You can confidently use this feature to improve your application's user experience.\n",
        "\n",
        "To implement streaming output, simply add the `stream=True` parameter to the previous code and adjust the output method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-15T09:00:04.752657Z",
          "iopub.status.busy": "2024-11-15T09:00:04.752276Z",
          "iopub.status.idle": "2024-11-15T09:00:18.769987Z",
          "shell.execute_reply": "2024-11-15T09:00:18.769374Z",
          "shell.execute_reply.started": "2024-11-15T09:00:04.752633Z"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "é€‰æ‹©é€‚åˆå…¬å¸é¡¹ç›®çš„ç®¡ç†å·¥å…·ä¸»è¦å–å†³äºä½ ä»¬çš„å…·ä½“éœ€æ±‚ã€å›¢é˜Ÿè§„æ¨¡ã€é¢„ç®—ä»¥åŠä½ ä»¬å¸Œæœ›è§£å†³çš„å…·ä½“é—®é¢˜ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›è¾ƒä¸ºæµè¡Œçš„é¡¹ç›®ç®¡ç†å·¥å…·ï¼Œæ‚¨å¯ä»¥æ ¹æ®è‡ªèº«æƒ…å†µè€ƒè™‘ï¼š\n",
            "\n",
            "1. **Trello**ï¼šéå¸¸é€‚åˆæ•æ·å¼€å‘å’Œå°å‹å›¢é˜Ÿä½¿ç”¨ï¼Œç•Œé¢ç›´è§‚æ˜“ä¸Šæ‰‹ï¼Œæ”¯æŒçœ‹æ¿å¼çš„ä»»åŠ¡ç®¡ç†æ–¹å¼ã€‚\n",
            "\n",
            "2. **Jira**ï¼šå¯¹äºè½¯ä»¶å¼€å‘å›¢é˜Ÿæ¥è¯´ï¼ŒJira æ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„å·¥å…·ï¼Œå®ƒä¸ä»…æ”¯æŒæ•æ·å¼€å‘æµç¨‹ï¼Œè¿˜æä¾›äº†è¯¦ç»†çš„æŠ¥å‘Šå’Œè·Ÿè¸ªåŠŸèƒ½ã€‚\n",
            "\n",
            "3. **Asana**ï¼šé€‚ç”¨äºå„ç§è§„æ¨¡çš„å›¢é˜Ÿï¼Œæä¾›ä»»åŠ¡åˆ†é…ã€è¿›åº¦è¿½è¸ªç­‰åŠŸèƒ½ï¼Œæ”¯æŒå¤šç§è§†å›¾ï¼ˆå¦‚åˆ—è¡¨ã€çœ‹æ¿ç­‰ï¼‰ä»¥é€‚åº”ä¸åŒå·¥ä½œåœºæ™¯ã€‚\n",
            "\n",
            "4. **Monday.com**ï¼šä¸€ä¸ªé«˜åº¦å¯å®šåˆ¶åŒ–çš„é¡¹ç›®ç®¡ç†å¹³å°ï¼Œé€‚åˆéœ€è¦é«˜åº¦çµæ´»æ€§çš„å·¥ä½œç¯å¢ƒã€‚æ”¯æŒåˆ›å»ºè‡ªå®šä¹‰çš„å·¥ä½œæµã€æŠ¥å‘Šç­‰ã€‚\n",
            "\n",
            "5. **Teambition**ï¼šå›½å†…è¾ƒå—æ¬¢è¿çš„ä¸€æ¬¾åä½œå·¥å…·ï¼Œé™¤äº†åŸºæœ¬çš„é¡¹ç›®ç®¡ç†åŠŸèƒ½å¤–ï¼Œè¿˜åŒ…æ‹¬æ—¥ç¨‹å®‰æ’ã€æ–‡ä»¶å…±äº«ç­‰ç‰¹æ€§ï¼Œéå¸¸é€‚åˆè¿œç¨‹å›¢é˜Ÿä½¿ç”¨ã€‚\n",
            "\n",
            "6. **Wrike**ï¼šæä¾›å…¨é¢çš„é¡¹ç›®ç®¡ç†è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ—¶é—´çº¿è§„åˆ’ã€èµ„æºç®¡ç†ç­‰åŠŸèƒ½ï¼Œé€‚åˆä¸­å¤§å‹ä¼ä¸šä½¿ç”¨ã€‚\n",
            "\n",
            "å»ºè®®æ‚¨å¯ä»¥å…ˆç¡®å®šå›¢é˜Ÿæœ€éœ€è¦å“ªäº›åŠŸèƒ½ï¼Œæ¯”å¦‚æ˜¯å¦é‡è§†æ•æ·å¼€å‘çš„æ”¯æŒã€æ˜¯å¦æœ‰å¤æ‚çš„æƒé™è®¾ç½®éœ€æ±‚ã€æ˜¯å¦éœ€è¦é›†æˆå…¶ä»–æœåŠ¡ç­‰ï¼Œç„¶åå†ä»ä¸Šè¿°é€‰é¡¹ä¸­æŒ‘é€‰æœ€é€‚åˆæ‚¨å›¢é˜Ÿçš„å·¥å…·è¿›è¡Œè¯•ç”¨ã€‚å¤§å¤šæ•°å·¥å…·éƒ½æä¾›å…è´¹è¯•ç”¨æœŸï¼Œåˆ©ç”¨è¿™æ®µæ—¶é—´å……åˆ†æµ‹è¯•å…¶æ˜¯å¦æ»¡è¶³æ‚¨çš„éœ€æ±‚æ˜¯éå¸¸é‡è¦çš„ã€‚"
          ]
        }
      ],
      "source": [
        "def get_qwen_stream_response(user_prompt, system_prompt):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"qwen-plus-0919\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        stream=True\n",
        "    )\n",
        "    for chunk in response:\n",
        "        yield chunk.choices[0].delta.content\n",
        "\n",
        "response = get_qwen_stream_response(user_prompt=\"What tools should our company use for project management?\", system_prompt=\"You are responsible for answering questions related to educational content development in the company. Your name is Company Xiaomi, and you need to answer your colleagues' questions.\")\n",
        "for chunk in response:\n",
        "    print(chunk, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By asking the Qwen-Max model twice, you may discover some interesting phenomena:\n",
        "1. Even if the questions are exactly the same, each response will be slightly different. This is one of the characteristics of large language models (large language models (large language models (LLMs))), similar to humans, as they express similar ideas in different ways.\n",
        "2. The suggestions provided by the model focus on some general project management tools, such as Jira, Trello, or Asana. Although large language models (large language models (LLMs)) are highly knowledgeable, they do not understand the specific situation of your company, such as the existing toolchain, team size, budget constraints, etc.\n",
        "> Information about the project management software used by the company can be found in the file docs/Content_Developer_Job_Guide.pdf.\n",
        "\n",
        "These two phenomena are actually very interesting! Why do large language models (large language models (LLMs)) exhibit such characteristics? To understand this, we need to lift the \"mysterious veil\" of large language models (large language models (LLMs)) and see how they think and work. Don't worry, you will understand these concepts through simple and intuitive explanations.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š 2. How Large Language Models Work\n",
        "In recent decades, artificial intelligence has undergone a profound evolution from basic algorithms to generative AI. Generative AI can create entirely new content, such as text, images, audio, and video, by learning from vast amounts of data, greatly promoting the widespread application of AI technology. Common application scenarios include intelligent question answering (such as Qwen-Max, GPT), creative drawing (such as Stable Diffusion), and code generation (such as Lingma), covering various fields and making AI accessible.\n",
        "\n",
        "<a href=\"https://img.alicdn.com/imgextra/i3/O1CN01XhNFzh1bj3EybLhgk_!!6000000003500-0-tps-2090-1138.jpg\" target=\"_blank\">\n",
        "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01XhNFzh1bj3EybLhgk_!!6000000003500-0-tps-2090-1138.jpg\" width=\"600\">\n",
        "</a>\n",
        "\n",
        "Intelligent question answering is one of the most classic and widely used applications of large language models (large language models (LLMs)), serving as the best example for exploring how large language models (LLMs) work. The following will introduce the workflow of large language models (LLMs) in question-answering scenarios to help you better understand the underlying technical principles.\n",
        "\n",
        "### 2.1. The Question-Answering Workflow of large language models (LLMs)\n",
        "Below is an example where the input text â€œACP is a veryâ€ initiates a query to the LLM. The diagram below shows the complete process from initiating the query to outputting the text.\n",
        "\n",
        "<a href=\"https://img.alicdn.com/imgextra/i1/O1CN01yLBhyu1gSAlt3oI0p_!!6000000004140-2-tps-2212-1070.png\" target=\"_blank\">\n",
        "<img src=\"https://img.alicdn.com/imgextra/i1/O1CN01yLBhyu1gSAlt3oI0p_!!6000000004140-2-tps-2212-1070.png\" width=\"800\">\n",
        "</a>\n",
        "\n",
        "The question-answering workflow of large language models (LLMs) consists of five stages:\n",
        "\n",
        "**Stage 1: Tokenization of Input Text**\n",
        "\n",
        "A tokenization is the basic unit of text processing in large language models (LLMs), typically representing words, phrases, or symbols. We need to break down the sentence â€œACP is a veryâ€ into smaller units with independent semantic meaning (tokenizations) and assign each tokenization an ID. If necessary, you can use the [Tokenizer API](https://help.aliyun.com/zh/dashscope/developer-reference/tokenization-api?spm=5176.28197632.0.0.2130607dUIVd7Y&disableWebsiteRedirect=true) to calculate tokenizations.\n",
        "\n",
        "<a href=\"https://gw.alicdn.com/imgextra/i1/O1CN019gAS3k1DrhwpIdHl6_!!6000000000270-0-tps-2414-546.jpg\" target=\"_blank\">\n",
        "<img src=\"https://gw.alicdn.com/imgextra/i1/O1CN019gAS3k1DrhwpIdHl6_!!6000000000270-0-tps-2414-546.jpg\" width=\"800\">\n",
        "</a>\n",
        "\n",
        "**Stage 2: Token Embedding**\n",
        "\n",
        "Computers can only understand numbers and cannot directly comprehend the meaning of tokenizations. Therefore, tokenizations must be converted into numerical representations (i.e., vectors) so that they can be understood by computers. Token embedding transforms each tokenization into a vector of fixed dimensions.\n",
        "\n",
        "\n",
        "**Stage 3: Inference by the LLM**\n",
        "\n",
        "The LLM learns knowledge from vast amounts of pre-trained data. When we input new content, such as â€œACP is a very,â€ the LLM combines its learned knowledge to make predictions. It calculates the probabilities of all possible tokenizations and generates a set of candidate tokenizations. Finally, the LLM selects one tokenization as the next output based on these calculations.\n",
        "\n",
        "This explains why, when asked about internal project management tools within a company, the model cannot provide suggestions for internal tools, as its predictive ability is based solely on the pre-trained data and it lacks knowledge of information it has not been exposed to. Therefore, when requiring a Q&A bot to answer domain-specific questions, this issue needs to be addressed specifically, which will be further elaborated in Section 3 of this chapter.\n",
        "\n",
        "**Stage 4: Output Tokens**\n",
        "\n",
        "Since the LLM randomly selects from the candidate tokenizations based on their probabilities, this leads to the phenomenon that â€œeven if the question is exactly the same, the answers are slightly different each time.â€ To control the randomness of the generated content, parameters such as temperature and top-p are commonly adjusted.\n",
        "\n",
        "For example, in the figure below, the first set of candidate tokenizations output by the LLM is â€œinformative (50%),â€ â€œfun (25%),â€ â€œenlightening (20%),â€ and â€œboring (5%).â€ Adjusting the temperature or top-p parameters will influence the LLM's preference in selecting from the candidate tokenizations, such as choosing the highest probability option, â€œinformative.â€ You can learn more about these two parameters in Section 2.2 of this chapter.\n",
        "\n",
        "<a href=\"https://img.alicdn.com/imgextra/i3/O1CN01N93ZE81e6zAZA4TiK_!!6000000003823-0-tps-582-340.jpg\" target=\"_blank\">\n",
        "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01N93ZE81e6zAZA4TiK_!!6000000003823-0-tps-582-340.jpg\" width=\"180\">\n",
        "</a>\n",
        "\n",
        "Specifically, â€œinformativeâ€ will continue to be fed back into the LLM to generate subsequent candidate tokenizations. This process is called auto-regressive model, utilizing both the input text and previously generated text. The LLM uses this method to sequentially generate candidate tokenizations.\n",
        "\n",
        "**Stage 5: Output Text**\n",
        "\n",
        "The processes of Stage 3 and Stage 4 are repeated until a special tokenization (such as <EOS>, end of sentence) is output or the output length reaches a threshold, concluding the question-answering session. The LLM then outputs all generated content. Of course, you can utilize the streaming output capability of the LLM, which predicts and immediately returns some tokenizations. In this example, the final output would be â€œACP is a very informative course.â€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Parameters Affecting the Randomness of Content Generation in Large Language Models (LLMs)\n",
        "\n",
        "Assume a question-and-answer scenario where the user asks: \"What can you learn in the large language models (LLMs) ACP course?\" To simulate the content generation process of an LLM, we have preset a candidate token set consisting of the following tokens: \"RAG\", \"prompt\", \"model\", \"writing\", and \"drawing\". The LLM will select one from these five candidate tokens as the output (next-token), as shown below.\n",
        "> User question: What can you learn in the large language models (LLMs) ACP course?<br><br>\n",
        "> LLM response: RAG <br>\n",
        "\n",
        "In this process, two important parameters influence the LLM's output: temperature and top_p. These parameters control the randomness and diversity of the content generated by the LLM. Below, we introduce how these two parameters work and how to use them.\n",
        "\n",
        "#### 2.2.1 Temperature: Adjusting the Probability Distribution of the Candidate Token Set\n",
        "\n",
        "Before generating the next word (next-token), the LLM first calculates an initial probability distribution for the candidate tokens. This distribution represents the probability of each candidate token being selected as the next-token. Temperature acts as a regulator that alters the probability distribution of the candidate tokens, influencing the content generation of the LLM. By adjusting this parameter, you can flexibly control the diversity and creativity of the generated text.\n",
        "\n",
        "To better understand, the figure below illustrates the impact of different temperature values on the probability distribution of the candidate tokens. The plotting code is located in the /resources/2_1 directory.\n",
        "\n",
        "<a href=\"https://img.alicdn.com/imgextra/i4/O1CN0137QeqL1o3uhFmaXHU_!!6000000005170-0-tps-3538-1242.jpg\" target=\"_blank\">\n",
        "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN0137QeqL1o3uhFmaXHU_!!6000000005170-0-tps-3538-1242.jpg\" width=\"1000\">\n",
        "</a>\n",
        "\n",
        "The low, medium, and high temperatures in the figure are based on the range [0, 2) of the Qwen-Plus model.\n",
        "\n",
        "As shown in the figure, as the temperature increases from low to high (0.1 -> 0.7 -> 1.2), the probability distribution becomes flatter. The probability of the candidate token \"RAG\" decreases from 0.8 -> 0.6 -> 0.3. Although it remains the most probable token, its probability is now closer to that of other candidate tokens. Consequently, the final output transitions from relatively fixed to increasingly diverse.\n",
        "\n",
        "For different use cases, you can refer to the following recommendations for setting the temperature parameter:\n",
        "- Clear answers (e.g., generating code): Lower the temperature.\n",
        "- Creative variety (e.g., ad copy): Increase the temperature.\n",
        "- No special requirements: Use the default temperature (usually within the medium range).\n",
        "\n",
        "Note that when temperature = 0, although randomness is minimized, it does not guarantee identical outputs every time. For a deeper understanding, you can refer to [the underlying algorithm implementation of temperature](https://github.com/huggingface/transformers/blob/v4.49.0/src/transformers/generation/logits_process.py#L226).\n",
        "\n",
        "Next, letâ€™s experience the effect of temperature. By adjusting the temperature value, ask the same question 10 times and observe the fluctuations in the responses.\n",
        "> The example code for temperature is similar to the upcoming explanation of top_p, so it has been encapsulated for subsequent use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "execution": {
          "iopub.execute_input": "2024-11-15T12:06:39.470715Z",
          "iopub.status.busy": "2024-11-15T12:06:39.470351Z",
          "iopub.status.idle": "2024-11-15T12:06:44.434877Z",
          "shell.execute_reply": "2024-11-15T12:06:44.434229Z",
          "shell.execute_reply.started": "2024-11-15T12:06:39.470689Z"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "è¾“å‡º 1 : éª¥ã€éªŠã€é§®ã€éªã€‚\n",
            "è¾“å‡º 2 : éªé©¬\n",
            "è¾“å‡º 3 : é©·ã€é©¹ã€éªã€éª¥ã€‚\n",
            "è¾“å‡º 4 : èµ¤å…”\n",
            "è¾“å‡º 5 : é©·ã€é©¹ã€éªã€éª¥ã€‚\n",
            "è¾“å‡º 6 : é©¹å­\n",
            "è¾“å‡º 7 : éªé©¬\n",
            "è¾“å‡º 8 : é©¹å­\n",
            "è¾“å‡º 9 : é©·ã€é©¹ã€éª¥ã€é¨‹ã€‚\n",
            "è¾“å‡º 10 : é©¹å­\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def get_qwen_stream_response(user_prompt, system_prompt, temperature, top_p):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"qwen-plus-0919\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        stream=True\n",
        "    )\n",
        "    \n",
        "    for chunk in response:\n",
        "        yield chunk.choices[0].delta.content\n",
        "\n",
        "# The default values of temperature and top_p use the default values of the Qwen-Plus model\n",
        "def print_qwen_stream_response(user_prompt, system_prompt, temperature=0.7, top_p=0.8, iterations=10):\n",
        "    for i in range(iterations):\n",
        "        print(f\"Output {i + 1} : \", end=\"\")\n",
        "        ## Add delay to prevent rate limiting\n",
        "        time.sleep(0.5)\n",
        "        response = get_qwen_stream_response(user_prompt, system_prompt, temperature, top_p)\n",
        "        output_content = ''\n",
        "        for chunk in response:\n",
        "            output_content += chunk\n",
        "        print(output_content)\n",
        "\n",
        "# Qwen-Plus model: The value range of temperature is [0, 2), with a default value of 0.7\n",
        "# Set temperature=0\n",
        "print_qwen_stream_response(user_prompt=\"é©¬ä¹Ÿå¯ä»¥å«åš\", system_prompt=\"Please help me continue writing, with a word count requirement of 4 Chinese characters or less.\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-15T12:06:45.417531Z",
          "iopub.status.busy": "2024-11-15T12:06:45.417146Z",
          "iopub.status.idle": "2024-11-15T12:06:50.629069Z",
          "shell.execute_reply": "2024-11-15T12:06:50.628320Z",
          "shell.execute_reply.started": "2024-11-15T12:06:45.417506Z"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "è¾“å‡º 1 : åƒé‡Œé©¬ã€‚\n",
            "è¾“å‡º 2 : å››è¹„å…½ã€‚\n",
            "è¾“å‡º 3 : é©¹å­ã€‚\n",
            "è¾“å‡º 4 : åƒé‡Œé©¬ã€‚\n",
            "è¾“å‡º 5 : é©¹ï¼Œéª‰ã€‚\n",
            "è¾“å‡º 6 : éªé©¬ã€‚\n",
            "è¾“å‡º 7 : èµ¤å…”\n",
            "è¾“å‡º 8 : å››è¹„åŠ¨ç‰©\n",
            "è¾“å‡º 9 : èµ¤å…”é»„å¿ ã€‚ä½†å®é™…ä¸Šï¼Œé©¬çš„ç§°å‘¼æœ‰å¾ˆå¤šï¼Œå¦‚éªé©¬ã€èµ›é©¬ã€ Mustangï¼ˆæ³¨ï¼šæ­¤å¤„ Mustang ä¸ºé‡é©¬çš„ä¸€ä¸ªå“ç§ï¼‰ã€åéª‘ç­‰ã€‚å¦‚æœä½ åªæ˜¯å¸Œæœ›è·å¾—ä¸¤ä¸ªæ±‰å­—çš„ç­”æ¡ˆï¼Œé‚£åº”è¯¥æ˜¯â€œéªé©¬â€ã€‚ä½†ä¾æ®ä½ çš„è¦æ±‚é™å®šåœ¨å››ä¸ªæ±‰å­—ä»¥å†…ï¼Œæˆ‘ç»™å‡ºçš„ç­”æ¡ˆæ˜¯\"èµ¤å…”\"ã€‚éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œ\"èµ¤å…”\"æ˜¯æŒ‡ä¸€ç§å¤ä»£ä¼ è¯´ä¸­çš„åé©¬ï¼Œè€Œ\"é»„å¿ \"æ˜¯ä¸‰å›½æ—¶æœŸçš„äººç‰©ï¼Œè¿™é‡Œå°†å…¶å»æ‰ä»¥ç¬¦åˆæ‚¨çš„å­—æ•°é™åˆ¶è¦æ±‚ã€‚ä¸è¿‡éœ€è¦æ³¨æ„ï¼Œä¸åŒæƒ…å†µä¸‹å¯¹äºé©¬çš„ç§°å‘¼ä¹Ÿä¼šæœ‰æ‰€ä¸åŒï¼Œæ¯”å¦‚æŒ‰ç…§é¢œè‰²ã€ä½“å‹ã€é€Ÿåº¦ç­‰ç‰¹å¾å‘½åã€‚å¦‚æœæ‚¨å¯¹æŸä¸ªç‰¹å®šçš„æ–¹é¢æˆ–è¯­å¢ƒä¸‹é©¬çš„ç§°å‘¼æ„Ÿå…´è¶£ï¼Œè¯·è¿›ä¸€æ­¥å‘ŠçŸ¥ï¼Œæˆ‘ä¼šå°½åŠ›æä¾›å¸®åŠ©ã€‚ä¸è¿‡åŸºäºä½ ç»™çš„è¦æ±‚ç­”æ¡ˆæ˜¯ï¼šèµ¤å…”ã€‚\n",
            "è¾“å‡º 10 : éªé©¬ã€‚\n"
          ]
        }
      ],
      "source": [
        "# Set temperature=1.9\n",
        "print_qwen_stream_response(user_prompt=\"A horse can also be called\", system_prompt=\"Please help me continue writing, the word count requirement is within 4 Chinese characters.\", temperature=1.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "source": [
        "It can be clearly observed from the experiment that the higher the temperature value, the more varied and diverse the content generated by the model becomes.\n",
        "\n",
        "#### 2.2.2 top_p: Control the sampling range of the candidate token set\n",
        "\n",
        "top_p is a filtering mechanism used to select a \"small subset\" meeting specific conditions from the candidate token set. The specific method is as follows: sort by probability from high to low, and select tokens whose cumulative probability reaches the set threshold to form a new candidate set, thereby narrowing down the selection range.\n",
        "\n",
        "The figure below shows the sampling effect of different top_p values on the candidate token set.\n",
        "\n",
        "<a href=\"https://img.alicdn.com/imgextra/i1/O1CN01xmkonv21sNL6VtQpi_!!6000000007040-0-tps-2732-1282.jpg\" target=\"_blank\">\n",
        "<img src=\"https://img.alicdn.com/imgextra/i1/O1CN01xmkonv21sNL6VtQpi_!!6000000007040-0-tps-2732-1282.jpg\" width=\"700\">\n",
        "</a>\n",
        "\n",
        "In the illustration, the blue part represents tokens whose cumulative probability reaches the top_p threshold (e.g., 0.5 or 0.9), forming a new candidate set; the gray part represents tokens that are not selected.\n",
        "\n",
        "When top_p=0.5, the model prioritizes selecting the highest-probability token, i.e., \"RAG\"; when top_p=0.9, the model randomly selects one among \"RAG,\" \"Prompt,\" and \"Model\" to generate output.\n",
        "\n",
        "\n",
        "From this, it can be seen that the impact of the top_p value on the content generated by large language models (LLMs) can be summarized as follows:\n",
        "- Larger value: Wider candidate range, more diverse content, suitable for creative writing, poetry generation, and other scenarios.\n",
        "- Smaller value: Narrower candidate range, more stable output, suitable for news drafts, code generation, and other scenarios requiring clear answers.\n",
        "- Extremely small value (e.g., 0.0001): Theoretically, the model only selects the highest-probability token, resulting in very stable output. However, in practice, due to factors such as distributed systems and additional adjustments to model outputs, slight randomness may still be introduced, making it impossible to guarantee completely consistent output every time.\n",
        "\n",
        "\n",
        "Below, experience the effect of top_p. By adjusting the top_p value, ask the same question 10 times and observe the fluctuations in the response content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-15T12:06:52.894507Z",
          "iopub.status.busy": "2024-11-15T12:06:52.894139Z",
          "iopub.status.idle": "2024-11-15T12:06:58.171739Z",
          "shell.execute_reply": "2024-11-15T12:06:58.171137Z",
          "shell.execute_reply.started": "2024-11-15T12:06:52.894481Z"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "è¾“å‡º 1 : \"æ™ºæ¸¸æ— ç•Œ\"\n",
            "è¾“å‡º 2 : \"æ™ºæ¸¸æ— ç•Œ\"\n",
            "è¾“å‡º 3 : \"æ™ºæ¸¸æ— ç•Œ\"\n",
            "è¾“å‡º 4 : \"æ™ºæ¸¸æ— ç•Œ\"\n",
            "è¾“å‡º 5 : \"æ™ºæ¸¸æ— ç•Œ\"\n",
            "è¾“å‡º 6 : \"æ™ºæ¸¸æ— ç•Œ\"\n",
            "è¾“å‡º 7 : \"æ™ºæ¸¸æ— ç•Œ\"\n",
            "è¾“å‡º 8 : \"æ™ºæ¸¸æ— ç•Œ\"\n",
            "è¾“å‡º 9 : \"æ™ºæ¸¸æ— ç•Œ\"\n",
            "è¾“å‡º 10 : \"æ™ºæ¸¸æ— ç•Œ\"\n"
          ]
        }
      ],
      "source": [
        "# Qwen-Plus model: The value range of top_p is (0,1], with a default value of 0.8.\n",
        "# Set top_p=0.001\n",
        "print_qwen_stream_response(user_prompt=\"Name an intelligent gaming smartphone, it could be\", system_prompt=\"Please help me name it, the requirement is within 4 Chinese characters.\", top_p=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-15T12:06:58.394593Z",
          "iopub.status.busy": "2024-11-15T12:06:58.394205Z",
          "iopub.status.idle": "2024-11-15T12:07:03.378361Z",
          "shell.execute_reply": "2024-11-15T12:07:03.377556Z",
          "shell.execute_reply.started": "2024-11-15T12:06:58.394567Z"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "è¾“å‡º 1 : \"æ™ºç©æ— ç•Œ\"\n",
            "è¾“å‡º 2 : \"æ™ºæ¸¸æ— ç•Œ\"\n",
            "è¾“å‡º 3 : \"æ™ºèƒœæ¸¸æˆç‹\"æˆ–\"ææ™ºæˆ˜ç¥\"ã€‚è¿™ä¸¤æ¬¾åå­—éƒ½çªå‡ºäº†æ‰‹æœºçš„æ™ºèƒ½åŒ–å’Œå¼ºå¤§çš„æ¸¸æˆæ€§èƒ½ï¼Œèƒ½å¤Ÿå¸å¼•ç›®æ ‡æ¶ˆè´¹è€…çš„æ³¨æ„ã€‚\n",
            "è¾“å‡º 4 : \"æ™ºèƒœæ¸¸æˆç‹\" æˆ– \"ç”µç«æ€§èƒ½è€…\"ã€‚è¿™ä¸¤ä¸ªåå­—éƒ½å¼ºè°ƒäº†æ‰‹æœºçš„æ™ºèƒ½å’Œå¼ºå¤§çš„æ¸¸æˆæ€§èƒ½ã€‚å¦‚æœéœ€è¦æ›´ç®€æ´ä¸€äº›ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘\"æ™ºèƒœç‹\"æˆ–\"æ€§èƒ½è€…\"ã€‚\n",
            "è¾“å‡º 5 : \"æ™ºæ¸¸æ— ç•Œ\"\n",
            "è¾“å‡º 6 : \"æ™ºæ¸¸æ— ç•Œ\"\n",
            "è¾“å‡º 7 : \"æ™ºæ¸¸æ— ç•Œ\"\n",
            "è¾“å‡º 8 : \"æ™ºæ¸¸æ— ç•Œ\"\n",
            "è¾“å‡º 9 : \"æ™ºæ¸¸æ— ç•Œ\"\n",
            "è¾“å‡º 10 : \"æ™ºèƒœæ¸¸æˆç‹\" æˆ– \"ææ™ºæˆ˜ç¥\"ã€‚è¿™ä¸¤ä¸ªåå­—éƒ½å¼ºè°ƒäº†æ‰‹æœºçš„æ™ºèƒ½åŒ–å’Œå¼ºå¤§çš„æ¸¸æˆæ€§èƒ½ï¼Œèƒ½å¤Ÿå¸å¼•å–œæ¬¢ç©æ‰‹æ¸¸çš„æ¶ˆè´¹è€…ã€‚å¸Œæœ›å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼\n"
          ]
        }
      ],
      "source": [
        "# Set top_p=0.9\n",
        "print_qwen_stream_response(user_prompt=\"Name an intelligent gaming smartphone, it could be\", system_prompt=\"Please help me name it, the requirement is within 4 Chinese characters.\", top_p=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the experimental results, it can be observed that the higher the top_p value, the greater the randomness in the output of large language models (LLMs).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.2.3 Summary\n",
        "\n",
        "**Should temperature and top_p be adjusted simultaneously?**\n",
        "\n",
        "To ensure the controllability of the generated content, it is recommended not to adjust top_p and temperature at the same time. Simultaneous adjustment may lead to unpredictable output results. You can prioritize adjusting one parameter, observe its impact on the results, and then fine-tune gradually.\n",
        "\n",
        "><br>**Knowledge Extension: top_k**<br>In the Qwen series models, the parameter top_k also has capabilities similar to top_p. Refer to the [Qwen API Documentation](https://help.aliyun.com/zh/model-studio/developer-reference/use-qwen-by-calling-api?spm=a2c4g.11186623.help-menu-2400256.d_3_3_0.68332bdb2Afk2s&scm=20140722.H_2712576._.OR_help-V_1). It is a sampling mechanism that randomly selects one Token from the top k ranked by probability for output. Generally speaking, the larger the top_k, the more diverse the generated content; the smaller the top_k, the more fixed the content. When top_k is set to 1, the model only selects the Token with the highest probability, making the output more stable but also causing a lack of variation and creativity.<br><br>\n",
        ">**Knowledge Extension: seed**<br>In the Qwen series models, the parameter seed also supports controlling the determinism of the generated content. Refer to the [Qwen API Documentation](https://help.aliyun.com/zh/model-studio/developer-reference/use-qwen-by-calling-api?spm=a2c4g.11186623.help-menu-2400256.d_3_3_0.68332bdb2Afk2s&scm=20140722.H_2712576._.OR_help-V_1). Passing the same seed value during each model invocation while keeping other parameters unchanged will make the model return the exact same response every time as much as possible, but it cannot guarantee that the results will be completely identical every time.<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why does randomness still exist when controlling the output of large language models (LLM) by setting temperature, top_p, and seed?**\n",
        "\n",
        "Even if temperature is set to 0, top_p is set to an extremely small value (e.g., 0.0001), and the same seed is used, the generated results for the same question may still be inconsistent. This is because some complex factors may introduce slight randomness, such as the large language models (LLMs) running in a distributed system or optimization being applied to the model's output.\n",
        "\n",
        "For example:\n",
        "A distributed system is like slicing bread with different machines. Although each machine operates according to the same settings, subtle differences between devices may still result in slightly different slices of bread.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ 3. Enable Large Language Models (LLMs) to Answer Private Knowledge Questions\n",
        "To enable large language models (LLMs) to answer private knowledge questions, you can choose one of the following two approaches:\n",
        "- **Without modifying the model**<br>\n",
        "    Directly provide private knowledge-related reference information when asking questions.\n",
        "- **Modifying the model**<br>\n",
        "    Fine-tuning or even train a new model so that it can better understand and answer questions in specific domains.\n",
        "\n",
        "Considering the high cost of fine-tuning and training new models, in private knowledge question-answering scenarios, you can prioritize passing private knowledge through prompt. This method is simpler and more efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ä½ å¥½ï¼ä½ æ˜¯è½¯ä»¶ä¸€ç»„çš„æˆå‘˜ï¼Œæ ¹æ®å…¬å¸çš„å®‰æ’ï¼Œä½ ä»¬ç›®å‰ä»åœ¨ä½¿ç”¨ **Jira** è¿›è¡Œé¡¹ç›®ç®¡ç†ã€‚ä¸è¿‡ï¼Œå…¬å¸è®¡åˆ’åœ¨2026å¹´ä¹‹å‰é€æ­¥å°†æ‰€æœ‰å›¢é˜Ÿåˆ‡æ¢åˆ° **Microsoft Project**ã€‚\n",
            "\n",
            "å¦‚æœä½ æœ‰ä»»ä½•å…³äº Jira ä½¿ç”¨çš„é—®é¢˜ï¼Œæˆ–è€…éœ€è¦äº†è§£å¦‚ä½•é€æ­¥åˆ‡æ¢åˆ° Microsoft Project çš„å…·ä½“è®¡åˆ’ï¼Œå¯ä»¥éšæ—¶å‘æˆ‘å’¨è¯¢ã€‚å¸Œæœ›è¿™å¯¹ä½ æœ‰å¸®åŠ©ï¼"
          ]
        }
      ],
      "source": [
        "# User question\n",
        "user_question = \"I'm from software group one. What tool should be used for project management?\"\n",
        "\n",
        "# Company project management tool related knowledge\n",
        "knowledge = \"\"\"There are two options for company project management tools:\n",
        "  1. **Jira**: For software development teams, Jira is a very powerful tool that supports agile development methods such as Scrum and Kanban. It provides rich features including issue tracking, time tracking, etc.\n",
        "\n",
        "  2. **Microsoft Project**: For large enterprises or complex projects, Microsoft Project offers detailed planning, resource allocation, and cost control functions. It is more suitable for scenarios where strict control over project timelines and costs is required.\n",
        "\n",
        "  In general, please use Microsoft Project as the company has purchased full licenses. Software development groups one, three, and four are currently using Jira and are planned to gradually switch to Microsoft Project before 2026.\n",
        "\"\"\"\n",
        "\n",
        "response = get_qwen_stream_response(\n",
        "    user_prompt=user_question,\n",
        "    # Pass the company project management tool related knowledge as background information into the system prompt\n",
        "    system_prompt=\"You are responsible for answering questions in an educational content development company. Your name is Xiao Mi. You need to answer students' questions.\" + knowledge,\n",
        "    temperature=0.7,\n",
        "    top_p=0.8\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    print(chunk, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After passing in the reference information when asking a question, LLMs can answer questions about private knowledge. However, the disadvantages of this method are also obvious: the prompt length is limited, and when the amount of private data is too large, passing in all background information may result in an excessively long prompt, which could affect the model's processing efficiency or reach the length limit.\n",
        "\n",
        "To solve this problem, you can automatically retrieve relevant private knowledge when users ask questions, then merge the retrieved document snippets with user input before sending them to the LLM to generate the final answer. This avoids directly passing a large amount of background information into the prompt. This implementation approach is also known as Retrieval-Augmented Generation (RAG).\n",
        "\n",
        "Building a RAG application typically involves two phases:\n",
        "\n",
        "### 3.1 Indexing Phase\n",
        "<img src=\"https://gw.alicdn.com/imgextra/i2/O1CN010zLf411zVoZQ9cWsI_!!6000000006720-2-tps-1592-503.png\" width=\"600\">\n",
        "\n",
        "Indexing aims to convert private knowledge documents or fragments into a form that can be efficiently retrieved by splitting file content and transforming it into text embedding (using dedicated Embedding models), while retaining semantic information through vector storage for similarity calculations. Vectorization enables the model to efficiently retrieve and match relevant content, especially when dealing with large-scale knowledge bases, significantly improving query accuracy and response speed.\n",
        "\n",
        "These vectors, processed by the Embedding model, not only capture the semantic information of the text content well but also, due to the vectorized and standardized semantics, facilitate subsequent relevance calculations with search semantic vectors.\n",
        "\n",
        "### 3.2 Retrieval and Generation Phase\n",
        "<img src=\"https://img.alicdn.com/imgextra/i1/O1CN01vbkBXC1HQ0SBrC1Ii_!!6000000000751-2-tps-1776-639.png\" width=\"600\">\n",
        "\n",
        "Retrieval and generation involve retrieving relevant document fragments from the index based on the userâ€™s question. These fragments will be input together with the question into the LLM to generate the final response. In this way, the LLM can answer questions about private knowledge.\n",
        "\n",
        "In summary, applications based on the RAG structure avoid various issues caused by inputting entire reference documents as background information while extracting the most relevant parts through retrieval, thus improving the accuracy and relevance of the LLM output.\n",
        "\n",
        "## ğŸ“ 4. Summary of this Section\n",
        "In this section, we learned the following:\n",
        "\n",
        "- **How to use the LLM API**<br>\n",
        "    Through practical code examples, we learned how to call the LLM API and experience its capabilities in question-answering tasks.\n",
        "- **A preliminary understanding of how LLMs work**<br>\n",
        "    We explored how LLMs understand questions and generate responses, while also discussing the limitations of randomness and knowledge scope, and how to address these shortcomings.\n",
        "\n",
        "Beyond the tasks demonstrated in this section, LLMs can handle more types of tasks such as content generation, structured information extraction, text classification, and sentiment analysis. Additionally, introducing the RAG solution into your LLM applications can expand the scope of knowledge they can handle. In the next section, we will introduce methods for creating RAG applications.\n",
        "\n",
        "### Further Reading\n",
        "- While studying this course, if you want to learn more about related concepts and principles, you can try asking the LLM to provide further explanations or learning suggestions:\n",
        "> Qwen-Max supports enabling the enable_search parameter, which allows the LLM to enrich its responses using internet search results during the response generation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024å¹´å·´é»å¥¥è¿ä¼šï¼Œä¸­å›½ä½“è‚²ä»£è¡¨å›¢å…±è·å¾— **40æšé‡‘ç‰Œ**ï¼Œåœ¨é‡‘ç‰Œæ¦œä¸Šæ’åç¬¬äºŒï¼Œé‡‘ç‰Œæ•°ä¸æ’åç¬¬ä¸€çš„ç¾å›½é˜ŸæŒå¹³ã€‚è¿™ä¸€æˆç»©ä¹Ÿåˆ›é€ äº†ä¸­å›½å¢ƒå¤–å¥¥è¿ä¼šå‚èµ›é‡‘ç‰Œæ•°çš„æ–°çºªå½•ã€‚\n"
          ]
        }
      ],
      "source": [
        "completion = client.chat.completions.create(\n",
        "    model=\"qwen-plus\",  # This example uses qwen-plus. You can replace it with other model names as needed. Model list: https://help.aliyun.com/zh/model-studio/getting-started/models\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Number of gold medals won by Team China at the Paris Olympics\"},\n",
        "    ],\n",
        "    extra_body={\"enable_search\": True},\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reasoning model [QwQ](https://help.aliyun.com/zh/model-studio/user-guide/qwq) of Alibaba Cloud has strong reasoning capabilities. The model will first output the thought process and then provide the response content.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====================æ€è€ƒè¿‡ç¨‹====================\n",
            "\n",
            "å—¯ï¼Œç”¨æˆ·é—®çš„æ˜¯â€œ9.9å’Œ9.11è°å¤§â€ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤ç”¨æˆ·çš„é—®é¢˜æ˜¯å…³äºæ•°å€¼å¤§å°æ¯”è¾ƒçš„ã€‚è¿™é‡Œçš„â€œ9.9â€å’Œâ€œ9.11â€çœ‹èµ·æ¥åƒæ˜¯ä¸¤ä¸ªå°æ•°ï¼Œä½†å¯èƒ½ç”¨æˆ·æœ‰ç‰¹å®šçš„èƒŒæ™¯ï¼Œæ¯”å¦‚æ—¥æœŸæˆ–è€…ç‰ˆæœ¬å·ä¹‹ç±»çš„ï¼Œä¸è¿‡é€šå¸¸æ•°å­—æ¯”è¾ƒçš„è¯åº”è¯¥å°±æ˜¯æ•°å€¼å¤§å°ã€‚\n",
            "\n",
            "é¦–å…ˆï¼Œæˆ‘åº”è¯¥å…ˆå›å¿†ä¸€ä¸‹å°æ•°æ¯”è¾ƒçš„æ–¹æ³•ã€‚å°æ•°æ¯”è¾ƒçš„æ—¶å€™ï¼Œå…ˆæ¯”è¾ƒæ•´æ•°éƒ¨åˆ†ï¼Œæ•´æ•°éƒ¨åˆ†å¤§çš„é‚£ä¸ªæ•°å°±å¤§ã€‚å¦‚æœæ•´æ•°éƒ¨åˆ†ç›¸åŒï¼Œå†æ¯”è¾ƒå°æ•°éƒ¨åˆ†ï¼Œä»ååˆ†ä½å¼€å§‹ä¾æ¬¡æ¯”è¾ƒæ¯ä¸€ä½ï¼Œç›´åˆ°åˆ†å‡ºå¤§å°ã€‚\n",
            "\n",
            "ç°åœ¨æ¥çœ‹è¿™ä¸¤ä¸ªæ•°ï¼Œ9.9å’Œ9.11ã€‚å®ƒä»¬çš„æ•´æ•°éƒ¨åˆ†éƒ½æ˜¯9ï¼Œæ‰€ä»¥æ•´æ•°éƒ¨åˆ†ç›¸ç­‰ã€‚æ¥ä¸‹æ¥éœ€è¦æ¯”è¾ƒå°æ•°éƒ¨åˆ†ã€‚è¿™é‡Œè¦æ³¨æ„çš„æ˜¯ï¼Œ9.9å…¶å®å¯ä»¥çœ‹ä½œæ˜¯9.90ï¼Œè€Œ9.11åˆ™æ˜¯9.11ï¼Œè¿™æ ·å°æ•°éƒ¨åˆ†å°±æ˜¯ä¸¤ä½å°æ•°çš„æ¯”è¾ƒã€‚è¿™æ ·ï¼Œååˆ†ä½ä¸Šéƒ½æ˜¯9å’Œ1å—ï¼Ÿä¸å¯¹ï¼Œç­‰ä¸€ä¸‹ï¼Œå¯èƒ½æˆ‘å“ªé‡Œå¼„é”™äº†ã€‚\n",
            "\n",
            "è®©æˆ‘å†ä»”ç»†çœ‹ä¸€ä¸‹ï¼š9.9çš„å°æ•°éƒ¨åˆ†æ˜¯0.9ï¼Œä¹Ÿå°±æ˜¯ååˆ†ä½æ˜¯9ï¼Œç™¾åˆ†ä½æ˜¯0ã€‚è€Œ9.11çš„å°æ•°éƒ¨åˆ†æ˜¯0.11ï¼Œä¹Ÿå°±æ˜¯ååˆ†ä½æ˜¯1ï¼Œç™¾åˆ†ä½æ˜¯1ã€‚è¿™æ—¶å€™æ¯”è¾ƒçš„è¯ï¼Œååˆ†ä½ä¸Šï¼Œ9.9çš„ååˆ†ä½æ˜¯9ï¼Œè€Œ9.11çš„ååˆ†ä½æ˜¯1ï¼Œæ˜¾ç„¶9æ¯”1å¤§ï¼Œæ‰€ä»¥æ•´ä¸ªæ•°9.9åº”è¯¥æ¯”9.11å¤§ï¼Ÿ\n",
            "\n",
            "å¯æ˜¯è¿™å¥½åƒæœ‰é—®é¢˜ï¼Œå› ä¸ºé€šå¸¸åƒç‰ˆæœ¬å·æˆ–è€…æ—¥æœŸçš„è¯ï¼Œæ¯”å¦‚9.11å¯èƒ½æŒ‡çš„æ˜¯9æœˆ11æ—¥ï¼Œä½†å¦‚æœæ˜¯æ•°å€¼çš„è¯ï¼Œå¯èƒ½ç”¨æˆ·æ˜¯æƒ³é—®å“ªä¸ªæ›´å¤§ã€‚ä¸è¿‡æŒ‰ç…§æ•°å­¦ä¸Šçš„æ¯”è¾ƒï¼Œç¡®å®æ˜¯9.9æ›´å¤§ï¼Œå› ä¸ºå°æ•°ç‚¹åç¬¬ä¸€ä½9æ¯”1å¤§ã€‚ä½†å¯èƒ½ç”¨æˆ·æœ‰å…¶ä»–çš„è€ƒè™‘ï¼Ÿ\n",
            "\n",
            "æˆ–è€…ç”¨æˆ·å¯èƒ½æŠŠ9.11å½“æˆäº†9.11ï¼Œè€Œ9.9æ˜¯9.90ï¼Œè¿™æ—¶å€™æ¯”è¾ƒçš„è¯ï¼Œ0.90å’Œ0.11ï¼Œæ˜¾ç„¶0.90æ›´å¤§ï¼Œæ‰€ä»¥9.9æ›´å¤§ã€‚ä¸è¿‡æœ‰æ—¶å€™åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ¯”å¦‚åƒç‰ˆæœ¬å·ä¸­ï¼Œæ¯”å¦‚2.10å’Œ2.9ï¼Œé€šå¸¸2.10ä¼šæ¯”2.9å¤§ï¼Œå› ä¸ºç‰ˆæœ¬å·çš„æ•°å­—ä½æ•°ä¸åŒçš„è¯ï¼Œå¯èƒ½éœ€è¦è¡¥é›¶æ¯”è¾ƒï¼Œæ¯”å¦‚2.9è§†ä¸º2.09ï¼Œè¿™æ ·2.10æ›´å¤§ã€‚ä½†è¿™é‡Œçš„æƒ…å†µæ˜¯9.9å’Œ9.11ï¼Œå¯èƒ½ç”¨æˆ·æ˜¯å¦åœ¨ç‰ˆæœ¬å·çš„æƒ…å†µä¸‹ï¼Ÿ\n",
            "\n",
            "ä¸è¿‡é—®é¢˜ä¸­æ²¡æœ‰æåˆ°ç‰ˆæœ¬å·ï¼Œæ‰€ä»¥åº”è¯¥æŒ‰æ™®é€šçš„æ•°å€¼æ¯”è¾ƒã€‚é‚£è¿™æ ·çš„è¯ï¼Œæ­£ç¡®çš„æ¯”è¾ƒåº”è¯¥æ˜¯9.9æ¯”9.11å¤§ï¼Œå› ä¸ºååˆ†ä½çš„9æ¯”1å¤§ã€‚ä¸è¿‡å¯èƒ½ç”¨æˆ·ä¼šç–‘æƒ‘ï¼Œå› ä¸ºæœ‰æ—¶å€™äººä»¬å¯èƒ½ä¼šè¯¯ä»¥ä¸º11æ¯”9å¤§ï¼Œä½†å…¶å®æ˜¯åœ¨å°æ•°ç‚¹åçš„ä¸åŒä½ç½®ã€‚\n",
            "\n",
            "æˆ–è€…ç”¨æˆ·å¯èƒ½æŠŠ9.11å½“æˆäº†åè¿›åˆ¶çš„9.11ï¼Œè€Œ9.9æ˜¯9.9ï¼Œè¿™æ—¶å€™ç¡®å®æ˜¯9.9æ›´å¤§ï¼Œå› ä¸ºå°æ•°ç‚¹åç¬¬ä¸€ä½9å¤§äº1ã€‚æ‰€ä»¥ç»“è®ºåº”è¯¥æ˜¯9.9æ›´å¤§ã€‚ä¸è¿‡æˆ‘éœ€è¦å†ä»”ç»†æ£€æŸ¥ä¸€ä¸‹ã€‚\n",
            "\n",
            "æ¯”å¦‚ï¼Œå°†ä¸¤è€…è½¬æ¢ä¸ºç›¸åŒçš„å°æ•°ä½æ•°ï¼š9.9 = 9.90ï¼Œè€Œ9.11 = 9.11ã€‚æ¯”è¾ƒæ—¶ï¼Œç¬¬ä¸€ä½å°æ•°æ˜¯9 vs 1ï¼Œæ‰€ä»¥9.90æ›´å¤§ã€‚å› æ­¤ç»“è®ºæ­£ç¡®ã€‚\n",
            "\n",
            "å¯èƒ½ç”¨æˆ·ä¼šæ··æ·†çš„æ˜¯ï¼Œåƒåœ¨æœˆä»½ä¸­ï¼Œæ¯”å¦‚9æœˆå’Œ11æœˆï¼Œ11æœˆæ›´å¤§ï¼Œä½†è¿™é‡Œä¸æ˜¯æœˆä»½æ¯”è¾ƒã€‚æˆ–è€…ç”¨æˆ·å¯èƒ½å†™é”™äº†ï¼Œæ¯”å¦‚åº”è¯¥æ˜¯9.1å’Œ9.11ï¼Ÿä¸è¿‡é—®é¢˜é‡Œæ˜ç¡®æ˜¯9.9å’Œ9.11ã€‚\n",
            "\n",
            "æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆåº”è¯¥æ˜¯9.9æ›´å¤§ã€‚ä¸è¿‡æˆ‘éœ€è¦ç¡®ä¿è‡ªå·±æ²¡æœ‰å“ªé‡Œç–æ¼ï¼Œæ¯”å¦‚æ˜¯å¦å­˜åœ¨å…¶ä»–è§£é‡Šæ–¹å¼ï¼Ÿ\n",
            "====================å®Œæ•´å›å¤====================\n",
            "\n",
            "åœ¨æ¯”è¾ƒæ•°å€¼å¤§å°æ—¶ï¼Œ9.9å’Œ9.11çš„å¤§å°å¯ä»¥é€šè¿‡ä»¥ä¸‹æ­¥éª¤ç¡®å®šï¼š\n",
            "\n",
            "1. **æ¯”è¾ƒæ•´æ•°éƒ¨åˆ†**ï¼šä¸¤è€…å‡ä¸º9ï¼Œæ•´æ•°éƒ¨åˆ†ç›¸ç­‰ã€‚\n",
            "2. **æ¯”è¾ƒå°æ•°éƒ¨åˆ†**ï¼š\n",
            "   - **9.9**å¯ä»¥è§†ä¸º **9.90**ï¼ˆè¡¥é›¶è‡³ä¸¤ä½å°æ•°ï¼‰ã€‚\n",
            "   - **9.11**ä¿æŒä¸º **9.11**ã€‚\n",
            "   - **ååˆ†ä½ï¼ˆç¬¬ä¸€ä½å°æ•°ï¼‰**ï¼š9.90çš„ååˆ†ä½æ˜¯ **9**ï¼Œè€Œ9.11çš„ååˆ†ä½æ˜¯ **1**ã€‚\n",
            "   - ç”±äº **9 > 1**ï¼Œå› æ­¤ **9.90 > 9.11**ã€‚\n",
            "\n",
            "**ç»“è®º**ï¼š**9.9æ¯”9.11å¤§**ã€‚\n",
            "\n",
            "å¦‚æœé—®é¢˜æ¶‰åŠå…¶ä»–è¯­å¢ƒï¼ˆå¦‚æ—¥æœŸæˆ–ç‰ˆæœ¬å·ï¼‰ï¼Œéœ€è¿›ä¸€æ­¥æ˜ç¡®ã€‚ä½†åœ¨æ•°å€¼æ¯”è¾ƒä¸­ï¼Œç­”æ¡ˆæ˜ç¡®ä¸º **9.9æ›´å¤§**ã€‚"
          ]
        }
      ],
      "source": [
        "reasoning_content = \"\"  # Define the complete reasoning process\n",
        "answer_content = \"\"     # Define the complete response\n",
        "is_answering = False   # Determine whether the reasoning process has ended and the response has started\n",
        "\n",
        "# Create a chat completion request\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"qwq-32b\",  # Here, qwq-32b is used as an example; you can replace it with another model name as needed\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Which is larger, 9.9 or 9.11?\"}\n",
        "    ],\n",
        "    stream=True,\n",
        "    # Uncomment the following to return token usage in the last chunk\n",
        "    # stream_options={\n",
        "    #     \"include_usage\": True\n",
        "    # }\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 20 + \" Reasoning Process \" + \"=\" * 20 + \"\\n\")\n",
        "\n",
        "for chunk in completion:\n",
        "    # If chunk.choices is empty, print usage\n",
        "    if not chunk.choices:\n",
        "        print(\"\\nUsage:\")\n",
        "        print(chunk.usage)\n",
        "    else:\n",
        "        delta = chunk.choices[0].delta\n",
        "        # Print the reasoning process\n",
        "        if hasattr(delta, 'reasoning_content') and delta.reasoning_content != None:\n",
        "            print(delta.reasoning_content, end='', flush=True)\n",
        "            reasoning_content += delta.reasoning_content\n",
        "        else:\n",
        "            # Start responding\n",
        "            if delta.content != \"\" and is_answering is False:\n",
        "                print(\"\\n\" + \"=\" * 20 + \" Complete Response \" + \"=\" * 20 + \"\\n\")\n",
        "                is_answering = True\n",
        "            # Print the response process\n",
        "            print(delta.content, end='', flush=True)\n",
        "            answer_content += delta.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- If you are interested in multi-modal large models, you can refer to:\n",
        "\n",
        "    * [Visual Understanding](https://help.aliyun.com/zh/model-studio/user-guide/vision)\n",
        "    * [Audio Understanding](https://help.aliyun.com/zh/model-studio/user-guide/audio-language-model)\n",
        "    * [Omni-modal](https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni)\n",
        "    * [Text-to-Image](https://help.aliyun.com/zh/model-studio/user-guide/text-to-image)\n",
        "    * [AI Video Generation](https://help.aliyun.com/zh/model-studio/user-guide/video-generation)  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”¥ Post-class Quiz\n",
        "\n",
        "### ğŸ” Multiple Choice Question 2.1.1\n",
        "<details>\n",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
        "<b>What is the purpose of the following code snippetâ“</b>\n",
        "<pre style=\"margin: 10px 0;\">\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
        "</pre>\n",
        "\n",
        "- A. Load the API key from disk  \n",
        "- B. Store the API key in memory  \n",
        "- C. Set the API key as an environment variable  \n",
        "- D. Create a new API key  \n",
        "\n",
        "**[Click to view answer]**\n",
        "</summary>\n",
        "\n",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
        "\n",
        "âœ… **Reference Answer: C**  \n",
        "ğŸ“ **Explanation**:  \n",
        "The code injects the API key into the current runtime environment's memory space using the operating system's environment variable interface.\n",
        "</div>\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“ Case Analysis Question 2.1.2\n",
        "<details>\n",
        "<summary style=\"cursor: pointer; padding: 12px;  border: 1px solid #dee2e6; border-radius: 6px; font-color:#000;\">\n",
        "<b>Xiaoming, while developing a writing assistant, encounters the following two scenarios. How should he solve these problemsâ“</b> \n",
        "\n",
        "- Scenario ğŸ…°ï¸ Lack of creativity in generated content: Every time he asks the model to write an article about \"the development of artificial intelligence,\" the generated content is very similar.  \n",
        "- Scenario ğŸ…±ï¸ Generated content deviates from the topic: When asking the model to write a technical document, the generated content often includes irrelevant information.  \n",
        "\n",
        "**Question:**  \n",
        "1. Based on the large language models (LLMs) workflow learned in this section, what might be the causes of these issues in the two scenarios?  \n",
        "2. How should the temperature or top_p parameters be adjusted to address these problems?  \n",
        "\n",
        "**[Click to view answer]**\n",
        "</summary>\n",
        "\n",
        "\n",
        "<div style=\"margin-top: 10px;  padding: 15px;  border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
        "\n",
        "#### ğŸ¯ Solution for Scenario A\n",
        "**ğŸ” Cause Analysis**  \n",
        "The `temperature` value is too low (e.g., 0.3), causing the model to make single choices, resulting in a lack of diversity in the generated content.\n",
        "\n",
        "**âš™ï¸ Parameter Adjustment**  \n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "temperature = 0.7~0.9  # Increase creativity\n",
        " top_p = 0.9            # Expand the range of word selection\n",
        "```\n",
        "\n",
        "#### ğŸ¯ Solution for Scenario B\n",
        "**ğŸ” Cause Analysis**  \n",
        "`temperature` is too high (e.g., 1.2) or `top_p` is too large\n",
        "\n",
        "**âš™ï¸ Parameter Adjustment**  \n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "temperature = 0.5~0.7  # Reduce randomness\n",
        " top_p = 0.7~0.8        # Focus on high probability words\n",
        "```\n",
        "\n",
        "ğŸŒŸ **Parameter Tuning Tips**\n",
        "> It is recommended to adjust the parameters by Â±0.2 each time and observe the effect changes through A/B testing.\n",
        "> If you need to balance Scenario A and Scenario B, it is recommended to use the combination: `temperature=0.6` + `top_p=0.8`</div>\n",
        "</details>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ… Evaluation and Feedback\n",
        "We welcome you to participate in the [Alibaba Cloud Large Language Model ACP Course Survey](https://survey.aliyun.com/apps/zhiliao/Mo5O9vuie) to provide feedback on your learning experience and course evaluation.\n",
        "Your criticism and encouragement are our motivation to move forward!  \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
