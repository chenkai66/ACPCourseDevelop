{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.2 Expanding the Knowledge Scope of the Q&A Bot\n",
        "\n",
        "## üöÑ Preface\n",
        "\n",
        "You have already learned that RAG chatbot is an effective solution for expanding the knowledge scope of large language models (LLMs). In this section, you will learn about the workflow of RAG chatbot and how to create a RAG chatbot application so that it can answer questions based on the company's policy documents.\n",
        "\n",
        "## üçÅ Course Objectives\n",
        "\n",
        "After completing this course, you will be able to:\n",
        "\n",
        "* Understand the workflow of RAG chatbot\n",
        "* Create a RAG chatbot application\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. How RAG Works\n",
        "\n",
        "You might lose points in an exam because you forgot a concept or formula, but if the exam is open-book, you only need to find the most relevant knowledge point and add your understanding to answer the question.\n",
        "\n",
        "The same applies to large language models (large language models (large language models (LLMs)s)). During training, if the model has not seen certain knowledge points (e.g., your company's policy documents), directly asking it related questions will result in inaccurate answers. However, if relevant knowledge is provided as a reference during content generation, similar to an open-book exam, the quality of the large language models (LLMs)'s responses will significantly improve.\n",
        "\n",
        "Retrieval Augmented Generation (RAG) is a solution that provides reference materials for large language models (large language models (LLMs)s). RAG applications typically consist of two parts: **indexing** and **retrieval generation**.\n",
        "\n",
        "### 1.1 Indexing\n",
        "You might mark reference materials before an exam to help you quickly locate relevant information during the test. Similarly, RAG applications often pre-mark references, a process called **indexing**, which includes four steps:<br>\n",
        "1. **Document Parsing**<br>\n",
        "Just as you convert visual information from books into text, RAG applications also need to load and parse knowledge base documents into a textual format that large language models (large language models (LLMs)s) can understand.\n",
        "2. **Text Chunking**<br>\n",
        "You usually don't flip through an entire book when solving a problem; instead, you look for the most relevant paragraphs. Similarly, RAG applications segment the parsed documents to quickly retrieve the most relevant content later.\n",
        "3. **Text Vectorization**<br>\n",
        "During an open-book exam, you first search for the most relevant paragraphs in the reference materials before answering. In RAG applications, embedding models are used to digitally represent both the paragraphs and the question. After comparing their similarity, the most relevant paragraph is identified. This process is called text vectorization.<br>\n",
        "    > If you're interested in the details of this process, you can explore the extended reading section of this tutorial.\n",
        "4. **Index Storage**<br>\n",
        "Index storage saves the vectorized paragraphs into a vector database, so RAG applications don't need to repeat these steps every time they respond, thus increasing response speed.\n",
        "\n",
        "    <img src=\"https://gw.alicdn.com/imgextra/i2/O1CN010zLf411zVoZQ9cWsI_!!6000000006720-2-tps-1592-503.png\" width=\"600\"><br>\n",
        "\n",
        "    After indexing, RAG applications can retrieve relevant text segments based on user questions.\n",
        "\n",
        "### 1.2 Retrieval Generation\n",
        "Retrieval and generation correspond to the `Retrieval` and `Generation` stages in RAG. **Retrieval** is like searching for materials during an open-book exam, while **generation** involves answering based on the retrieved materials and the question.<br>\n",
        "1. **Retrieval**<br>\n",
        "The retrieval phase recalls the most relevant text segments. The question is vectorized using an embedding model, and semantic similarity is compared with the paragraphs in the vector database to identify the most relevant ones. Retrieval is the most critical part of a RAG application. Imagine finding the wrong material during an exam‚Äîyour answer would be inaccurate. To improve retrieval accuracy, besides using powerful embedding models, techniques like reranking and sentence window retrieval can be applied. You can learn more about these in the next chapter.\n",
        "2. **Generation**<br>\n",
        "After retrieving relevant text segments, the RAG application generates the final prompt by combining the question and the retrieved text segments through a prompt template. The large language models (LLMs) then generates the response, leveraging its summarization abilities rather than relying solely on its internal knowledge.\n",
        "    > A typical prompt template is: `Please answer the user's question based on the following information: {retrieved text segments}. The user's question is: {question}.`\n",
        "\n",
        "    <img src=\"https://img.alicdn.com/imgextra/i1/O1CN01vbkBXC1HQ0SBrC1Ii_!!6000000000751-2-tps-1776-639.png\" width=\"600\"><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Creating a RAG Application\n",
        "\n",
        "Building a RAG application requires implementing the above functionalities, and this process is not easy. However, with LlamaIndex, you can achieve the aforementioned functionalities without writing too much code.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Please confirm your current Python environment  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before running the code in this section of the course, please make sure you have switched to the newly created Python environment, such as the `Python (llm_learn)` environment created in the previous lessons.\n",
        "\n",
        "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01rn0jvB1Z1QJXUWaG2_!!6000000003134-0-tps-3138-914.jpg\" width=\"600\">\n",
        "\n",
        "**Note: In each subsequent lesson, you should check whether you need to manually switch the Notebook environment.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 A Simple RAG chatbot\n",
        "\n",
        "As with the tutorial in the previous section, you need to run the following code to configure the Model Studio API Key into the environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-15T09:00:41.030766Z",
          "iopub.status.busy": "2024-11-15T09:00:41.030362Z",
          "iopub.status.idle": "2024-11-15T09:00:41.236899Z",
          "shell.execute_reply": "2024-11-15T09:00:41.236115Z",
          "shell.execute_reply.started": "2024-11-15T09:00:41.030739Z"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from config.load_key import load_key\n",
        "import os\n",
        "\n",
        "load_key()\n",
        "# In production environments, do not output the API Key to logs to avoid leakage\n",
        "print(f\"Your configured API Key is: {os.environ[\"DASHSCOPE_API_KEY\"][:5]+\"*\"*5}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have prepared some fictional company policy documents in the docs folder, and next you will create a RAG application based on these documents.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-15T09:00:43.822829Z",
          "iopub.status.busy": "2024-11-15T09:00:43.822278Z",
          "iopub.status.idle": "2024-11-15T09:00:58.744414Z",
          "shell.execute_reply": "2024-11-15T09:00:58.743812Z",
          "shell.execute_reply.started": "2024-11-15T09:00:43.822800Z"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Import dependencies\n",
        "from llama_index.embeddings.dashscope import DashScopeEmbedding, DashScopeTextEmbeddingModels\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.llms.openai_like import OpenAILike\n",
        "\n",
        "# These two lines of code are used to suppress WARNING messages to avoid interference with reading and learning. It is recommended to set the log level as needed in a production environment.\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "print(\"Parsing files...\")\n",
        "# LlamaIndex provides the SimpleDirectoryReader method, which can directly load files from a specified folder into document objects, corresponding to the parsing process.\n",
        "documents = SimpleDirectoryReader('./docs').load_data()\n",
        "\n",
        "print(\"Creating index...\")\n",
        "# The from_documents method includes slicing and index creation steps.\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    # Specify embedding model\n",
        "    embed_model=DashScopeEmbedding(\n",
        "        # You can also use other embedding models provided by Alibaba Cloud: https://help.aliyun.com/zh/model-studio/getting-started/models#3383780daf8hw\n",
        "        model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2\n",
        "    ))\n",
        "print(\"Creating query engine...\")\n",
        "query_engine = index.as_query_engine(\n",
        "    # Set to streaming output\n",
        "    streaming=True,\n",
        "    # Here we use the qwen-plus-0919 model. You can also use other Qwen text generation models provided by Alibaba Cloud: https://help.aliyun.com/zh/model-studio/getting-started/models#9f8890ce29g5u\n",
        "    llm=OpenAILike(\n",
        "        model=\"qwen-plus-0919\",\n",
        "        api_base=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
        "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
        "        is_chat_model=True\n",
        "        ))\n",
        "print(\"Generating response...\")\n",
        "streaming_response = query_engine.query('What tools should our company use for project management?')\n",
        "print(\"The answer is:\")\n",
        "# Use streaming output\n",
        "streaming_response.print_response_stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Saving and Loading Index\n",
        "You may find that creating an index takes a relatively long time. If you can save the index locally and load it directly when needed, instead of rebuilding the index, this can significantly improve the response speed. LlamaIndex provides an easy-to-implement method for saving and loading indexes.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-15T09:00:59.966266Z",
          "iopub.status.busy": "2024-11-15T09:00:59.965889Z",
          "iopub.status.idle": "2024-11-15T09:01:00.240477Z",
          "shell.execute_reply": "2024-11-15T09:01:00.239682Z",
          "shell.execute_reply.started": "2024-11-15T09:00:59.966241Z"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Save the index as a local file\n",
        "index.storage_context.persist(\"knowledge_base/test\")\n",
        "print(\"Index files saved to knowledge_base/test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-15T09:01:02.142167Z",
          "iopub.status.busy": "2024-11-15T09:01:02.141798Z",
          "iopub.status.idle": "2024-11-15T09:01:02.675970Z",
          "shell.execute_reply": "2024-11-15T09:01:02.675221Z",
          "shell.execute_reply.started": "2024-11-15T09:01:02.142142Z"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Load the local index file as an index\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"knowledge_base/test\")\n",
        "index = load_index_from_storage(storage_context, embed_model=DashScopeEmbedding(\n",
        "        model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2\n",
        "    ))\n",
        "print(\"Successfully loaded index from knowledge_base/test path\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After loading the index locally, you can test it again by asking questions to see if it works properly.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-15T09:01:11.982327Z",
          "iopub.status.busy": "2024-11-15T09:01:11.981943Z",
          "iopub.status.idle": "2024-11-15T09:01:14.921721Z",
          "shell.execute_reply": "2024-11-15T09:01:14.921129Z",
          "shell.execute_reply.started": "2024-11-15T09:01:11.982304Z"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(\"Creating the query engine...\")\n",
        "query_engine = index.as_query_engine(\n",
        "    # Set to streaming output\n",
        "    streaming=True,\n",
        "    # Use the qwen-plus-0919 model here. You can also use other text generation models provided by Alibaba Cloud: https://help.aliyun.com/zh/model-studio/getting-started/models#9f8890ce29g5u\n",
        "    llm=OpenAILike(\n",
        "        model=\"qwen-plus-0919\",\n",
        "        api_base=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
        "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
        "        is_chat_model=True\n",
        "        ))\n",
        "print(\"Generating response...\")\n",
        "streaming_response = query_engine.query('What tools should our company use for project management?')\n",
        "print(\"The answer is:\")\n",
        "streaming_response.print_response_stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can encapsulate the above code so that it can be quickly reused in subsequent iterations.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-15T09:01:16.991663Z",
          "iopub.status.busy": "2024-11-15T09:01:16.991276Z",
          "iopub.status.idle": "2024-11-15T09:01:20.492123Z",
          "shell.execute_reply": "2024-11-15T09:01:20.491499Z",
          "shell.execute_reply.started": "2024-11-15T09:01:16.991640Z"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from chatbot import rag\n",
        "\n",
        "# The citations have been indexed in previous steps, so the index can be loaded directly here. If you need to rebuild the index, you can add a line of code: rag.indexing()\n",
        "index = rag.load_index(persist_path='./knowledge_base/test')\n",
        "query_engine = rag.create_query_engine(index=index)\n",
        "\n",
        "rag.ask('What tools should our company use for project management?', query_engine=query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Multi-round Conversation\n",
        "The multi-round conversation in RAG is slightly different from the mechanism of directly initiating multi-round conversations with large language models. From the tutorial in section 2.1, you have learned that multi-round conversations allow large models to refer to historical dialogue information. The method is to add historical dialogue information to the messages list.\n",
        "\n",
        "During the retrieval phase in RAG applications, the system usually compares the semantic similarity between the user's input and text segments. However, directly comparing the user's input with text segments may lose historical dialogue information, leading to inaccurate retrieval results.\n",
        "\n",
        "Suppose a user asks \"Where is Zhang San's workstation?\" in the first round of dialogue, and then asks \"Who is his supervisor?\" in the second round. If the question in the second round is directly compared with text segments for similarity, the retrieval system will not know who \"he\" refers to, thus likely retrieving incorrect text segments.\n",
        "\n",
        "If both the complete historical dialogue and the question are input into the retrieval system, due to the large number of words, the retrieval system may fail to process it (embedding models perform worse on long texts than on short texts). The commonly used solution in the industry is:\n",
        "\n",
        "1. Through the large model, based on historical dialogue information, query rewriting. The new query will include key information from the historical dialogue.\n",
        "2. Use the new query to follow the original process for retrieval and generation.\n",
        "\n",
        "LlamaIndex provides convenient tools that can quickly implement multi-round conversations in RAG applications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-15T09:01:26.566550Z",
          "iopub.status.busy": "2024-11-15T09:01:26.566171Z",
          "iopub.status.idle": "2024-11-15T09:01:33.772277Z",
          "shell.execute_reply": "2024-11-15T09:01:33.771645Z",
          "shell.execute_reply.started": "2024-11-15T09:01:26.566525Z"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core.llms import ChatMessage, MessageRole\n",
        "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
        "\n",
        "custom_prompt = PromptTemplate(\n",
        "    \"\"\"\n",
        "Given a conversation (between a human and an assistant) and a follow-up message from the human,\n",
        "rewrite the message as a standalone question that includes all relevant context from the conversation.\n",
        "\n",
        "<Chat History>\n",
        "{chat_history}\n",
        "\n",
        "<Follow-up Message>\n",
        "{question}\n",
        "\n",
        "<Standalone Question>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Historical conversation information\n",
        "custom_chat_history = [\n",
        "    ChatMessage(role=MessageRole.USER,content=\"What are the subtypes of content development engineers?\"),\n",
        "    ChatMessage(role=MessageRole.ASSISTANT, content=\"Comprehensive technical positions.\"),\n",
        "]\n",
        "\n",
        "query_engine = index.as_query_engine(\n",
        "    # Set to streaming output\n",
        "    streaming=True,\n",
        "    # Use the qwen-plus-0919 model here; you can also use other text generation models provided by Alibaba Cloud: https://help.aliyun.com/zh/model-studio/getting-started/models#9f8890ce29g5u\n",
        "    llm=OpenAILike(\n",
        "        model=\"qwen-plus-0919\",\n",
        "        api_base=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
        "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
        "        is_chat_model=True\n",
        "        ))\n",
        "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
        "    query_engine=query_engine,\n",
        "    condense_question_prompt=custom_prompt,\n",
        "    chat_history=custom_chat_history,\n",
        "    llm=OpenAILike(\n",
        "        model=\"qwen-plus-0919\",\n",
        "        api_base=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
        "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
        "        is_chat_model=True\n",
        "        ),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "streaming_response = chat_engine.stream_chat(\"What are the core responsibilities?\")\n",
        "for token in streaming_response.response_gen:\n",
        "    print(token, end=\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although the last question did not mention \"content development engineer,\" the large model still rewrote the question based on the historical dialogue information as \"What are the core responsibilities of a content development engineer?\" and provided the correct answer.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù3.Summary of this section\n",
        "In this section, you have learned the following content:\n",
        "1. **The working principle of RAG**<br>\n",
        "A complete RAG application usually includes two phases: index building and retrieval generation. Index building consists of four steps: document parsing, text segmentation, text vectorization, and index storage. The retrieval generation phase includes two steps: retrieval and generation. After understanding the working principle of RAG, you can optimize and iterate on the RAG chatbot more effectively.\n",
        "2. **Creating a RAG application**<br>\n",
        "Using the highly integrated tools provided by LlamaIndex, you created a RAG application, and mastered the methods for saving and loading indexes. You also learned how to implement multi-round conversation in a RAG application.\n",
        "\n",
        "Although the RAG chatbot can already answer questions like \"What tools should our company use for project management?\" quite well, its current functionality is still relatively simple. In subsequent tutorials, we will introduce methods to expand the capabilities of the RAG chatbot. The next section will cover how to improve the quality of the RAG chatbot's responses by optimizing prompts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Further Reading\n",
        "\n",
        "#### Text Vectorization\n",
        "Computers cannot directly understand how similar the two sentences \"I like to eat apples\" and \"I love to eat apples\" are, but they can understand the similarity between two vectors of the same dimension (usually measured using cosine similarity). Text vectorization converts natural language into numerical forms that computers can understand through embedding models.\n",
        "\n",
        "The training of embedding models typically includes a phase of **contrastive learning**, where the input data consists of many text pairs (s1, s2) labeled as either related or unrelated. The model's training objective is to make the vector similarity of related text pairs as high as possible and the vector similarity of unrelated text pairs as low as possible.\n",
        "\n",
        "In the **indexing** phase, assuming n chunks [c1, c2, c3, ..., cn] have been obtained through text segmentation, the embedding model will convert these n chunks into vectors: [v1, v2, v3, ..., vn], which are then stored in a vector database.\n",
        "\n",
        "In the **retrieval** phase, assuming the user‚Äôs question is q, the embedding model will convert the question q into a vector vq and find the n most similar vectors to vq in the vector database (this value can be set by you). Through the index relationship between vectors and text segments, the corresponding text segments are retrieved as the search results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî• Post-class Quiz\n",
        "### üîç Multiple Choice Question\n",
        "\n",
        "<details>\n",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
        "<b>How should retrieval be conducted during multi-turn conversations in RAG applications? ‚ùì</b>\n",
        "\n",
        "- A. Input the complete historical dialogue information during the retrieval phase<br>\n",
        "- B. Rewrite the input question based on historical dialogue information before entering the retrieval phase<br>\n",
        "- C. Input the latest question during the retrieval phase<br>\n",
        "- D. Migrate the text segments recalled from the previous round<br>\n",
        "\n",
        "**[Click to view the answer]**\n",
        "</summary>\n",
        "\n",
        "<div style=\"margin-top: 10px; padding: 15px;  border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
        "\n",
        "‚úÖ **Reference Answer: B**  \n",
        "üìù **Explanation**:  \n",
        "- In multi-turn conversations, directly using the original question (Option C) or the full history (Option A) can lead to retrieval noise or information redundancy.\n",
        "- Option B dynamically rewrites the current question, maintaining conversational coherence while avoiding the outdated text migration issue of Option D, making it the optimal solution balancing efficiency and accuracy.\n",
        "\n",
        "</div>\n",
        "</details>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Evaluation Feedback\n",
        "We welcome you to participate in the [Alibaba Cloud Large Language Model ACP Course Survey](https://survey.aliyun.com/apps/zhiliao/Mo5O9vuie) to provide feedback on your learning experience and course evaluation.\n",
        "Your criticism and encouragement are our motivation to move forward!  \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "learnacp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
